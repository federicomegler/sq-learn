

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.linear_model package &mdash; Qsklearn  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Qsklearn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">sklearn</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Qsklearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>sklearn.linear_model package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/sklearn.linear_model.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sklearn-linear-model-package">
<h1>sklearn.linear_model package<a class="headerlink" href="#sklearn-linear-model-package" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sklearn.linear_model.tests.html">sklearn.linear_model.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-base-module">sklearn.linear_model.tests.test_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-bayes-module">sklearn.linear_model.tests.test_bayes module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-common-module">sklearn.linear_model.tests.test_common module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-coordinate-descent-module">sklearn.linear_model.tests.test_coordinate_descent module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#module-sklearn.linear_model.tests.test_huber">sklearn.linear_model.tests.test_huber module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-least-angle-module">sklearn.linear_model.tests.test_least_angle module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-logistic-module">sklearn.linear_model.tests.test_logistic module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-omp-module">sklearn.linear_model.tests.test_omp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-passive-aggressive-module">sklearn.linear_model.tests.test_passive_aggressive module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-perceptron-module">sklearn.linear_model.tests.test_perceptron module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-ransac-module">sklearn.linear_model.tests.test_ransac module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-ridge-module">sklearn.linear_model.tests.test_ridge module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-sag-module">sklearn.linear_model.tests.test_sag module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-sgd-module">sklearn.linear_model.tests.test_sgd module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-sparse-coordinate-descent-module">sklearn.linear_model.tests.test_sparse_coordinate_descent module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#sklearn-linear-model-tests-test-theil-sen-module">sklearn.linear_model.tests.test_theil_sen module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.linear_model.tests.html#module-sklearn.linear_model.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-sklearn.linear_model.setup">
<span id="sklearn-linear-model-setup-module"></span><h2>sklearn.linear_model.setup module<a class="headerlink" href="#module-sklearn.linear_model.setup" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.setup.configuration">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.setup.</span></span><span class="sig-name descname"><span class="pre">configuration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parent_package</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.setup.configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-sklearn.linear_model">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sklearn.linear_model" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#module-sklearn.linear_model" title="sklearn.linear_model"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code></a> module implements a variety of linear models.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">ARDRegression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">threshold_lambda</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ARDRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Bayesian ARD regression.</p>
<p>Fit the weights of a regression model, using an ARD prior. The weights of
the regression model are assumed to be in Gaussian distributions.
Also estimate the parameters lambda (precisions of the distributions of the
weights) and alpha (precision of the distribution of the noise).
The estimation is done by an iterative procedures (Evidence Maximization)</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_iter</strong> (<em>int</em><em>, </em><em>default=300</em>) – Maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Stop the algorithm if w has converged.</p></li>
<li><p><strong>alpha_1</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter.</p></li>
<li><p><strong>alpha_2</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter.</p></li>
<li><p><strong>lambda_1</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter.</p></li>
<li><p><strong>lambda_2</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter.</p></li>
<li><p><strong>compute_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If True, compute the objective function at each step of the model.</p></li>
<li><p><strong>threshold_lambda</strong> (<em>float</em><em>, </em><em>default=10 000</em>) – threshold for removing (pruning) weights with high precision from
the computation.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Verbose mode when fitting the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients of the regression model (mean of distribution)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>estimated precision of the noise.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.lambda_">
<span class="sig-name descname"><span class="pre">lambda_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.lambda_" title="Permalink to this definition">¶</a></dt>
<dd><p>estimated precisions of the weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.sigma_">
<span class="sig-name descname"><span class="pre">sigma_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.sigma_" title="Permalink to this definition">¶</a></dt>
<dd><p>estimated variance-covariance matrix of the weights</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.scores_">
<span class="sig-name descname"><span class="pre">scores_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>if computed, value of the objective function (to be maximized)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.X_offset_">
<span class="sig-name descname"><span class="pre">X_offset_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.X_offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>normalize=True</cite>, offset subtracted for centering data to a
zero mean.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.X_scale_">
<span class="sig-name descname"><span class="pre">X_scale_</span></span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.X_scale_" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>normalize=True</cite>, parameter used to scale data to a unit
standard deviation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">ARDRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">ARDRegression()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([1.])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>For an example, see <span class="xref std std-ref">examples/linear_model/plot_ard.py</span>.</p>
<p class="rubric">References</p>
<p>D. J. C. MacKay, Bayesian nonlinear modeling for the prediction
competition, ASHRAE Transactions, 1994.</p>
<p>R. Salakhutdinov, Lecture notes on Statistical Machine Learning,
<a class="reference external" href="http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15">http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15</a>
Their beta is our <code class="docutils literal notranslate"><span class="pre">self.alpha_</span></code>
Their alpha is our <code class="docutils literal notranslate"><span class="pre">self.lambda_</span></code>
ARD is a little different than the slide: only dimensions/features for
which <code class="docutils literal notranslate"><span class="pre">self.lambda_</span> <span class="pre">&lt;</span> <span class="pre">self.threshold_lambda</span></code> are kept and the rest are
discarded.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the ARDRegression model according to the given training data
and parameters.</p>
<p>Iterative procedure to maximize the evidence</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples in the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values (integers). Will be cast to X’s dtype if necessary</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.ARDRegression.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ARDRegression.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the linear model.</p>
<p>In addition to the mean of the predictive distribution, also its
standard deviation can be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Samples.</p></li>
<li><p><strong>return_std</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the standard deviation of posterior prediction.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>y_mean</strong> (<em>array-like of shape (n_samples,)</em>) – Mean of predictive distribution of query points.</p></li>
<li><p><strong>y_std</strong> (<em>array-like of shape (n_samples,)</em>) – Standard deviation of predictive distribution of query points.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">BayesianRidge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-06</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lambda_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">compute_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Bayesian ridge regression.</p>
<p>Fit a Bayesian ridge model. See the Notes section for details on this
implementation and the optimization of the regularization parameters
lambda (precision of the weights) and alpha (precision of the noise).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_iter</strong> (<em>int</em><em>, </em><em>default=300</em>) – Maximum number of iterations. Should be greater than or equal to 1.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Stop the algorithm if w has converged.</p></li>
<li><p><strong>alpha_1</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : shape parameter for the Gamma distribution prior
over the alpha parameter.</p></li>
<li><p><strong>alpha_2</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the alpha parameter.</p></li>
<li><p><strong>lambda_1</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : shape parameter for the Gamma distribution prior
over the lambda parameter.</p></li>
<li><p><strong>lambda_2</strong> (<em>float</em><em>, </em><em>default=1e-6</em>) – Hyper-parameter : inverse scale parameter (rate parameter) for the
Gamma distribution prior over the lambda parameter.</p></li>
<li><p><strong>alpha_init</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Initial value for alpha (precision of the noise).
If not set, alpha_init is 1/Var(y).</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</div></blockquote>
</p></li>
<li><p><strong>lambda_init</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Initial value for lambda (precision of the weights).
If not set, lambda_init is 1.</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</div></blockquote>
</p></li>
<li><p><strong>compute_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If True, compute the log marginal likelihood at each iteration of the
optimization.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model.
The intercept is not treated as a probabilistic parameter
and thus has no associated variance. If set
to False, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Verbose mode when fitting the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients of the regression model (mean of distribution)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated precision of the noise.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.lambda_">
<span class="sig-name descname"><span class="pre">lambda_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.lambda_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated precision of the weights.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.sigma_">
<span class="sig-name descname"><span class="pre">sigma_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.sigma_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated variance-covariance matrix of the weights</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.scores_">
<span class="sig-name descname"><span class="pre">scores_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>If computed_score is True, value of the log marginal likelihood (to be
maximized) at each iteration of the optimization. The array starts
with the value of the log marginal likelihood obtained for the initial
values of alpha and lambda and ends with the value obtained for the
estimated alpha and lambda.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_iter_+1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations to reach the stopping criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.X_offset_">
<span class="sig-name descname"><span class="pre">X_offset_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.X_offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>normalize=True</cite>, offset subtracted for centering data to a
zero mean.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.X_scale_">
<span class="sig-name descname"><span class="pre">X_scale_</span></span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.X_scale_" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>normalize=True</cite>, parameter used to scale data to a unit
standard deviation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">BayesianRidge</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">BayesianRidge()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([1.])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>There exist several strategies to perform Bayesian ridge regression. This
implementation is based on the algorithm described in Appendix A of
(Tipping, 2001) where updates of the regularization parameters are done as
suggested in (MacKay, 1992). Note that according to A New
View of Automatic Relevance Determination (Wipf and Nagarajan, 2008) these
update rules do not guarantee that the marginal likelihood is increasing
between two consecutive iterations of the optimization.</p>
<p class="rubric">References</p>
<p>D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems,
Vol. 4, No. 3, 1992.</p>
<p>M. E. Tipping, Sparse Bayesian Learning and the Relevance Vector Machine,
Journal of Machine Learning Research, Vol. 1, 2001.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values. Will be cast to X’s dtype if necessary</p></li>
<li><p><strong>sample_weight</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Individual weights for each sample</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>parameter <em>sample_weight</em> support to BayesianRidge.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.BayesianRidge.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_std</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.BayesianRidge.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the linear model.</p>
<p>In addition to the mean of the predictive distribution, also its
standard deviation can be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Samples.</p></li>
<li><p><strong>return_std</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the standard deviation of posterior prediction.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>y_mean</strong> (<em>array-like of shape (n_samples,)</em>) – Mean of predictive distribution of query points.</p></li>
<li><p><strong>y_std</strong> (<em>array-like of shape (n_samples,)</em>) – Standard deviation of predictive distribution of query points.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">ElasticNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Linear regression with combined L1 and L2 priors as regularizer.</p>
<p>Minimizes the objective function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">b</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_2</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package while
alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio
= 1 is the lasso penalty. Currently, l1_ratio &lt;= 0.01 is not reliable,
unless you supply your own sequence of alpha.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Constant that multiplies the penalty terms. Defaults to 1.0.
See the notes for the exact mathematical meaning of this
parameter. <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square,
solved by the <a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> object. For numerical
reasons, using <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> object is not advised.
Given this, you should use the <a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> object.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – The ElasticNet mixing parameter, with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>. For
<code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code> the penalty is an L2 penalty. <code class="docutils literal notranslate"><span class="pre">For</span> <span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it
is an L1 penalty.  For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a
combination of L1 and L2.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the
data is assumed to be already centered.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default=False</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. The Gram matrix can also be passed as argument.
For sparse input this option is always <code class="docutils literal notranslate"><span class="pre">False</span></code> to preserve sparsity.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, forces the coefficients to be positive.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the cost function formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.sparse_coef_">
<span class="sig-name descname"><span class="pre">sparse_coef_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.sparse_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse representation of the <cite>coef_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_features,) or             (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>Given param alpha, the dual gaps at the end of the optimization,
same shape as each observation of y.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNet</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">ElasticNet</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ElasticNet(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[18.83816048 64.55968825]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">1.451...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1.451...]</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></dt><dd><p>Elastic net model with best model selection by cross-validation.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a></dt><dd><p>Implements elastic net regression with incremental training.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a></dt><dd><p>Implements logistic regression with elastic net penalty (<code class="docutils literal notranslate"><span class="pre">SGDClassifier(loss=&quot;log&quot;,</span> <span class="pre">penalty=&quot;elasticnet&quot;)</span></code>).</p>
</dd>
</dl>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit model with coordinate descent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Data.</p></li>
<li><p><strong>y</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or             </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target. Will be cast to X’s dtype if necessary.</p></li>
<li><p><strong>sample_weight</strong> (<em>float</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weight. Internally, the <cite>sample_weight</cite> vector will be
rescaled to sum to <cite>n_samples</cite>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Coordinate descent is an algorithm that considers each column of
data at a time hence it will automatically convert the X input
as a Fortran-contiguous numpy array if necessary.</p>
<p>To avoid memory re-allocation it is advised to allocate the
initial data in memory directly using that format.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNet.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ElasticNet.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute elastic net path with coordinate descent.</p>
<p>The elastic net optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Number between 0 and 1 passed to elastic net (scaling between
l1 and l2 penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the Lasso.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to False, the input validation checks are skipped (including the
Gram matrix when provided). It is assumed that they are handled
by the caller.</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – Keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.
(Is returned when <code class="docutils literal notranslate"><span class="pre">return_n_iter</span></code> is set to True).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id0">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">sparse_coef_</span></span><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse representation of the fitted <cite>coef_</cite>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">ElasticNetCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.LinearModelCV</span></code></p>
<p>Elastic Net model with iterative fitting along a regularization path.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>l1_ratio</strong> (<em>float</em><em> or </em><em>list of float</em><em>, </em><em>default=0.5</em>) – float between 0 and 1 passed to ElasticNet (scaling between
l1 and l2 penalties). For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">0</span></code>
the penalty is an L2 penalty. For <code class="docutils literal notranslate"><span class="pre">l1_ratio</span> <span class="pre">=</span> <span class="pre">1</span></code> it is an L1 penalty.
For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1 and L2
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal notranslate"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path, used for each l1_ratio.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=0</em>) – Amount of verbosity.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, forces the coefficients to be positive.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>The amount of penalization chosen by cross validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.l1_ratio_">
<span class="sig-name descname"><span class="pre">l1_ratio_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.l1_ratio_" title="Permalink to this definition">¶</a></dt>
<dd><p>The compromise between l1 and l2 penalization chosen by
cross validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the cost function formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in the decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square error for the test set on each fold, varying l1_ratio and
alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_l1_ratio, n_alpha, n_folds)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>The grid of alphas used for fitting, for each l1_ratio.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gaps at the end of the optimization for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">ElasticNetCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ElasticNetCV(cv=5, random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
<span class="go">0.199...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">0.398...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[0.398...]</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_model_selection.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>The parameter l1_ratio corresponds to alpha in the glmnet R package
while alpha corresponds to the lambda parameter in glmnet.
More specifically, the optimization objective is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>If you are interested in controlling the L1 and L2 penalty
separately, keep in mind that this is equivalent to:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">*</span> <span class="n">L1</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">L2</span>
</pre></div>
</div>
<p>for:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">alpha</span> <span class="o">=</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span> <span class="ow">and</span> <span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">a</span> <span class="o">/</span> <span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.enet_path" title="sklearn.linear_model.enet_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enet_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.ElasticNetCV.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ElasticNetCV.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute elastic net path with coordinate descent.</p>
<p>The elastic net optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Number between 0 and 1 passed to elastic net (scaling between
l1 and l2 penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the Lasso.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to False, the input validation checks are skipped (including the
Gram matrix when provided). It is assumed that they are handled
by the caller.</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – Keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.
(Is returned when <code class="docutils literal notranslate"><span class="pre">return_n_iter</span></code> is set to True).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.GammaRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">GammaRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.GammaRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._glm.glm.GeneralizedLinearRegressor</span></code></p>
<p>Generalized Linear Model with a Gamma distribution.</p>
<p>This regressor uses the ‘log’ link function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1</em>) – Constant that multiplies the penalty term and thus determines the
regularization strength. <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to unpenalized
GLMs. In this case, the design matrix <cite>X</cite> must have full column rank
(no collinearities).</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Specifies if a constant (a.k.a. bias or intercept) should be
added to the linear predictor (X &#64; coef + intercept).</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – The maximal number of iterations for the solver.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Stopping criterion. For the lbfgs solver,
the iteration will stop when <code class="docutils literal notranslate"><span class="pre">max{|g_j|,</span> <span class="pre">j</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">d}</span> <span class="pre">&lt;=</span> <span class="pre">tol</span></code>
where <code class="docutils literal notranslate"><span class="pre">g_j</span></code> is the j-th component of the gradient (derivative) of
the objective function.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to <code class="docutils literal notranslate"><span class="pre">fit</span></code>
as initialization for <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> .</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – For the lbfgs solver set verbose to any positive number for verbosity.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.GammaRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.GammaRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated coefficients for the linear predictor (<cite>X * coef_ +
intercept_</cite>) in the GLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.GammaRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.GammaRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.GammaRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.GammaRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations used in the solver.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">GammaRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">19</span><span class="p">,</span> <span class="mi">26</span><span class="p">,</span> <span class="mi">33</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">GammaRegressor()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.773...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.072..., 0.066...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>
<span class="go">2.896...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]])</span>
<span class="go">array([19.483..., 35.795...])</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.GammaRegressor.family">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">family</span></span><a class="headerlink" href="#sklearn.linear_model.GammaRegressor.family" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Hinge">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Hinge</span></span><a class="headerlink" href="#sklearn.linear_model.Hinge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._sgd_fast.Classification</span></code></p>
<p>Hinge loss for binary classification tasks with y in {-1,1}</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>threshold</strong> (<em>float &gt; 0.0</em>) – Margin threshold. When threshold=1.0, one gets the loss used by SVM.
When threshold=0.0, one gets the loss used by the Perceptron.</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Huber">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Huber</span></span><a class="headerlink" href="#sklearn.linear_model.Huber" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._sgd_fast.Regression</span></code></p>
<p>Huber regression loss</p>
<p>Variant of the SquaredLoss that is robust to outliers (quadratic near zero,
linear in for large errors).</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Huber_Loss_Function">https://en.wikipedia.org/wiki/Huber_Loss_Function</a></p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">HuberRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.35</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-05</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Linear regression model that is robust to outliers.</p>
<p>The Huber Regressor optimizes the squared loss for the samples where
<code class="docutils literal notranslate"><span class="pre">|(y</span> <span class="pre">-</span> <span class="pre">X'w)</span> <span class="pre">/</span> <span class="pre">sigma|</span> <span class="pre">&lt;</span> <span class="pre">epsilon</span></code> and the absolute loss for the samples
where <code class="docutils literal notranslate"><span class="pre">|(y</span> <span class="pre">-</span> <span class="pre">X'w)</span> <span class="pre">/</span> <span class="pre">sigma|</span> <span class="pre">&gt;</span> <span class="pre">epsilon</span></code>, where w and sigma are parameters
to be optimized. The parameter sigma makes sure that if y is scaled up
or down by a certain factor, one does not need to rescale epsilon to
achieve the same robustness. Note that this does not take into account
the fact that the different features of X may be of different scales.</p>
<p>This makes sure that the loss function is not heavily influenced by the
outliers while not completely ignoring their effect.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span></p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>greater than 1.0</em><em>, </em><em>default=1.35</em>) – The parameter epsilon controls the number of samples that should be
classified as outliers. The smaller the epsilon, the more robust it is
to outliers.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – Maximum number of iterations that
<code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)</span></code> should run for.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – Regularization parameter.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This is useful if the stored attributes of a previously used model
has to be reused. If set to False, then the coefficients will
be rewritten for every call to fit.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to fit the intercept. This can be set to False
if the data is already centered around the origin.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-05</em>) – The iteration will stop when
<code class="docutils literal notranslate"><span class="pre">max{|proj</span> <span class="pre">g_i</span> <span class="pre">|</span> <span class="pre">i</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">n}</span></code> &lt;= <code class="docutils literal notranslate"><span class="pre">tol</span></code>
where pg_i is the i-th component of the projected gradient.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Features got by optimizing the Huber loss.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Bias.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.scale_">
<span class="sig-name descname"><span class="pre">scale_</span></span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.scale_" title="Permalink to this definition">¶</a></dt>
<dd><p>The value by which <code class="docutils literal notranslate"><span class="pre">|y</span> <span class="pre">-</span> <span class="pre">X'w</span> <span class="pre">-</span> <span class="pre">c|</span></code> is scaled down.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations that
<code class="docutils literal notranslate"><span class="pre">scipy.optimize.minimize(method=&quot;L-BFGS-B&quot;)</span></code> has run for.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.20: </span>In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
<code class="docutils literal notranslate"><span class="pre">max_iter</span></code>. <code class="docutils literal notranslate"><span class="pre">n_iter_</span></code> will now report at most <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.outliers_">
<span class="sig-name descname"><span class="pre">outliers_</span></span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.outliers_" title="Permalink to this definition">¶</a></dt>
<dd><p>A boolean mask which is set to True where the samples are identified
as outliers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">HuberRegressor</span><span class="p">,</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span> <span class="o">=</span> <span class="n">HuberRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">-7.284...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">huber</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([806.7200...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True coefficients:&quot;</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
<span class="go">True coefficients: [20.4923...  34.1698...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Huber coefficients:&quot;</span><span class="p">,</span> <span class="n">huber</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">Huber coefficients: [17.7906... 31.0106...]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Linear Regression coefficients:&quot;</span><span class="p">,</span> <span class="n">linear</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">Linear Regression coefficients: [-1.9221...  7.0226...]</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Peter J. Huber, Elvezio M. Ronchetti, Robust Statistics
Concomitant scale estimates, pg 172</p>
</dd>
<dt class="label" id="id2"><span class="brackets">2</span></dt>
<dd><p>Art B. Owen (2006), A robust hybrid of lasso and ridge regression.
<a class="reference external" href="https://statweb.stanford.edu/~owen/reports/hhu.pdf">https://statweb.stanford.edu/~owen/reports/hhu.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.HuberRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.HuberRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples in the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target vector relative to X.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Weight given to each sample.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Lars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero_coefs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Lars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Least Angle Regression model a.k.a. LAR</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em>, </em><em>'auto'</em><em> or </em><em>array-like</em><em> , </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>n_nonzero_coefs</strong> (<em>int</em><em>, </em><em>default=500</em>) – Target number of non-zero coefficients. Use <code class="docutils literal notranslate"><span class="pre">np.inf</span></code> for no limit.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>fit_path</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True the full path is stored in the <code class="docutils literal notranslate"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal notranslate"><span class="pre">fit_path</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</p></li>
<li><p><strong>jitter</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Upper bound on a uniform noise parameter to be added to the
<cite>y</cite> values, to satisfy the model’s assumption of
one-at-a-time computations. Might help with stability.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Determines random number generation for jittering. Pass an int
for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>. Ignored if <cite>jitter</cite> is None.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximum of covariances (in absolute value) at each iteration.
<code class="docutils literal notranslate"><span class="pre">n_alphas</span></code> is either <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> or the
number of nodes in the path with <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">&gt;=</span> <span class="pre">alpha_min</span></code>, whichever
is smaller. If this is a list of array-like, the length of the outer
list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas + 1,) or list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.active_">
<span class="sig-name descname"><span class="pre">active_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.active_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indices of active variables at the end of the path.
If this is a list of list, the length of the outer list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (n_alphas,) or list of such lists</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.coef_path_">
<span class="sig-name descname"><span class="pre">coef_path_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.coef_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal notranslate"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal notranslate"><span class="pre">False</span></code>. If this is a list
of array-like, the length of the outer list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_alphas + 1) or list             of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the formulation formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or array-like of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.Lars.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations taken by lars_path to find the
grid of alphas for each target.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like or int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lars</span><span class="p">(</span><span class="n">n_nonzero_coefs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="go">Lars(n_nonzero_coefs=1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[ 0. -1.11...]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Lars.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model using X, y as training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em><em>,                 </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns an instance of self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'lar'</span></em><a class="headerlink" href="#sklearn.linear_model.Lars.method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lars.positive">
<span class="sig-name descname"><span class="pre">positive</span></span><em class="property"> <span class="pre">=</span> <span class="pre">False</span></em><a class="headerlink" href="#sklearn.linear_model.Lars.positive" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LarsCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LarsCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model._least_angle.Lars"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._least_angle.Lars</span></code></a></p>
<p>Cross-validated Least Angle Regression model.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em>, </em><em>'auto'</em><em> or </em><em>array-like</em><em> , </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram matrix
cannot be passed as argument since we will use only subsets of X.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>integer, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>max_n_alphas</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of points on the path used to compute the
residuals in the cross-validation</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.active_">
<span class="sig-name descname"><span class="pre">active_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.active_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indices of active variables at the end of the path.
If this is a list of lists, the outer list length is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of length n_alphas or list of such lists</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>parameter vector (w in the formulation formula)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>independent term in decision function</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.coef_path_">
<span class="sig-name descname"><span class="pre">coef_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.coef_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>the varying values of the coefficients along the path</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>the estimated regularization parameter alpha</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>the different values of alpha along the path</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.cv_alphas_">
<span class="sig-name descname"><span class="pre">cv_alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.cv_alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>all the values of alpha along the path for the different folds</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_cv_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal notranslate"><span class="pre">cv_alphas</span></code>)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_folds, n_cv_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LarsCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>the number of iterations run by Lars with the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like or int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LarsCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9996...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.0254...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([154.0842...])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LarsCV.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model using X, y as training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns an instance of self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LarsCV.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'lar'</span></em><a class="headerlink" href="#sklearn.linear_model.LarsCV.method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Lasso</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Lasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model._coordinate_descent.ElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.ElasticNet</span></code></a></p>
<p>Linear Model trained with L1 prior as regularizer (aka the Lasso)</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Technically the Lasso model is optimizing the same objective function as
the Elastic Net with <code class="docutils literal notranslate"><span class="pre">l1_ratio=1.0</span></code> (no L2 penalty).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Constant that multiplies the L1 term. Defaults to 1.0.
<code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by the <a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> object. For numerical
reasons, using <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the <code class="docutils literal notranslate"><span class="pre">Lasso</span></code> object is not advised.
Given this, you should use the <a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> object.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to False, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default=False</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. The Gram matrix can also be passed as argument.
For sparse input this option is always <code class="docutils literal notranslate"><span class="pre">False</span></code> to preserve sparsity.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, forces the coefficients to be positive.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.Lasso.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the cost function formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.Lasso.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>Given param alpha, the dual gaps at the end of the optimization,
same shape as each observation of y.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.sparse_coef_">
<span class="sig-name descname"><span class="pre">sparse_coef_</span></span><a class="headerlink" href="#sklearn.linear_model.Lasso.sparse_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Readonly property derived from <code class="docutils literal notranslate"><span class="pre">coef_</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_features, 1) or             (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.Lasso.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.Lasso.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list of int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">Lasso(alpha=0.1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[0.85 0.  ]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">0.15...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.Lasso.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Lasso.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute elastic net path with coordinate descent.</p>
<p>The elastic net optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Number between 0 and 1 passed to elastic net (scaling between
l1 and l2 penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the Lasso.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to False, the input validation checks are skipped (including the
Gram matrix when provided). It is assumed that they are handled
by the caller.</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – Keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.
(Is returned when <code class="docutils literal notranslate"><span class="pre">return_n_iter</span></code> is set to True).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LassoCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.LinearModelCV</span></code></p>
<p>Lasso linear model with iterative fitting along a regularization path.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The best model is selected by cross-validation.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> alphas are set automatically.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If positive, restrict regression coefficients to be positive.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>The amount of penalization chosen by cross validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the cost function formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square error for the test set on each fold, varying alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas, n_folds)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>The grid of alphas used for fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gap at the end of the optimization for the optimal alpha
(<code class="docutils literal notranslate"><span class="pre">alpha_</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9993...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.4951...])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_model_selection.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoCV.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoCV.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Lasso path with coordinate descent</p>
<p>The Lasso optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code></p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> alphas are set automatically</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Note that in certain cases, the Lars solver may be significantly
faster to implement this functionality. In particular, linear
interpolation can be used to retrieve model coefficients between the
values output by lars_path</p>
<p class="rubric">Examples</p>
<p>Comparing lasso_path and lars_path with interpolation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use lasso_path to compute a coefficient path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">coef_path</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lasso_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path</span><span class="p">)</span>
<span class="go">[[0.         0.         0.46874778]</span>
<span class="go"> [0.2159048  0.4425765  0.23689075]]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now use lars_path and 1D linear interpolation to compute the</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># same path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alphas</span><span class="p">,</span> <span class="n">active</span><span class="p">,</span> <span class="n">coef_path_lars</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">interpolate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coef_path_continuous</span> <span class="o">=</span> <span class="n">interpolate</span><span class="o">.</span><span class="n">interp1d</span><span class="p">(</span><span class="n">alphas</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                                            <span class="n">coef_path_lars</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path_continuous</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">]))</span>
<span class="go">[[0.         0.         0.46915237]</span>
<span class="go"> [0.2159048  0.4425765  0.23668876]]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LassoLars</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">jitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoLars" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model._least_angle.Lars"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._least_angle.Lars</span></code></a></p>
<p>Lasso model fit with Least Angle Regression a.k.a. Lars</p>
<p>It is a Linear Model trained with an L1 prior as regularizer.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Constant that multiplies the penalty term. Defaults to 1.0.
<code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to an ordinary least square, solved
by <a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a>. For numerical reasons, using
<code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> with the LassoLars object is not advised and you
should prefer the LinearRegression object.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em>, </em><em>'auto'</em><em> or </em><em>array-like</em><em>, </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>fit_path</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code> the full path is stored in the <code class="docutils literal notranslate"><span class="pre">coef_path_</span></code> attribute.
If you compute the solution for a large problem or many targets,
setting <code class="docutils literal notranslate"><span class="pre">fit_path</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code> will lead to a speedup, especially
with a small alpha.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients will not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal notranslate"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.</p></li>
<li><p><strong>jitter</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Upper bound on a uniform noise parameter to be added to the
<cite>y</cite> values, to satisfy the model’s assumption of
one-at-a-time computations. Might help with stability.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Determines random number generation for jittering. Pass an int
for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>. Ignored if <cite>jitter</cite> is None.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximum of covariances (in absolute value) at each iteration.
<code class="docutils literal notranslate"><span class="pre">n_alphas</span></code> is either <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> or the
number of nodes in the path with <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">&gt;=</span> <span class="pre">alpha_min</span></code>, whichever
is smaller. If this is a list of array-like, the length of the outer
list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas + 1,) or list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.active_">
<span class="sig-name descname"><span class="pre">active_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.active_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indices of active variables at the end of the path.
If this is a list of list, the length of the outer list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of length n_alphas or list of such lists</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.coef_path_">
<span class="sig-name descname"><span class="pre">coef_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.coef_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>If a list is passed it’s expected to be one of n_targets such arrays.
The varying values of the coefficients along the path. It is not
present if the <code class="docutils literal notranslate"><span class="pre">fit_path</span></code> parameter is <code class="docutils literal notranslate"><span class="pre">False</span></code>. If this is a list
of array-like, the length of the outer list is <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_alphas + 1) or list             of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the formulation formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or array-like of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLars.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations taken by lars_path to find the
grid of alphas for each target.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like or int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLars</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="go">LassoLars(alpha=0.01)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[ 0.         -0.963257...]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsIC" title="sklearn.linear_model.LassoLarsIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsIC</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLars.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'lasso'</span></em><a class="headerlink" href="#sklearn.linear_model.LassoLars.method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LassoLarsCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model._least_angle.LarsCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._least_angle.LarsCV</span></code></a></p>
<p>Cross-validated Lasso, using the LARS algorithm.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em> or </em><em>'auto'</em><em> , </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram matrix
cannot be passed as argument since we will use only subsets of X.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>integer, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>max_n_alphas</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of points on the path used to compute the
residuals in the cross-validation</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal notranslate"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsCV only makes sense for problems where
a sparse solution is expected and/or reached.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>parameter vector (w in the formulation formula)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.coef_path_">
<span class="sig-name descname"><span class="pre">coef_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.coef_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>the varying values of the coefficients along the path</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features, n_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>the estimated regularization parameter alpha</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>the different values of alpha along the path</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.cv_alphas_">
<span class="sig-name descname"><span class="pre">cv_alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.cv_alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>all the values of alpha along the path for the different folds</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_cv_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>the mean square error on left-out for each fold along the path
(alpha values given by <code class="docutils literal notranslate"><span class="pre">cv_alphas</span></code>)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_folds, n_cv_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>the number of iterations run by Lars with the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like or int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.active_">
<span class="sig-name descname"><span class="pre">active_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.active_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indices of active variables at the end of the path.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoLarsCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9992...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.0484...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-77.8723...])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The object solves the same problem as the LassoCV object. However,
unlike the LassoCV, it find the relevant alphas values by itself.
In general, because of this property, it will be more stable.
However, it is more fragile to heavily multicollinear datasets.</p>
<p>It is more efficient than the LassoCV if only a small number of
features are selected compared to the total number, for instance if
there are very few samples compared to the number of features.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LarsCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a></p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsCV.method">
<span class="sig-name descname"><span class="pre">method</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'lasso'</span></em><a class="headerlink" href="#sklearn.linear_model.LassoLarsCV.method" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LassoLarsIC</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'aic'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model._least_angle.LassoLars"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._least_angle.LassoLars</span></code></a></p>
<p>Lasso model fit with Lars using BIC or AIC for model selection</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>AIC is the Akaike information criterion and BIC is the Bayes
Information criterion. Such criteria are useful to select the value
of the regularization parameter by making a trade-off between the
goodness of fit and the complexity of the model. A good model should
explain well the data while being simple.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{'bic'</em><em> , </em><em>'aic'}</em><em>, </em><em>default='aic'</em>) – The type of criterion to use.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>bool</em><em>, </em><em>'auto'</em><em> or </em><em>array-like</em><em>, </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform. Can be used for
early stopping.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Restrict coefficients to be &gt;= 0. Be aware that you might want to
remove fit_intercept which is set True by default.
Under the positive restriction the model coefficients do not converge
to the ordinary-least-squares solution for small values of alpha.
Only coefficients up to the smallest alpha value (<code class="docutils literal notranslate"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span>
<span class="pre">0.].min()</span></code> when fit_path=True) reached by the stepwise Lars-Lasso
algorithm are typically in congruence with the solution of the
coordinate descent Lasso estimator.
As a consequence using LassoLarsIC only makes sense for problems where
a sparse solution is expected and/or reached.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>parameter vector (w in the formulation formula)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>the alpha parameter chosen by the information criterion</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximum of covariances (in absolute value) at each iteration.
<code class="docutils literal notranslate"><span class="pre">n_alphas</span></code> is either <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> or the
number of nodes in the path with <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">&gt;=</span> <span class="pre">alpha_min</span></code>, whichever
is smaller. If a list, it will be of length <cite>n_targets</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas + 1,) or list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>number of iterations run by lars_path to find the grid of
alphas.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.criterion_">
<span class="sig-name descname"><span class="pre">criterion_</span></span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.criterion_" title="Permalink to this definition">¶</a></dt>
<dd><p>The value of the information criteria (‘aic’, ‘bic’) across all
alphas. The alpha which has the smallest information criterion is
chosen. This value is larger by a factor of <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> compared to
Eqns. 2.15 and 2.16 in (Zou et al, 2007).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LassoLarsIC</span><span class="p">(</span><span class="n">criterion</span><span class="o">=</span><span class="s1">&#39;bic&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.1111</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.1111</span><span class="p">])</span>
<span class="go">LassoLarsIC(criterion=&#39;bic&#39;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[ 0.  -1.11...]</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>The estimation of the number of degrees of freedom is given by:</p>
<p>“On the degrees of freedom of the lasso”
Hui Zou, Trevor Hastie, and Robert Tibshirani
Ann. Statist. Volume 35, Number 5 (2007), 2173-2192.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Akaike_information_criterion">https://en.wikipedia.org/wiki/Akaike_information_criterion</a>
<a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">https://en.wikipedia.org/wiki/Bayesian_information_criterion</a></p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LassoLarsIC.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LassoLarsIC.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model using X, y as training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – target values. Will be cast to X’s dtype if necessary</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=None</em>) – If provided, this parameter will override the choice
of copy_X made at instance creation.
If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns an instance of self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LinearRegression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LinearRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Ordinary least squares Linear Regression.</p>
<p>LinearRegression fits a linear model with coefficients w = (w1, …, wp)
to minimize the residual sum of squares between the observed targets in
the dataset, and the targets predicted by the linear approximation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to False, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><cite>normalize</cite> was deprecated in version 1.0 and will be
removed in 1.2.</p>
</div>
</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to use for the computation. This will only provide
speedup for n_targets &gt; 1 and sufficient large problems.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, forces the coefficients to be positive. This
option is only supported for dense arrays.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LinearRegression.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated coefficients for the linear regression problem.
If multiple targets are passed during the fit (y 2D), this
is a 2D array of shape (n_targets, n_features), while if only
one target is passed, this is a 1D array of length n_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_features, ) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression.rank_">
<span class="sig-name descname"><span class="pre">rank_</span></span><a class="headerlink" href="#sklearn.linear_model.LinearRegression.rank_" title="Permalink to this definition">¶</a></dt>
<dd><p>Rank of matrix <cite>X</cite>. Only available when <cite>X</cite> is dense.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression.singular_">
<span class="sig-name descname"><span class="pre">singular_</span></span><a class="headerlink" href="#sklearn.linear_model.LinearRegression.singular_" title="Permalink to this definition">¶</a></dt>
<dd><p>Singular values of <cite>X</cite>. Only available when <cite>X</cite> is dense.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (min(X, y),)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LinearRegression.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in the linear model. Set to 0.0 if
<cite>fit_intercept = False</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or array of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a></dt><dd><p>Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a></dt><dd><p>The Lasso is a linear model that estimates sparse coefficients with l1 regularization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a></dt><dd><p>Elastic-Net is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>From the implementation point of view, this is just plain Ordinary
Least Squares (scipy.linalg.lstsq) or Non Negative Least Squares
(scipy.optimize.nnls) wrapped as a predictor object.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># y = 1 * x_0 + 2 * x_1 + 3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span> <span class="o">+</span> <span class="mi">3</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([1., 2.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span>
<span class="go">3.0000...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">]]))</span>
<span class="go">array([16.])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LinearRegression.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LinearRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values. Will be cast to X’s dtype if necessary</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Individual weights for each sample</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>parameter <em>sample_weight</em> support to LinearRegression.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Log">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Log</span></span><a class="headerlink" href="#sklearn.linear_model.Log" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._sgd_fast.Classification</span></code></p>
<p>Logistic regression loss for binary classification with y in {-1, 1}</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LogisticRegression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lbfgs'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.SparseCoefMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Logistic Regression (aka logit, MaxEnt) classifier.</p>
<p>In the multiclass case, the training algorithm uses the one-vs-rest (OvR)
scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the
cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.
(Currently the ‘multinomial’ option is supported only by the ‘lbfgs’,
‘sag’, ‘saga’ and ‘newton-cg’ solvers.)</p>
<p>This class implements regularized logistic regression using the
‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers. <strong>Note
that regularization is applied by default</strong>. It can handle both dense
and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit
floats for optimal performance; any other input format will be converted
(and copied).</p>
<p>The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization
with primal formulation, or no regularization. The ‘liblinear’ solver
supports both L1 and L2 regularization, with a dual formulation only for
the L2 penalty. The Elastic-Net regularization is only supported by the
‘saga’ solver.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>penalty</strong> (<em>{'l1'</em><em>, </em><em>'l2'</em><em>, </em><em>'elasticnet'</em><em>, </em><em>'none'}</em><em>, </em><em>default='l2'</em>) – <p>Used to specify the norm used in the penalization. The ‘newton-cg’,
‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is
only supported by the ‘saga’ solver. If ‘none’ (not supported by the
liblinear solver), no regularization is applied.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)</p>
</div>
</p></li>
<li><p><strong>dual</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for stopping criteria.</p></li>
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Inverse of regularization strength; must be a positive float.
Like in support vector machines, smaller values specify stronger
regularization.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</p></li>
<li><p><strong>intercept_scaling</strong> (<em>float</em><em>, </em><em>default=1</em>) – <p>Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes <code class="docutils literal notranslate"><span class="pre">intercept_scaling</span> <span class="pre">*</span> <span class="pre">synthetic_feature_weight</span></code>.</p>
<p>Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>class_weight=’balanced’</em></p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used when <code class="docutils literal notranslate"><span class="pre">solver</span></code> == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the
data. See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>solver</strong> (<em>{'newton-cg'</em><em>, </em><em>'lbfgs'</em><em>, </em><em>'liblinear'</em><em>, </em><em>'sag'</em><em>, </em><em>'saga'}</em><em>,             </em><em>default='lbfgs'</em>) – <p>Algorithm to use in the optimization problem.</p>
<ul>
<li><p>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and
‘saga’ are faster for large ones.</p></li>
<li><p>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
handle multinomial loss; ‘liblinear’ is limited to one-versus-rest
schemes.</p></li>
<li><p>’newton-cg’, ‘lbfgs’, ‘sag’ and ‘saga’ handle L2 or no penalty</p></li>
<li><p>’liblinear’ and ‘saga’ also handle L1 penalty</p></li>
<li><p>’saga’ also supports ‘elasticnet’ penalty</p></li>
<li><p>’liblinear’ does not support setting <code class="docutils literal notranslate"><span class="pre">penalty='none'</span></code></p></li>
</ul>
<p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on
features with approximately the same scale. You can
preprocess the data with a scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default solver changed from ‘liblinear’ to ‘lbfgs’ in 0.22.</p>
</div>
</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – Maximum number of iterations taken for the solvers to converge.</p></li>
<li><p><strong>multi_class</strong> (<em>{'auto'</em><em>, </em><em>'ovr'</em><em>, </em><em>'multinomial'}</em><em>, </em><em>default='auto'</em>) – <p>If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, <em>even when the data is
binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>Default changed from ‘ovr’ to ‘auto’ in 0.22.</p>
</div>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – For the liblinear and lbfgs solvers set verbose to any positive
number for verbosity.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
Useless for liblinear solver. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>warm_start</em> to support <em>lbfgs</em>, <em>newton-cg</em>, <em>sag</em>, <em>saga</em> solvers.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPU cores used when parallelizing over classes if
multi_class=’ovr’”. This parameter is ignored when the <code class="docutils literal notranslate"><span class="pre">solver</span></code> is
set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
not. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors.
See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=None</em>) – The Elastic-Net mixing parameter, with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>. Only
used if <code class="docutils literal notranslate"><span class="pre">penalty='elasticnet'</span></code>. Setting <code class="docutils literal notranslate"><span class="pre">l1_ratio=0</span></code> is equivalent
to using <code class="docutils literal notranslate"><span class="pre">penalty='l2'</span></code>, while setting <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> is equivalent
to using <code class="docutils literal notranslate"><span class="pre">penalty='l1'</span></code>. For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;1</span></code>, the penalty is a
combination of L1 and L2.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of class labels known to the classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficient of the features in the decision function.</p>
<p><cite>coef_</cite> is of shape (1, n_features) when the given problem is binary.
In particular, when <cite>multi_class=’multinomial’</cite>, <cite>coef_</cite> corresponds
to outcome 1 (True) and <cite>-coef_</cite> corresponds to outcome 0 (False).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) or (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (a.k.a. bias) added to the decision function.</p>
<p>If <cite>fit_intercept</cite> is set to False, the intercept is set to zero.
<cite>intercept_</cite> is of shape (1,) when the given problem is binary.
In particular, when <cite>multi_class=’multinomial’</cite>, <cite>intercept_</cite>
corresponds to outcome 1 (True) and <cite>-intercept_</cite> corresponds to
outcome 0 (False).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,) or (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations for all classes. If binary or multinomial,
it returns only 1 element. For liblinear solver, only the maximum
number of iteration across all classes is given.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.20: </span>In SciPy &lt;= 1.0.0 the number of lbfgs iterations may exceed
<code class="docutils literal notranslate"><span class="pre">max_iter</span></code>. <code class="docutils literal notranslate"><span class="pre">n_iter_</span></code> will now report at most <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or (1, )</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a></dt><dd><p>Incrementally trained logistic regression (when given the parameter <code class="docutils literal notranslate"><span class="pre">loss=&quot;log&quot;</span></code>).</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.LogisticRegressionCV" title="sklearn.linear_model.LogisticRegressionCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticRegressionCV</span></code></a></dt><dd><p>Logistic regression with built-in cross validation.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The underlying C implementation uses a random number generator to
select features when fitting the model. It is thus not uncommon,
to have slightly different results for the same input data. If
that happens, try with a smaller tol parameter.</p>
<p>Predict output may not match that of standalone liblinear in certain
cases. See <span class="xref std std-ref">differences from liblinear</span>
in the narrative documentation.</p>
<p class="rubric">References</p>
<dl class="simple">
<dt>L-BFGS-B – Software for Large-scale Bound-constrained Optimization</dt><dd><p>Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.
<a class="reference external" href="http://users.iems.northwestern.edu/~nocedal/lbfgsb.html">http://users.iems.northwestern.edu/~nocedal/lbfgsb.html</a></p>
</dd>
<dt>LIBLINEAR – A Library for Large Linear Classification</dt><dd><p><a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/">https://www.csie.ntu.edu.tw/~cjlin/liblinear/</a></p>
</dd>
<dt>SAG – Mark Schmidt, Nicolas Le Roux, and Francis Bach</dt><dd><p>Minimizing Finite Sums with the Stochastic Average Gradient
<a class="reference external" href="https://hal.inria.fr/hal-00860051/document">https://hal.inria.fr/hal-00860051/document</a></p>
</dd>
<dt>SAGA – Defazio, A., Bach F. &amp; Lacoste-Julien S. (2014).</dt><dd><p>SAGA: A Fast Incremental Gradient Method With Support
for Non-Strongly Convex Composite Objectives
<a class="reference external" href="https://arxiv.org/abs/1407.0202">https://arxiv.org/abs/1407.0202</a></p>
</dd>
<dt>Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent</dt><dd><p>methods for logistic regression and maximum entropy models.
Machine Learning 85(1-2):41-75.
<a class="reference external" href="https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf">https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([0, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([[9.8...e-01, 1.8...e-02, 1.4...e-08],</span>
<span class="go">       [9.7...e-01, 2.8...e-02, ...e-08]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.97...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target vector relative to X.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) </em><em>default=None</em>) – <p>Array of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>sample_weight</em> support to LogisticRegression.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>self</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The SAGA solver supports both float64 and float32 bit arrays.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict logarithm of probability estimates.</p>
<p>The returned estimates for all classes are ordered by the
label of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Vector to be scored, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>T</strong> – Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in <code class="docutils literal notranslate"><span class="pre">self.classes_</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegression.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegression.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Probability estimates.</p>
<p>The returned estimates for all classes are ordered by the
label of classes.</p>
<p>For a multi_class problem, if multi_class is set to be “multinomial”
the softmax function is used to find the predicted probability of
each class.
Else use a one-vs-rest approach, i.e calculate the probability
of each class assuming it to be positive using the logistic function.
and normalize these values across all the classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Vector to be scored, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>T</strong> – Returns the probability of the sample for each class in the model,
where classes are ordered as they are in <code class="docutils literal notranslate"><span class="pre">self.classes_</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">LogisticRegressionCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Cs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dual</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lbfgs'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">refit</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_scaling</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratios</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model._logistic.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._logistic.LogisticRegression</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Logistic Regression CV (aka logit, MaxEnt) classifier.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>This class implements logistic regression using liblinear, newton-cg, sag
of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2
regularization with primal formulation. The liblinear solver supports both
L1 and L2 regularization, with a dual formulation only for the L2 penalty.
Elastic-Net penalty is only supported by the saga solver.</p>
<p>For the grid of <cite>Cs</cite> values and <cite>l1_ratios</cite> values, the best hyperparameter
is selected by the cross-validator
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code></a>, but it can be changed
using the <span class="xref std std-term">cv</span> parameter. The ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
solvers can warm-start the coefficients (see <span class="xref std std-term">Glossary</span>).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Cs</strong> (<em>int</em><em> or </em><em>list of floats</em><em>, </em><em>default=10</em>) – Each of the values in Cs describes the inverse of regularization
strength. If Cs is as an int, then a grid of Cs values are chosen
in a logarithmic scale between 1e-4 and 1e4.
Like in support vector machines, smaller values specify stronger
regularization.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Specifies if a constant (a.k.a. bias or intercept) should be
added to the decision function.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em> or </em><em>cross-validation generator</em><em>, </em><em>default=None</em>) – <p>The default cross-validation generator used is Stratified K-Folds.
If an integer is provided, then it is the number of folds used.
See the module <a class="reference internal" href="sklearn.model_selection.html#module-sklearn.model_selection" title="sklearn.model_selection"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.model_selection</span></code></a> module for the
list of possible cross-validation objects.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>dual</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Dual or primal formulation. Dual formulation is only implemented for
l2 penalty with liblinear solver. Prefer dual=False when
n_samples &gt; n_features.</p></li>
<li><p><strong>penalty</strong> (<em>{'l1'</em><em>, </em><em>'l2'</em><em>, </em><em>'elasticnet'}</em><em>, </em><em>default='l2'</em>) – Used to specify the norm used in the penalization. The ‘newton-cg’,
‘sag’ and ‘lbfgs’ solvers support only l2 penalties. ‘elasticnet’ is
only supported by the ‘saga’ solver.</p></li>
<li><p><strong>scoring</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=None</em>) – A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>. For a list of scoring functions
that can be used, look at <a class="reference internal" href="sklearn.metrics.html#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a>. The
default scoring option used is ‘accuracy’.</p></li>
<li><p><strong>solver</strong> (<em>{'newton-cg'</em><em>, </em><em>'lbfgs'</em><em>, </em><em>'liblinear'</em><em>, </em><em>'sag'</em><em>, </em><em>'saga'}</em><em>,             </em><em>default='lbfgs'</em>) – <p>Algorithm to use in the optimization problem.</p>
<ul>
<li><p>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and
‘saga’ are faster for large ones.</p></li>
<li><p>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
handle multinomial loss; ‘liblinear’ is limited to one-versus-rest
schemes.</p></li>
<li><p>’newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas
‘liblinear’ and ‘saga’ handle L1 penalty.</p></li>
<li><p>’liblinear’ might be slower in LogisticRegressionCV because it does
not handle warm-starting.</p></li>
</ul>
<p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on
features with approximately the same scale. You can preprocess the data
with a scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for stopping criteria.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – Maximum number of iterations of the optimization algorithm.</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>class_weight == ‘balanced’</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPU cores used during the cross-validation loop.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – For the ‘liblinear’, ‘sag’ and ‘lbfgs’ solvers set verbose to any
positive number for verbosity.</p></li>
<li><p><strong>refit</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to True, the scores are averaged across all folds, and the
coefs and the C that corresponds to the best score is taken, and a
final refit is done using these parameters.
Otherwise the coefs, intercepts and C that correspond to the
best scores across folds are averaged.</p></li>
<li><p><strong>intercept_scaling</strong> (<em>float</em><em>, </em><em>default=1</em>) – <p>Useful only when the solver ‘liblinear’ is used
and self.fit_intercept is set to True. In this case, x becomes
[x, self.intercept_scaling],
i.e. a “synthetic” feature with constant value equal to
intercept_scaling is appended to the instance vector.
The intercept becomes <code class="docutils literal notranslate"><span class="pre">intercept_scaling</span> <span class="pre">*</span> <span class="pre">synthetic_feature_weight</span></code>.</p>
<p>Note! the synthetic feature weight is subject to l1/l2 regularization
as all other features.
To lessen the effect of regularization on synthetic feature weight
(and therefore on the intercept) intercept_scaling has to be increased.</p>
</p></li>
<li><p><strong>multi_class</strong> (<em>{'auto</em><em>, </em><em>'ovr'</em><em>, </em><em>'multinomial'}</em><em>, </em><em>default='auto'</em>) – <p>If the option chosen is ‘ovr’, then a binary problem is fit for each
label. For ‘multinomial’ the loss minimised is the multinomial loss fit
across the entire probability distribution, <em>even when the data is
binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.
‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
and otherwise selects ‘multinomial’.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>Default changed from ‘ovr’ to ‘auto’ in 0.22.</p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used when <cite>solver=’sag’</cite>, ‘saga’ or ‘liblinear’ to shuffle the data.
Note that this only applies to the solver and not the cross-validation
generator. See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>l1_ratios</strong> (<em>list of float</em><em>, </em><em>default=None</em>) – The list of Elastic-Net mixing parameter, with <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;=</span> <span class="pre">1</span></code>.
Only used if <code class="docutils literal notranslate"><span class="pre">penalty='elasticnet'</span></code>. A value of 0 is equivalent to
using <code class="docutils literal notranslate"><span class="pre">penalty='l2'</span></code>, while 1 is equivalent to using
<code class="docutils literal notranslate"><span class="pre">penalty='l1'</span></code>. For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;1</span></code>, the penalty is a combination
of L1 and L2.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>A list of class labels known to the classifier.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, )</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficient of the features in the decision function.</p>
<p><cite>coef_</cite> is of shape (1, n_features) when the given problem
is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) or (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (a.k.a. bias) added to the decision function.</p>
<p>If <cite>fit_intercept</cite> is set to False, the intercept is set to zero.
<cite>intercept_</cite> is of shape(1,) when the problem is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,) or (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.Cs_">
<span class="sig-name descname"><span class="pre">Cs_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.Cs_" title="Permalink to this definition">¶</a></dt>
<dd><p>Array of C i.e. inverse of regularization parameter values used
for cross-validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_cs)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.l1_ratios_">
<span class="sig-name descname"><span class="pre">l1_ratios_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.l1_ratios_" title="Permalink to this definition">¶</a></dt>
<dd><p>Array of l1_ratios used for cross-validation. If no l1_ratio is used
(i.e. penalty is not ‘elasticnet’), this is set to <code class="docutils literal notranslate"><span class="pre">[None]</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_l1_ratios)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.coefs_paths_">
<span class="sig-name descname"><span class="pre">coefs_paths_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.coefs_paths_" title="Permalink to this definition">¶</a></dt>
<dd><p>dict with classes as the keys, and the path of coefficients obtained
during cross-validating across each fold and then across each Cs
after doing an OvR for the corresponding class as values.
If the ‘multi_class’ option is set to ‘multinomial’, then
the coefs_paths are the coefficients corresponding to each class.
Each dict value has shape <code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_features)</span></code> or
<code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code> depending on whether the
intercept is fit or not. If <code class="docutils literal notranslate"><span class="pre">penalty='elasticnet'</span></code>, the shape is
<code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_l1_ratios_,</span> <span class="pre">n_features)</span></code> or
<code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_l1_ratios_,</span> <span class="pre">n_features</span> <span class="pre">+</span> <span class="pre">1)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_folds, n_cs, n_features) or                    (n_folds, n_cs, n_features + 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.scores_">
<span class="sig-name descname"><span class="pre">scores_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.scores_" title="Permalink to this definition">¶</a></dt>
<dd><p>dict with classes as the keys, and the values as the
grid of scores obtained during cross-validating each fold, after doing
an OvR for the corresponding class. If the ‘multi_class’ option
given is ‘multinomial’ then the same scores are repeated across
all classes, since this is the multinomial class. Each dict value
has shape <code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs</span></code> or <code class="docutils literal notranslate"><span class="pre">(n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_l1_ratios)</span></code> if
<code class="docutils literal notranslate"><span class="pre">penalty='elasticnet'</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.C_">
<span class="sig-name descname"><span class="pre">C_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.C_" title="Permalink to this definition">¶</a></dt>
<dd><p>Array of C that maps to the best scores across every class. If refit is
set to False, then for each class, the best C is the average of the
C’s that correspond to the best scores for each fold.
<cite>C_</cite> is of shape(n_classes,) when the problem is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or (n_classes - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.l1_ratio_">
<span class="sig-name descname"><span class="pre">l1_ratio_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.l1_ratio_" title="Permalink to this definition">¶</a></dt>
<dd><p>Array of l1_ratio that maps to the best scores across every class. If
refit is set to False, then for each class, the best l1_ratio is the
average of the l1_ratio’s that correspond to the best scores for each
fold.  <cite>l1_ratio_</cite> is of shape(n_classes,) when the problem is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or (n_classes - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations for all classes, folds and Cs.
In the binary or multinomial cases, the first dimension is equal to 1.
If <code class="docutils literal notranslate"><span class="pre">penalty='elasticnet'</span></code>, the shape is <code class="docutils literal notranslate"><span class="pre">(n_classes,</span> <span class="pre">n_folds,</span>
<span class="pre">n_cs,</span> <span class="pre">n_l1_ratios)</span></code> or <code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">n_folds,</span> <span class="pre">n_cs,</span> <span class="pre">n_l1_ratios)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([0, 0])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">2</span><span class="p">,</span> <span class="p">:])</span><span class="o">.</span><span class="n">shape</span>
<span class="go">(2, 3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.98...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model according to the given training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vector, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target vector relative to X.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) </em><em>default=None</em>) – Array of weights that are assigned to individual samples.
If not provided, then each sample is given unit weight.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.LogisticRegressionCV.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.LogisticRegressionCV.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the score using the <cite>scoring</cite> option on the given
test data and labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Test samples.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True labels for X.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – Score of self.predict(X) wrt. y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.ModifiedHuber">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">ModifiedHuber</span></span><a class="headerlink" href="#sklearn.linear_model.ModifiedHuber" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._sgd_fast.Classification</span></code></p>
<p>Modified Huber loss for binary classification with y in {-1, 1}</p>
<p>This is equivalent to quadratically smoothed SVM with gamma = 2.</p>
<p>See T. Zhang ‘Solving Large Scale Linear Prediction Problems Using
Stochastic Gradient Descent’, ICML’04.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">MultiTaskElasticNet</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model._coordinate_descent.Lasso"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.Lasso</span></code></a></p>
<p>Multi-task ElasticNet model trained with L1/L2 mixed-norm as
regularizer.</p>
<p>The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> <span class="n">sum_i</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">sum_j</span> <span class="n">W_ij</span> <span class="o">^</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>i.e. the sum of norms of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Constant that multiplies the L1/L2 term. Defaults to 1.0.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
is an L2 penalty.
For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (W in the cost function formula). If a 1D y is
passed in at fit (non multi-task usage), <code class="docutils literal notranslate"><span class="pre">coef_</span></code> is then a 1D array.
Note that <code class="docutils literal notranslate"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal notranslate"><span class="pre">W</span></code>, <code class="docutils literal notranslate"><span class="pre">W.T</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gaps at the end of the optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.eps_">
<span class="sig-name descname"><span class="pre">eps_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.eps_" title="Permalink to this definition">¶</a></dt>
<dd><p>The tolerance scaled scaled by the variance of the target <cite>y</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.sparse_coef_">
<span class="sig-name descname"><span class="pre">sparse_coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.sparse_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse representation of the <cite>coef_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_features,) or             (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">MultiTaskElasticNet(alpha=0.1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.45663524 0.45612256]</span>
<span class="go"> [0.45663524 0.45612256]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.0872422 0.0872422]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a></dt><dd><p>Multi-task L1/L2 ElasticNet with built-in cross-validation.</p>
</dd>
</dl>
<p><a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNet.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNet.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit MultiTaskElasticNet model with coordinate descent</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Data.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_tasks</em><em>)</em>) – Target. Will be cast to X’s dtype if necessary.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Coordinate descent is an algorithm that considers each column of
data at a time hence it will automatically convert the X input
as a Fortran-contiguous numpy array if necessary.</p>
<p>To avoid memory re-allocation it is advised to allocate the
initial data in memory directly using that format.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">MultiTaskElasticNetCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.LinearModelCV</span></code></p>
<p>Multi-task L1/L2 ElasticNet with built-in cross-validation.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for MultiTaskElasticNet is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>l1_ratio</strong> (<em>float</em><em> or </em><em>list of float</em><em>, </em><em>default=0.5</em>) – The ElasticNet mixing parameter, with 0 &lt; l1_ratio &lt;= 1.
For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it
is an L2 penalty.
For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">l1_ratio</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, the penalty is a combination of L1/L2 and L2.
This parameter can be a list, in which case the different
values are tested by cross-validation and the one giving the best
prediction score is used. Note that a good choice of list of
values for l1_ratio is often to put more values close to 1
(i.e. Lasso) and less close to 0 (i.e. Ridge), as in <code class="docutils literal notranslate"><span class="pre">[.1,</span> <span class="pre">.5,</span> <span class="pre">.7,</span>
<span class="pre">.9,</span> <span class="pre">.95,</span> <span class="pre">.99,</span> <span class="pre">1]</span></code></p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If not provided, set automatically.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=0</em>) – Amount of verbosity.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation. Note that this is
used only if multiple values for l1_ratio are given.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal notranslate"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal notranslate"><span class="pre">W</span></code>, <code class="docutils literal notranslate"><span class="pre">W.T</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>The amount of penalization chosen by cross validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square error for the test set on each fold, varying alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>The grid of alphas used for fitting, for each l1_ratio.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas,) or (n_l1_ratio, n_alphas)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.l1_ratio_">
<span class="sig-name descname"><span class="pre">l1_ratio_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.l1_ratio_" title="Permalink to this definition">¶</a></dt>
<dd><p>Best l1_ratio obtained by cross-validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gap at the end of the optimization for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskElasticNetCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span>
<span class="gp">... </span>        <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="go">MultiTaskElasticNetCV(cv=3)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.52875032 0.46958558]</span>
<span class="go"> [0.52875032 0.46958558]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[0.00166409 0.00166409]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskLassoCV" title="sklearn.linear_model.MultiTaskLassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskLassoCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskElasticNetCV.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskElasticNetCV.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute elastic net path with coordinate descent.</p>
<p>The elastic net optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Number between 0 and 1 passed to elastic net (scaling between
l1 and l2 penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the Lasso.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to False, the input validation checks are skipped (including the
Gram matrix when provided). It is assumed that they are handled
by the caller.</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – Keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.
(Is returned when <code class="docutils literal notranslate"><span class="pre">return_n_iter</span></code> is set to True).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">MultiTaskLasso</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model._coordinate_descent.MultiTaskElasticNet"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.MultiTaskElasticNet</span></code></a></p>
<p>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</p>
<p>The optimization objective for Lasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Constant that multiplies the L1/L2 term. Defaults to 1.0.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal notranslate"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal notranslate"><span class="pre">W</span></code>, <code class="docutils literal notranslate"><span class="pre">W.T</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gaps at the end of the optimization for each alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.eps_">
<span class="sig-name descname"><span class="pre">eps_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.eps_" title="Permalink to this definition">¶</a></dt>
<dd><p>The tolerance scaled scaled by the variance of the target <cite>y</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLasso.sparse_coef_">
<span class="sig-name descname"><span class="pre">sparse_coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLasso.sparse_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Sparse representation of the <cite>coef_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_features,) or             (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">MultiTaskLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="go">MultiTaskLasso(alpha=0.1)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.         0.60809415]</span>
<span class="go">[0.         0.94592424]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[-0.41888636 -0.87382323]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.MultiTaskLasso" title="sklearn.linear_model.MultiTaskLasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskLasso</span></code></a></dt><dd><p>Multi-task L1/L2 Lasso with built-in cross-validation.</p>
</dd>
</dl>
<p><a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">MultiTaskLassoCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">selection</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cyclic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._coordinate_descent.LinearModelCV</span></code></p>
<p>Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>The optimization objective for MultiTaskLasso is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="n">Fro_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If not provided, set automatically.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – The maximum number of iterations.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – The tolerance for the optimization: if the updates are
smaller than <code class="docutils literal notranslate"><span class="pre">tol</span></code>, the optimization code checks the
dual gap for optimality and continues until it is smaller
than <code class="docutils literal notranslate"><span class="pre">tol</span></code>.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>int, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For int/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation. Note that this is
used only if multiple values for l1_ratio are given.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The seed of the pseudo random number generator that selects a random
feature to update. Used when <code class="docutils literal notranslate"><span class="pre">selection</span></code> == ‘random’.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>selection</strong> (<em>{'cyclic'</em><em>, </em><em>'random'}</em><em>, </em><em>default='cyclic'</em>) – If set to ‘random’, a random coefficient is updated every iteration
rather than looping over features sequentially by default. This
(setting to ‘random’) often leads to significantly faster convergence
especially when tol is higher than 1e-4.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (W in the cost function formula).
Note that <code class="docutils literal notranslate"><span class="pre">coef_</span></code> stores the transpose of <code class="docutils literal notranslate"><span class="pre">W</span></code>, <code class="docutils literal notranslate"><span class="pre">W.T</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_tasks, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>The amount of penalization chosen by cross validation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.mse_path_">
<span class="sig-name descname"><span class="pre">mse_path_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.mse_path_" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean square error for the test set on each fold, varying alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas, n_folds)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.alphas_">
<span class="sig-name descname"><span class="pre">alphas_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.alphas_" title="Permalink to this definition">¶</a></dt>
<dd><p>The grid of alphas used for fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_alphas,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations run by the coordinate descent solver to reach
the specified tolerance for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.dual_gap_">
<span class="sig-name descname"><span class="pre">dual_gap_</span></span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.dual_gap_" title="Permalink to this definition">¶</a></dt>
<dd><p>The dual gap at the end of the optimization for the optimal alpha.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">MultiTaskLassoCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_targets</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">MultiTaskLassoCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">0.9994...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">alpha_</span>
<span class="go">0.5713...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([[153.7971...,  94.9015...]])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The algorithm used to fit the model is coordinate descent.</p>
<p>To avoid unnecessary memory duplication the X and y arguments of the fit
method should be directly passed as Fortran-contiguous numpy arrays.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.MultiTaskLassoCV.path">
<em class="property"><span class="pre">static</span> </em><span class="sig-name descname"><span class="pre">path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.MultiTaskLassoCV.path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Lasso path with coordinate descent</p>
<p>The Lasso optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code></p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> alphas are set automatically</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Note that in certain cases, the Lars solver may be significantly
faster to implement this functionality. In particular, linear
interpolation can be used to retrieve model coefficients between the
values output by lars_path</p>
<p class="rubric">Examples</p>
<p>Comparing lasso_path and lars_path with interpolation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use lasso_path to compute a coefficient path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">coef_path</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lasso_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path</span><span class="p">)</span>
<span class="go">[[0.         0.         0.46874778]</span>
<span class="go"> [0.2159048  0.4425765  0.23689075]]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now use lars_path and 1D linear interpolation to compute the</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># same path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alphas</span><span class="p">,</span> <span class="n">active</span><span class="p">,</span> <span class="n">coef_path_lars</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">interpolate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coef_path_continuous</span> <span class="o">=</span> <span class="n">interpolate</span><span class="o">.</span><span class="n">interp1d</span><span class="p">(</span><span class="n">alphas</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                                            <span class="n">coef_path_lars</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path_continuous</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">]))</span>
<span class="go">[[0.         0.         0.46915237]</span>
<span class="go"> [0.2159048  0.4425765  0.23668876]]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">OrthogonalMatchingPursuit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero_coefs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Orthogonal Matching Pursuit model (OMP).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_nonzero_coefs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Desired number of non-zero entries in the solution. If None (by
default) this value is set to 10% of n_features.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=None</em>) – Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em> or </em><em>bool</em><em>, </em><em>default='auto'</em>) – Whether to use a precomputed Gram and Xy matrix to speed up
calculations. Improves performance when <span class="xref std std-term">n_targets</span> or
<span class="xref std std-term">n_samples</span> is very large. Note that if you already have such
matrices, you can pass them directly to the fit method.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the formula).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of active features across every target.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or array-like</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit.n_nonzero_coefs_">
<span class="sig-name descname"><span class="pre">n_nonzero_coefs_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit.n_nonzero_coefs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of non-zero coefficients in the solution. If
<cite>n_nonzero_coefs</cite> is None and <cite>tol</cite> is None this value is either set
to 10% of <cite>n_features</cite> or 1, whichever is greater.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuit</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">OrthogonalMatchingPursuit</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9991...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.3854...])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(<a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf</a>)</p>
<p>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
<a class="reference external" href="https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.orthogonal_mp_gram" title="sklearn.linear_model.orthogonal_mp_gram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp_gram</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code>, <a class="reference internal" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV" title="sklearn.linear_model.OrthogonalMatchingPursuitCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuitCV</span></code></a></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuit.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuit.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model using X, y as training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values. Will be cast to X’s dtype if necessary</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns an instance of self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">OrthogonalMatchingPursuitCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Cross-validated Orthogonal Matching Pursuit model (OMP).</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>copy</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the design matrix X must be copied by the algorithm. A false
value is only helpful if X is already Fortran-ordered, otherwise a
copy is made anyway.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=None</em>) – Maximum numbers of iterations to perform, therefore maximum features
to include. 10% of <code class="docutils literal notranslate"><span class="pre">n_features</span></code> but at least 5 if available.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross-validation,</p></li>
<li><p>integer, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For integer/None inputs, <code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">cv</span></code> default value if None changed from 3-fold to 5-fold.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Sets the verbosity amount.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Parameter vector (w in the problem formulation).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV.n_nonzero_coefs_">
<span class="sig-name descname"><span class="pre">n_nonzero_coefs_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV.n_nonzero_coefs_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated number of non-zero coefficients giving the best mean squared
error over the cross-validation folds.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of active features across every target for the model refit with
the best hyperparameters got by cross-validating across all folds.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or array-like</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">OrthogonalMatchingPursuitCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">noise</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">OrthogonalMatchingPursuitCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9991...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">n_nonzero_coefs_</span>
<span class="go">10</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-78.3854...])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.orthogonal_mp_gram" title="sklearn.linear_model.orthogonal_mp_gram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp_gram</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuit</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LarsCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.OrthogonalMatchingPursuitCV.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.OrthogonalMatchingPursuitCV.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model using X, y as training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values. Will be cast to X’s dtype if necessary.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – returns an instance of self.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">PassiveAggressiveClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hinge'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGDClassifier</span></code></p>
<p>Passive Aggressive Classifier</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Maximum step size (regularization). Defaults to 1.0.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – <p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<a class="reference internal" href="#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit" title="sklearn.linear_model.PassiveAggressiveClassifier.partial_fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code></a> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>default=1e-3</em>) – <p>The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=5</em>) – <p>Number of iterations with no improvement to wait before early stopping.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not the training data should be shuffled after each epoch.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – The verbosity level</p></li>
<li><p><strong>loss</strong> (<em>string</em><em>, </em><em>default=&quot;hinge&quot;</em>) – The loss function to be used:
hinge: equivalent to PA-I in the reference paper.
squared_hinge: equivalent to PA-II in the reference paper.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used to shuffle the training data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>. Pass an int for reproducible output across multiple
function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.</p>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>{class_label: weight}</em><em> or </em><em>&quot;balanced&quot;</em><em> or </em><em>None</em><em>,             </em><em>default=None</em>) – <p>Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>parameter <em>class_weight</em> to automatically weight samples.</p>
</div>
</p></li>
<li><p><strong>average</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – <p>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So average=10 will begin averaging after seeing 10 samples.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>parameter <em>average</em> to use weights averaging in SGD</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1] if n_classes == 2 else [n_classes]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations to reach the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The unique classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.loss_function_">
<span class="sig-name descname"><span class="pre">loss_function_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.loss_function_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss function used by the algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">PassiveAggressiveClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">PassiveAggressiveClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">PassiveAggressiveClassifier(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[[0.26642044 0.45070924 0.67251877 0.64185414]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[1.84127814]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Perceptron</span></code></a></p>
</div>
<p class="rubric">References</p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model with Passive Aggressive algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values</p></li>
<li><p><strong>coef_init</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_classes</em><em>,</em><em>n_features</em><em>]</em>) – The initial coefficients to warm-start the optimization.</p></li>
<li><p><strong>intercept_init</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_classes</em><em>]</em>) – The initial intercept to warm-start the optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveClassifier.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">classes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveClassifier.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model with Passive Aggressive algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Subset of the training data</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Subset of the target values</p></li>
<li><p><strong>classes</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_classes</em><em>]</em>) – Classes across all calls to partial_fit.
Can be obtained by via <cite>np.unique(y_all)</cite>, where y_all is the
target vector of the entire dataset.
This argument is required for the first call to partial_fit
and can be omitted in the subsequent calls.
Note that y doesn’t need to contain all labels in <cite>classes</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">PassiveAggressiveRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">C</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'epsilon_insensitive'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGDRegressor</span></code></p>
<p>Passive Aggressive Regressor</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>C</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Maximum step size (regularization). Defaults to 1.0.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered. Defaults to True.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – <p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<a class="reference internal" href="#sklearn.linear_model.PassiveAggressiveRegressor.partial_fit" title="sklearn.linear_model.PassiveAggressiveRegressor.partial_fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code></a> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>default=1e-3</em>) – <p>The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate
training when validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=5</em>) – <p>Number of iterations with no improvement to wait before early stopping.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not the training data should be shuffled after each epoch.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>default=0</em>) – The verbosity level</p></li>
<li><p><strong>loss</strong> (<em>string</em><em>, </em><em>default=&quot;epsilon_insensitive&quot;</em>) – The loss function to be used:
epsilon_insensitive: equivalent to PA-I in the reference paper.
squared_epsilon_insensitive: equivalent to PA-II in the reference
paper.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – If the difference between the current prediction and the correct label
is below this threshold, the model is not updated.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used to shuffle the training data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>. Pass an int for reproducible output across multiple
function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.</p>
</p></li>
<li><p><strong>average</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – <p>When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So average=10 will begin averaging after seeing 10 samples.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>parameter <em>average</em> to use weights averaging in SGD</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = [1] if n_classes == 2 else [n_classes]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations to reach the stopping criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">PassiveAggressiveRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">PassiveAggressiveRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">PassiveAggressiveRegressor(max_iter=100, random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="go">[20.48736655 34.18818427 67.59122734 87.94731329]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
<span class="go">[-0.02306214]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-0.02306214]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a></p>
</div>
<p class="rubric">References</p>
<p>Online Passive-Aggressive Algorithms
&lt;<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf">http://jmlr.csail.mit.edu/papers/volume7/crammer06a/crammer06a.pdf</a>&gt;
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR (2006)</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">intercept_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model with Passive Aggressive algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Target values</p></li>
<li><p><strong>coef_init</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_features</em><em>]</em>) – The initial coefficients to warm-start the optimization.</p></li>
<li><p><strong>intercept_init</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>1</em><em>]</em>) – The initial intercept to warm-start the optimization.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.PassiveAggressiveRegressor.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PassiveAggressiveRegressor.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model with Passive Aggressive algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Subset of training data</p></li>
<li><p><strong>y</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>]</em>) – Subset of target values</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Perceptron</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Perceptron" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGDClassifier</span></code></p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>penalty</strong> (<em>{'l2'</em><em>,</em><em>'l1'</em><em>,</em><em>'elasticnet'}</em><em>, </em><em>default=None</em>) – The penalty (aka regularization term) to be used.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – Constant that multiplies the regularization term if regularization is
used.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.15</em>) – <p>The Elastic Net mixing parameter, with <cite>0 &lt;= l1_ratio &lt;= 1</cite>.
<cite>l1_ratio=0</cite> corresponds to L2 penalty, <cite>l1_ratio=1</cite> to L1.
Only used if <cite>penalty=’elasticnet’</cite>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – <p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – <p>The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol).</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not the training data should be shuffled after each epoch.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level</p></li>
<li><p><strong>eta0</strong> (<em>double</em><em>, </em><em>default=1</em>) – Constant by which the updates are multiplied.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used to shuffle the training data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">True</span></code>. Pass an int for reproducible output across multiple
function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether to use early stopping to terminate training when validation.
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score is not improving by at least tol for
n_iter_no_change consecutive epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=5</em>) – <p>Number of iterations with no improvement to wait before early stopping.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>{class_label: weight}</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution. See
<span class="xref std std-term">the Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The unique classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,) if n_classes == 2 else (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.loss_function_">
<span class="sig-name descname"><span class="pre">loss_function_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.loss_function_" title="Permalink to this definition">¶</a></dt>
<dd><p>The function that determines the loss, or difference between the
output of the algorithm and the target values.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>concrete LossFunction</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations to reach the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Perceptron.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.Perceptron.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p><code class="docutils literal notranslate"><span class="pre">Perceptron</span></code> is a classification algorithm which shares the same
underlying implementation with <code class="docutils literal notranslate"><span class="pre">SGDClassifier</span></code>. In fact,
<code class="docutils literal notranslate"><span class="pre">Perceptron()</span></code> is equivalent to <cite>SGDClassifier(loss=”perceptron”,
eta0=1, learning_rate=”constant”, penalty=None)</cite>.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_digits</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Perceptron</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Perceptron</span><span class="p">(</span><span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Perceptron()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.939...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">SGDClassifier</span></code></a></p>
</div>
<p class="rubric">References</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">https://en.wikipedia.org/wiki/Perceptron</a> and references therein.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.PoissonRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">PoissonRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.PoissonRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._glm.glm.GeneralizedLinearRegressor</span></code></p>
<p>Generalized Linear Model with a Poisson distribution.</p>
<p>This regressor uses the ‘log’ link function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1</em>) – Constant that multiplies the penalty term and thus determines the
regularization strength. <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to unpenalized
GLMs. In this case, the design matrix <cite>X</cite> must have full column rank
(no collinearities).</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Specifies if a constant (a.k.a. bias or intercept) should be
added to the linear predictor (X &#64; coef + intercept).</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – The maximal number of iterations for the solver.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Stopping criterion. For the lbfgs solver,
the iteration will stop when <code class="docutils literal notranslate"><span class="pre">max{|g_j|,</span> <span class="pre">j</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">d}</span> <span class="pre">&lt;=</span> <span class="pre">tol</span></code>
where <code class="docutils literal notranslate"><span class="pre">g_j</span></code> is the j-th component of the gradient (derivative) of
the objective function.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to <code class="docutils literal notranslate"><span class="pre">fit</span></code>
as initialization for <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> .</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – For the lbfgs solver set verbose to any positive number for verbosity.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PoissonRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.PoissonRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated coefficients for the linear predictor (<cite>X &#64; coef_ +
intercept_</cite>) in the GLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PoissonRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.PoissonRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.PoissonRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.PoissonRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations used in the solver.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">PoissonRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">21</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">PoissonRegressor()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.990...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.121..., 0.158...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>
<span class="go">2.088...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([10.676..., 21.875...])</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.PoissonRegressor.family">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">family</span></span><a class="headerlink" href="#sklearn.linear_model.PoissonRegressor.family" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">RANSACRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">residual_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_data_valid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_model_valid</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_trials</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_skips</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_n_inliers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">inf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stop_probability</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.99</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'absolute_loss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MetaEstimatorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>RANSAC (RANdom SAmple Consensus) algorithm.</p>
<p>RANSAC is an iterative algorithm for the robust estimation of parameters
from a subset of inliers from the complete data set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – <p>Base estimator object which implements the following methods:</p>
<blockquote>
<div><ul>
<li><p><cite>fit(X, y)</cite>: Fit model to given training data and target values.</p></li>
<li><p><cite>score(X, y)</cite>: Returns the mean accuracy on the given test data,
which is used for the stop criterion defined by <cite>stop_score</cite>.
Additionally, the score is used to decide which of two equally
large consensus sets is chosen as the better one.</p></li>
<li><p><cite>predict(X)</cite>: Returns predicted values using the linear model,
which is used to compute residual error using loss function.</p></li>
</ul>
</div></blockquote>
<p>If <cite>base_estimator</cite> is None, then
<a class="reference internal" href="#sklearn.linear_model.LinearRegression" title="sklearn.linear_model.LinearRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearRegression</span></code></a> is used for
target values of dtype float.</p>
<p>Note that the current implementation only supports regression
estimators.</p>
</p></li>
<li><p><strong>min_samples</strong> (<em>int</em><em> (</em><em>&gt;= 1</em><em>) or </em><em>float</em><em> (</em><em>[</em><em>0</em><em>, </em><em>1</em><em>]</em><em>)</em><em>, </em><em>default=None</em>) – Minimum number of samples chosen randomly from original data. Treated
as an absolute number of samples for <cite>min_samples &gt;= 1</cite>, treated as a
relative number <cite>ceil(min_samples * X.shape[0]</cite>) for
<cite>min_samples &lt; 1</cite>. This is typically chosen as the minimal number of
samples necessary to estimate the given <cite>base_estimator</cite>. By default a
<code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.LinearRegression()</span></code> estimator is assumed and
<cite>min_samples</cite> is chosen as <code class="docutils literal notranslate"><span class="pre">X.shape[1]</span> <span class="pre">+</span> <span class="pre">1</span></code>.</p></li>
<li><p><strong>residual_threshold</strong> (<em>float</em><em>, </em><em>default=None</em>) – Maximum residual for a data sample to be classified as an inlier.
By default the threshold is chosen as the MAD (median absolute
deviation) of the target values <cite>y</cite>.</p></li>
<li><p><strong>is_data_valid</strong> (<em>callable</em><em>, </em><em>default=None</em>) – This function is called with the randomly selected data before the
model is fitted to it: <cite>is_data_valid(X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.</p></li>
<li><p><strong>is_model_valid</strong> (<em>callable</em><em>, </em><em>default=None</em>) – This function is called with the estimated model and the randomly
selected data: <cite>is_model_valid(model, X, y)</cite>. If its return value is
False the current randomly chosen sub-sample is skipped.
Rejecting samples with this function is computationally costlier than
with <cite>is_data_valid</cite>. <cite>is_model_valid</cite> should therefore only be used if
the estimated model is needed for making the rejection decision.</p></li>
<li><p><strong>max_trials</strong> (<em>int</em><em>, </em><em>default=100</em>) – Maximum number of iterations for random sample selection.</p></li>
<li><p><strong>max_skips</strong> (<em>int</em><em>, </em><em>default=np.inf</em>) – <p>Maximum number of iterations that can be skipped due to finding zero
inliers or invalid data defined by <code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code> or invalid models
defined by <code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>stop_n_inliers</strong> (<em>int</em><em>, </em><em>default=np.inf</em>) – Stop iteration if at least this number of inliers are found.</p></li>
<li><p><strong>stop_score</strong> (<em>float</em><em>, </em><em>default=np.inf</em>) – Stop iteration if score is greater equal than this threshold.</p></li>
<li><p><strong>stop_probability</strong> (<em>float in range</em><em> [</em><em>0</em><em>, </em><em>1</em><em>]</em><em>, </em><em>default=0.99</em>) – <p>RANSAC iteration stops if at least one outlier-free set of the training
data is sampled in RANSAC. This requires to generate at least N
samples (iterations):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">&gt;=</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">probability</span><span class="p">)</span> <span class="o">/</span> <span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">e</span><span class="o">**</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
<p>where the probability (confidence) is typically set to high value such
as 0.99 (the default) and e is the current fraction of inliers w.r.t.
the total number of samples.</p>
</p></li>
<li><p><strong>loss</strong> (<em>string</em><em>, </em><em>callable</em><em>, </em><em>default='absolute_loss'</em>) – <p>String inputs, ‘absolute_loss’ and ‘squared_error’ are supported which
find the absolute loss and squared error per sample respectively.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">loss</span></code> is a callable, then it should be a function that takes
two arrays as inputs, the true and predicted value and returns a 1-D
array with the i-th value of the array corresponding to the loss
on <code class="docutils literal notranslate"><span class="pre">X[i]</span></code>.</p>
<p>If the loss on a sample is greater than the <code class="docutils literal notranslate"><span class="pre">residual_threshold</span></code>,
then this sample is classified as an outlier.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The loss ‘squared_loss’ was deprecated in v1.0 and will be removed
in version 1.2. Use <cite>loss=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – The generator used to initialize the centers.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.estimator_">
<span class="sig-name descname"><span class="pre">estimator_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>Best fitted model (copy of the <cite>base_estimator</cite> object).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.n_trials_">
<span class="sig-name descname"><span class="pre">n_trials_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.n_trials_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of random selection trials until one of the stop criteria is
met. It is always <code class="docutils literal notranslate"><span class="pre">&lt;=</span> <span class="pre">max_trials</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.inlier_mask_">
<span class="sig-name descname"><span class="pre">inlier_mask_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.inlier_mask_" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean mask of inliers classified as <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool array of shape [n_samples]</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.n_skips_no_inliers_">
<span class="sig-name descname"><span class="pre">n_skips_no_inliers_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.n_skips_no_inliers_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations skipped due to finding zero inliers.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.n_skips_invalid_data_">
<span class="sig-name descname"><span class="pre">n_skips_invalid_data_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.n_skips_invalid_data_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations skipped due to invalid data defined by
<code class="docutils literal notranslate"><span class="pre">is_data_valid</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.n_skips_invalid_model_">
<span class="sig-name descname"><span class="pre">n_skips_invalid_model_</span></span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.n_skips_invalid_model_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations skipped due to an invalid model defined by
<code class="docutils literal notranslate"><span class="pre">is_model_valid</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RANSACRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">RANSACRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9885...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-31.9417...])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/RANSAC">https://en.wikipedia.org/wiki/RANSAC</a></p>
</dd>
<dt class="label" id="id4"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://www.sri.com/sites/default/files/publications/ransac-publication.pdf">https://www.sri.com/sites/default/files/publications/ransac-publication.pdf</a></p>
</dd>
<dt class="label" id="id5"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf">http://www.bmva.org/bmvc/2009/Papers/Paper355/Paper355.pdf</a></p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit estimator using RANSAC algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like</em><em> or </em><em>sparse matrix</em><em>, </em><em>shape</em><em> [</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Individual weights for each sample
raises error if sample_weight is passed and base_estimator
fit method does not support it.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>ValueError</strong> – If no valid consensus set could be found. This occurs if
    <cite>is_data_valid</cite> and <cite>is_model_valid</cite> return False for all
    <cite>max_trials</cite> randomly chosen sub-samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the estimated model.</p>
<p>This is a wrapper for <cite>estimator_.predict(X)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>numpy array of shape</em><em> [</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – </p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Returns predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array, shape = [n_samples] or [n_samples, n_targets]</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.RANSACRegressor.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RANSACRegressor.score" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the score of the prediction.</p>
<p>This is a wrapper for <cite>estimator_.score(X, y)</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>numpy array</em><em> or </em><em>sparse matrix of shape</em><em> [</em><em>n_samples</em><em>, </em><em>n_features</em><em>]</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>] or </em><em>[</em><em>n_samples</em><em>, </em><em>n_targets</em><em>]</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>z</strong> – Score of the prediction.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.Ridge">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">Ridge</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Ridge" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._ridge._BaseRidge</span></code></p>
<p>Linear least squares with l2 regularization.</p>
<p>Minimizes the objective function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>This model solves a regression model where the loss function is
the linear least squares function and regularization is given by
the l2-norm. Also known as Ridge Regression or Tikhonov regularization.
This estimator has built-in support for multi-variate regression
(i.e., when y is a 2d-array of shape (n_samples, n_targets)).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>{float</em><em>, </em><em>ndarray of shape</em><em> (</em><em>n_targets</em><em>,</em><em>)</em><em>}</em><em>, </em><em>default=1.0</em>) – Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(2C)</span></code> in other linear models such as
<a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> or
<a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to fit the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><code class="docutils literal notranslate"><span class="pre">normalize</span></code> was deprecated in version 1.0 and
will be removed in 1.2.</p>
</div>
</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=None</em>) – Maximum number of iterations for conjugate gradient solver.
For ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined
by scipy.sparse.linalg. For ‘sag’ solver, the default value is 1000.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Precision of the solution.</p></li>
<li><p><strong>solver</strong> (<em>{'auto'</em><em>, </em><em>'svd'</em><em>, </em><em>'cholesky'</em><em>, </em><em>'lsqr'</em><em>, </em><em>'sparse_cg'</em><em>, </em><em>'sag'</em><em>, </em><em>'saga'}</em><em>,         </em><em>default='auto'</em>) – <p>Solver to use in the computational routines:</p>
<ul>
<li><p>’auto’ chooses the solver automatically based on the type of data.</p></li>
<li><p>’svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than ‘cholesky’.</p></li>
<li><p>’cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</p></li>
<li><p>’sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</p></li>
<li><p>’lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.</p></li>
<li><p>’sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its improved, unbiased version named SAGA. Both methods also use an
iterative procedure, and are often faster than other solvers when
both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</p></li>
</ul>
<p>All last five solvers support both dense and sparse data. However, only
‘sag’ and ‘sparse_cg’ supports sparse input when <cite>fit_intercept</cite> is
True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p>
</div>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – <p>Used when <code class="docutils literal notranslate"><span class="pre">solver</span></code> == ‘sag’ or ‘saga’ to shuffle the data.
See <span class="xref std std-term">Glossary</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><cite>random_state</cite> to support Stochastic Average Gradient.</p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Ridge.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.Ridge.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight vector(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Ridge.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.Ridge.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.Ridge.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.Ridge.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>None or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeClassifier</span></code></a></dt><dd><p>Ridge classifier.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeCV</span></code></a></dt><dd><p>Ridge regression with built-in cross validation.</p>
</dd>
<dt><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelRidge</span></code></dt><dd><p>Kernel ridge regression combines ridge regression with the kernel trick.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Ridge()</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.Ridge.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.Ridge.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit Ridge regression model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values</p></li>
<li><p><strong>sample_weight</strong> (<em>float</em><em> or </em><em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Individual weights for each sample. If given a float, every sample
will have the same weight.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">RidgeCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.1,</span> <span class="pre">1.0,</span> <span class="pre">10.0)</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gcv_mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_cv_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_per_target</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RidgeCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._ridge._BaseRidgeCV</span></code></p>
<p>Ridge regression with built-in cross-validation.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>By default, it performs efficient Leave-One-Out Cross-Validation.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape</em><em> (</em><em>n_alphas</em><em>,</em><em>)</em><em>, </em><em>default=</em><em>(</em><em>0.1</em><em>, </em><em>1.0</em><em>, </em><em>10.0</em><em>)</em>) – Array of alpha values to try.
Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(2C)</span></code> in other linear models such as
<a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> or
<a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>.
If using Leave-One-Out cross-validation, alphas must be positive.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><code class="docutils literal notranslate"><span class="pre">normalize</span></code> was deprecated in version 1.0 and will be removed in
1.2.</p>
</div>
</p></li>
<li><p><strong>scoring</strong> (<em>string</em><em>, </em><em>callable</em><em>, </em><em>default=None</em>) – A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.
If None, the negative mean squared error if cv is ‘auto’ or None
(i.e. when using leave-one-out cross-validation), and r2 score
otherwise.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the efficient Leave-One-Out cross-validation</p></li>
<li><p>integer, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>For integer/None inputs, if <code class="docutils literal notranslate"><span class="pre">y</span></code> is binary or multiclass,
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code></a> is used, else,
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code></a> is used.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</p></li>
<li><p><strong>gcv_mode</strong> (<em>{'auto'</em><em>, </em><em>'svd'</em><em>, </em><em>eigen'}</em><em>, </em><em>default='auto'</em>) – <p>Flag indicating which strategy to use when performing
Leave-One-Out Cross-Validation. Options are:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;auto&#39;</span> <span class="p">:</span> <span class="n">use</span> <span class="s1">&#39;svd&#39;</span> <span class="k">if</span> <span class="n">n_samples</span> <span class="o">&gt;</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">otherwise</span> <span class="n">use</span> <span class="s1">&#39;eigen&#39;</span>
<span class="s1">&#39;svd&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">use</span> <span class="n">of</span> <span class="n">singular</span> <span class="n">value</span> <span class="n">decomposition</span> <span class="n">of</span> <span class="n">X</span> <span class="n">when</span> <span class="n">X</span> <span class="ow">is</span>
    <span class="n">dense</span><span class="p">,</span> <span class="n">eigenvalue</span> <span class="n">decomposition</span> <span class="n">of</span> <span class="n">X</span><span class="o">^</span><span class="n">T</span><span class="o">.</span><span class="n">X</span> <span class="n">when</span> <span class="n">X</span> <span class="ow">is</span> <span class="n">sparse</span><span class="o">.</span>
<span class="s1">&#39;eigen&#39;</span> <span class="p">:</span> <span class="n">force</span> <span class="n">computation</span> <span class="n">via</span> <span class="n">eigendecomposition</span> <span class="n">of</span> <span class="n">X</span><span class="o">.</span><span class="n">X</span><span class="o">^</span><span class="n">T</span>
</pre></div>
</div>
<p>The ‘auto’ mode is the default and is intended to pick the cheaper
option of the two depending on the shape of the training data.</p>
</p></li>
<li><p><strong>store_cv_values</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Flag indicating if the cross-validation values corresponding to
each alpha should be stored in the <code class="docutils literal notranslate"><span class="pre">cv_values_</span></code> attribute (see
below). This flag is only compatible with <code class="docutils literal notranslate"><span class="pre">cv=None</span></code> (i.e. using
Leave-One-Out Cross-Validation).</p></li>
<li><p><strong>alpha_per_target</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Flag indicating whether to optimize the alpha value (picked from the
<cite>alphas</cite> parameter list) for each target separately (for multi-output
settings: multiple prediction targets). When set to <cite>True</cite>, after
fitting, the <cite>alpha_</cite> attribute will contain a value for each target.
When set to <cite>False</cite>, a single alpha is used for all targets.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV.cv_values_">
<span class="sig-name descname"><span class="pre">cv_values_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeCV.cv_values_" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validation values for each alpha (only available if
<code class="docutils literal notranslate"><span class="pre">store_cv_values=True</span></code> and <code class="docutils literal notranslate"><span class="pre">cv=None</span></code>). After <code class="docutils literal notranslate"><span class="pre">fit()</span></code> has been
called, this attribute will contain the mean squared errors
(by default) or the values of the <code class="docutils literal notranslate"><span class="pre">{loss,score}_func</span></code> function
(if provided in the constructor).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_alphas) or         shape (n_samples, n_targets, n_alphas), optional</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight vector(s).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated regularization parameter, or, if <code class="docutils literal notranslate"><span class="pre">alpha_per_target=True</span></code>,
the estimated regularization parameter for each target.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeCV.best_score_">
<span class="sig-name descname"><span class="pre">best_score_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeCV.best_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of base estimator with best alpha, or, if
<code class="docutils literal notranslate"><span class="pre">alpha_per_target=True</span></code>, a score for each target.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.5166...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a></dt><dd><p>Ridge regression.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeClassifier</span></code></a></dt><dd><p>Ridge classifier.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV" title="sklearn.linear_model.RidgeClassifierCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeClassifierCV</span></code></a></dt><dd><p>Ridge classifier with built-in cross validation.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">RidgeClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._ridge._BaseRidge</span></code></p>
<p>Classifier using Ridge regression.</p>
<p>This classifier first converts the target values into <code class="docutils literal notranslate"><span class="pre">{-1,</span> <span class="pre">1}</span></code> and
then treats the problem as a regression task (multi-output regression in
the multiclass case).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(2C)</span></code> in other linear models such as
<a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> or
<a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set to false, no
intercept will be used in calculations (e.g. data is expected to be
already centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><code class="docutils literal notranslate"><span class="pre">normalize</span></code> was deprecated in version 1.0 and
will be removed in 1.2.</p>
</div>
</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=None</em>) – Maximum number of iterations for conjugate gradient solver.
The default value is determined by scipy.sparse.linalg.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Precision of the solution.</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
</p></li>
<li><p><strong>solver</strong> (<em>{'auto'</em><em>, </em><em>'svd'</em><em>, </em><em>'cholesky'</em><em>, </em><em>'lsqr'</em><em>, </em><em>'sparse_cg'</em><em>, </em><em>'sag'</em><em>, </em><em>'saga'}</em><em>,         </em><em>default='auto'</em>) – <p>Solver to use in the computational routines:</p>
<ul>
<li><p>’auto’ chooses the solver automatically based on the type of data.</p></li>
<li><p>’svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than ‘cholesky’.</p></li>
<li><p>’cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution.</p></li>
<li><p>’sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</p></li>
<li><p>’lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.</p></li>
<li><p>’sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its unbiased and more flexible version named SAGA. Both methods
use an iterative procedure, and are often faster than other solvers
when both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19: </span>SAGA solver.</p>
</div>
</li>
</ul>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used when <code class="docutils literal notranslate"><span class="pre">solver</span></code> == ‘sag’ or ‘saga’ to shuffle the data.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficient of the features in the decision function.</p>
<p><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is of shape (1, n_features) when the given problem is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) or (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations for each target. Available only for
sag and lsqr solvers. Other solvers will return None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>None or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a></dt><dd><p>Ridge regression.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeClassifierCV" title="sklearn.linear_model.RidgeClassifierCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeClassifierCV</span></code></a></dt><dd><p>Ridge classifier with built-in cross validation.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9595...</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="id6">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#id6" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit Ridge classifier model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>float</em><em> or </em><em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Individual weights for each sample. If given a float, every sample
will have the same weight.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>sample_weight</em> support to Classifier.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Instance of the estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">RidgeClassifierCV</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(0.1,</span> <span class="pre">1.0,</span> <span class="pre">10.0)</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">store_cv_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._ridge._BaseRidgeCV</span></code></p>
<p>Ridge classifier with built-in cross-validation.</p>
<p>See glossary entry for <span class="xref std std-term">cross-validation estimator</span>.</p>
<p>By default, it performs Leave-One-Out Cross-Validation. Currently,
only the n_features &gt; n_samples case is handled efficiently.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape</em><em> (</em><em>n_alphas</em><em>,</em><em>)</em><em>, </em><em>default=</em><em>(</em><em>0.1</em><em>, </em><em>1.0</em><em>, </em><em>10.0</em><em>)</em>) – Array of alpha values to try.
Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(2C)</span></code> in other linear models such as
<a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> or
<a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations
(i.e. data is expected to be centered).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter is ignored when <code class="docutils literal notranslate"><span class="pre">fit_intercept</span></code> is set to False.
If True, the regressors X will be normalized before regression by
subtracting the mean and dividing by the l2-norm.
If you wish to standardize, please use
<a class="reference internal" href="sklearn.preprocessing.html#sklearn.preprocessing.StandardScaler" title="sklearn.preprocessing.StandardScaler"><code class="xref py py-class docutils literal notranslate"><span class="pre">StandardScaler</span></code></a> before calling <code class="docutils literal notranslate"><span class="pre">fit</span></code>
on an estimator with <code class="docutils literal notranslate"><span class="pre">normalize=False</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><code class="docutils literal notranslate"><span class="pre">normalize</span></code> was deprecated in version 1.0 and
will be removed in 1.2.</p>
</div>
</p></li>
<li><p><strong>scoring</strong> (<em>string</em><em>, </em><em>callable</em><em>, </em><em>default=None</em>) – A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy.
Possible inputs for cv are:</p>
<ul>
<li><p>None, to use the efficient Leave-One-Out cross-validation</p></li>
<li><p>integer, to specify the number of folds.</p></li>
<li><p><span class="xref std std-term">CV splitter</span>,</p></li>
<li><p>An iterable yielding (train, test) splits as arrays of indices.</p></li>
</ul>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em> or </em><em>'balanced'</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
</p></li>
<li><p><strong>store_cv_values</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Flag indicating if the cross-validation values corresponding to
each alpha should be stored in the <code class="docutils literal notranslate"><span class="pre">cv_values_</span></code> attribute (see
below). This flag is only compatible with <code class="docutils literal notranslate"><span class="pre">cv=None</span></code> (i.e. using
Leave-One-Out Cross-Validation).</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.cv_values_">
<span class="sig-name descname"><span class="pre">cv_values_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.cv_values_" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross-validation values for each alpha (if <code class="docutils literal notranslate"><span class="pre">store_cv_values=True</span></code> and
<code class="docutils literal notranslate"><span class="pre">cv=None</span></code>). After <code class="docutils literal notranslate"><span class="pre">fit()</span></code> has been called, this attribute will
contain the mean squared errors (by default) or the values of the
<code class="docutils literal notranslate"><span class="pre">{loss,score}_func</span></code> function (if provided in the constructor). This
attribute exists only when <code class="docutils literal notranslate"><span class="pre">store_cv_values</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_targets, n_alphas), optional</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficient of the features in the decision function.</p>
<p><code class="docutils literal notranslate"><span class="pre">coef_</span></code> is of shape (1, n_features) when the given problem is binary.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) or (n_targets, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Independent term in decision function. Set to 0.0 if
<code class="docutils literal notranslate"><span class="pre">fit_intercept</span> <span class="pre">=</span> <span class="pre">False</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float or ndarray of shape (n_targets,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.alpha_">
<span class="sig-name descname"><span class="pre">alpha_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.alpha_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated regularization parameter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.best_score_">
<span class="sig-name descname"><span class="pre">best_score_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.best_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of base estimator with best alpha.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeClassifierCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifierCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9630...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a></dt><dd><p>Ridge regression.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeClassifier" title="sklearn.linear_model.RidgeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeClassifier</span></code></a></dt><dd><p>Ridge classifier.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RidgeCV</span></code></a></dt><dd><p>Ridge regression with built-in cross validation.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>For multi-class classification, n_class classifiers are trained in
a one-versus-all approach. Concretely, this is implemented by taking
advantage of the multi-variate response support in Ridge.</p>
<dl class="py property">
<dt class="sig sig-object py" id="id7">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#id7" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.RidgeClassifierCV.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.RidgeClassifierCV.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit Ridge classifier with cv.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples
and n_features is the number of features. When using GCV,
will be cast to float64 if necessary.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values. Will be cast to X’s dtype if necessary.</p></li>
<li><p><strong>sample_weight</strong> (<em>float</em><em> or </em><em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Individual weights for each sample. If given a float, every sample
will have the same weight.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">SGDClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hinge'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'optimal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGDClassifier</span></code></p>
<p>Linear classifiers (SVM, logistic regression, etc.) with SGD training.</p>
<p>This estimator implements regularized linear models with stochastic
gradient descent (SGD) learning: the gradient of the loss is estimated
each sample at a time and the model is updated along the way with a
decreasing strength schedule (aka learning rate). SGD allows minibatch
(online/out-of-core) learning via the <cite>partial_fit</cite> method.
For best results using the default learning rate schedule, the data should
have zero mean and unit variance.</p>
<p>This implementation works with data represented as dense or sparse arrays
of floating point values for the features. The model it fits can be
controlled with the loss parameter; by default, it fits a linear support
vector machine (SVM).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>str</em><em>, </em><em>default='hinge'</em>) – <p>The loss function to be used. Defaults to ‘hinge’, which gives a
linear SVM.</p>
<p>The possible options are ‘hinge’, ‘log’, ‘modified_huber’,
‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_error’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.</p>
<p>The ‘log’ loss gives logistic regression, a probabilistic classifier.
‘modified_huber’ is another smooth loss that brings tolerance to
outliers as well as probability estimates.
‘squared_hinge’ is like hinge but is quadratically penalized.
‘perceptron’ is the linear loss used by the perceptron algorithm.
The other losses are designed for regression but can be useful in
classification as well; see
<a class="reference internal" href="#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> for a description.</p>
<p>More details about the losses formulas can be found in the
<span class="xref std std-ref">User Guide</span>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The loss ‘squared_loss’ was deprecated in v1.0 and will be removed
in version 1.2. Use <cite>loss=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>penalty</strong> (<em>{'l2'</em><em>, </em><em>'l1'</em><em>, </em><em>'elasticnet'}</em><em>, </em><em>default='l2'</em>) – The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – Constant that multiplies the regularization term. The higher the
value, the stronger the regularization.
Also used to compute the learning rate when set to <cite>learning_rate</cite> is
set to ‘optimal’.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.15</em>) – The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if <cite>penalty</cite> is ‘elasticnet’.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – <p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – <p>The stopping criterion. If it is not None, training will stop
when (loss &gt; best_loss - tol) for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the <cite>early_stopping</cite> parameter.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not the training data should be shuffled after each epoch.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of CPUs to use to do the OVA (One Versus All, for
multi-class problems) computation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used for shuffling the data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>learning_rate</strong> (<em>str</em><em>, </em><em>default='optimal'</em>) – <p>The learning rate schedule:</p>
<ul>
<li><p>’constant’: <cite>eta = eta0</cite></p></li>
<li><p>’optimal’: <cite>eta = 1.0 / (alpha * (t + t0))</cite>
where t0 is chosen by a heuristic proposed by Leon Bottou.</p></li>
<li><p>’invscaling’: <cite>eta = eta0 / pow(t, power_t)</cite></p></li>
<li><p>’adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘adaptive’ option</p>
</div>
</div></blockquote>
</li>
</ul>
</p></li>
<li><p><strong>eta0</strong> (<em>double</em><em>, </em><em>default=0.0</em>) – The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.</p></li>
<li><p><strong>power_t</strong> (<em>double</em><em>, </em><em>default=0.5</em>) – The exponent for inverse scaling learning rate [default 0.5].</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a stratified fraction of training data as validation and terminate
training when validation score returned by the <cite>score</cite> method is not
improving by at least tol for n_iter_no_change consecutive epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘early_stopping’ option</p>
</div>
</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <cite>early_stopping</cite> is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘validation_fraction’ option</p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=5</em>) – <p>Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the <cite>early_stopping</cite> parameter.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘n_iter_no_change’ option</p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>{class_label: weight}</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Preset for the class_weight fit parameter.</p>
<p>Weights associated with classes. If not given, all classes
are supposed to have weight one.</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> will result in increasing the
existing counter.</p>
</p></li>
<li><p><strong>average</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – When set to True, computes the averaged SGD weights accross all
updates and stores the result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches <cite>average</cite>. So <code class="docutils literal notranslate"><span class="pre">average=10</span></code> will begin
averaging after seeing 10 samples.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Constants in decision function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,) if n_classes == 2 else (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations before reaching the stopping criterion.
For multiclass fits, it is the maximum over every binary fit.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.loss_function_">
<span class="sig-name descname"><span class="pre">loss_function_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.loss_function_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.LinearSVC</span></code></a></dt><dd><p>Linear support vector classification.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a></dt><dd><p>Logistic regression.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Perceptron</span></code></a></dt><dd><p>Inherits from SGDClassifier. <code class="docutils literal notranslate"><span class="pre">Perceptron()</span></code> is equivalent to <code class="docutils literal notranslate"><span class="pre">SGDClassifier(loss=&quot;perceptron&quot;,</span> <span class="pre">eta0=1,</span> <span class="pre">learning_rate=&quot;constant&quot;,</span> <span class="pre">penalty=None)</span></code>.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Always scale the input. The most convenient way is to use a pipeline.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                    <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;sgdclassifier&#39;, SGDClassifier())])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.predict_log_proba">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">predict_log_proba</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Log of probability estimates.</p>
<p>This method is only available for log loss and modified Huber loss.</p>
<p>When loss=”modified_huber”, probability estimates may be hard zeros
and ones, so taking the logarithm is not possible.</p>
<p>See <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data for prediction.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>T</strong> – Returns the log-probability of the sample for each class in the
model, where classes are ordered as they are in
<cite>self.classes_</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like, shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.predict_proba">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">predict_proba</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Probability estimates.</p>
<p>This method is only available for log loss and modified Huber loss.</p>
<p>Multiclass probability estimates are derived from binary (one-vs.-rest)
estimates by simple normalization, as recommended by Zadrozny and
Elkan.</p>
<p>Binary probability estimates for loss=”modified_huber” are given by
(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions
it is necessary to perform proper probability calibration by wrapping
the classifier with
<code class="xref py py-class docutils literal notranslate"><span class="pre">CalibratedClassifierCV</span></code> instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data for prediction.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the probability of the sample for each class in the model,
where classes are ordered as they are in <cite>self.classes_</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
<p class="rubric">References</p>
<p>Zadrozny and Elkan, “Transforming classifier scores into multiclass
probability estimates”, SIGKDD’02,
<a class="reference external" href="http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf">http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf</a></p>
<p>The justification for the formula in the loss=”modified_huber”
case is in the appendix B in:
<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf">http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</a></p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">SGDOneClassSVM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nu</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'optimal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGD</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.OutlierMixin</span></code></p>
<p>Solves linear One-Class SVM using Stochastic Gradient Descent.</p>
<p>This implementation is meant to be used with a kernel approximation
technique (e.g. <cite>sklearn.kernel_approximation.Nystroem</cite>) to obtain results
similar to <cite>sklearn.svm.OneClassSVM</cite> which uses a Gaussian kernel by
default.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>nu</strong> (<em>float</em><em>, </em><em>optional</em>) – The nu parameter of the One Class SVM: an upper bound on the
fraction of training errors and a lower bound of the fraction of
support vectors. Should be in the interval (0, 1]. By default 0.5
will be taken.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em>) – Whether the intercept should be estimated or not. Defaults to True.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>optional</em>) – The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<cite>partial_fit</cite>. Defaults to 1000.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em> or </em><em>None</em><em>, </em><em>optional</em>) – The stopping criterion. If it is not None, the iterations will stop
when (loss &gt; previous_loss - tol). Defaults to 1e-3.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>optional</em>) – Whether or not the training data should be shuffled after each epoch.
Defaults to True.</p></li>
<li><p><strong>verbose</strong> (<em>integer</em><em>, </em><em>optional</em>) – The verbosity level</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>optional</em><em> (</em><em>default=None</em><em>)</em>) – The seed of the pseudo random number generator to use when shuffling
the data.  If int, random_state is the seed used by the random number
generator; If RandomState instance, random_state is the random number
generator; If None, the random number generator is the RandomState
instance used by <cite>np.random</cite>.</p></li>
<li><p><strong>learning_rate</strong> (<em>string</em><em>, </em><em>optional</em>) – <p>The learning rate schedule:</p>
<dl class="simple">
<dt>’constant’:</dt><dd><p>eta = eta0</p>
</dd>
<dt>’optimal’: [default]</dt><dd><p>eta = 1.0 / (alpha * (t + t0))
where t0 is chosen by a heuristic proposed by Leon Bottou.</p>
</dd>
<dt>’invscaling’:</dt><dd><p>eta = eta0 / pow(t, power_t)</p>
</dd>
<dt>’adaptive’:</dt><dd><p>eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.</p>
</dd>
</dl>
</p></li>
<li><p><strong>eta0</strong> (<em>double</em>) – The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by
the default schedule ‘optimal’.</p></li>
<li><p><strong>power_t</strong> (<em>double</em>) – The exponent for inverse scaling learning rate [default 0.5].</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>optional</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>  will result in increasing the
existing counter.</p>
</p></li>
<li><p><strong>average</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>optional</em>) – When set to True, computes the averaged SGD weights and stores the
result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to an int greater than 1,
averaging will begin once the total number of samples seen reaches
average. So <code class="docutils literal notranslate"><span class="pre">average=10</span></code> will begin averaging after seeing 10
samples.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (1, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.offset_">
<span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores.
We have the relation: decision_function = score_samples - offset.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations to reach the stopping criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.loss_function_">
<span class="sig-name descname"><span class="pre">loss_function_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.loss_function_" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code></p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">SGDOneClassSVM</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">SGDOneClassSVM(random_state=42)</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="sklearn.svm.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.OneClassSVM</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>This estimator has a linear complexity in the number of training samples
and is thus better suited than the <cite>sklearn.svm.OneClassSVM</cite>
implementation for datasets with a large number of training samples (say
&gt; 10,000).</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Signed distance to the separating hyperplane.</p>
<p>Signed distance is positive for an inlier and negative for an
outlier.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Testing data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>dec</strong> – Decision function values of the samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">offset_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear One-Class SVM with Stochastic Gradient Descent.</p>
<p>This solves an equivalent optimization problem of the
One-Class SVM primal optimization problem and returns a weight vector
w and an offset rho such that the decision function is given by
&lt;w, x&gt; - rho.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>coef_init</strong> (<em>array</em><em>, </em><em>shape</em><em> (</em><em>n_classes</em><em>, </em><em>n_features</em><em>)</em>) – The initial coefficients to warm-start the optimization.</p></li>
<li><p><strong>offset_init</strong> (<em>array</em><em>, </em><em>shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em>) – The initial offset to warm-start the optimization.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>optional</em>) – Weights applied to individual samples.
If not provided, uniform weights are assumed. These weights will
be multiplied with class_weight (passed through the
constructor) if class_weight is specified.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.loss_functions">
<span class="sig-name descname"><span class="pre">loss_functions</span></span><em class="property"> <span class="pre">=</span> <span class="pre">{'hinge':</span> <span class="pre">(&lt;class</span> <span class="pre">'sklearn.linear_model._sgd_fast.Hinge'&gt;,</span> <span class="pre">1.0)}</span></em><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.loss_functions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear One-Class SVM with Stochastic Gradient Descent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Subset of the training data.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>optional</em>) – Weights applied to individual samples.
If not provided, uniform weights are assumed.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return labels (1 inlier, -1 outlier) of the samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Testing data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – Labels of the samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDOneClassSVM.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDOneClassSVM.score_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Raw scoring function of the samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix}</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Testing data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score_samples</strong> – Unshiffted scoring function values of the samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">SGDRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">penalty</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'l2'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1000</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'invscaling'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eta0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._stochastic_gradient.BaseSGDRegressor</span></code></p>
<p>Linear model fitted by minimizing a regularized empirical loss with SGD</p>
<p>SGD stands for Stochastic Gradient Descent: the gradient of the loss is
estimated each sample at a time and the model is updated along the way with
a decreasing strength schedule (aka learning rate).</p>
<p>The regularizer is a penalty added to the loss function that shrinks model
parameters towards the zero vector using either the squared euclidean norm
L2 or the absolute norm L1 or a combination of both (Elastic Net). If the
parameter update crosses the 0.0 value because of the regularizer, the
update is truncated to 0.0 to allow for learning sparse models and achieve
online feature selection.</p>
<p>This implementation works with data represented as dense numpy arrays of
floating point values for the features.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>str</em><em>, </em><em>default='squared_error'</em>) – <p>The loss function to be used. The possible values are ‘squared_error’,
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’</p>
<p>The ‘squared_error’ refers to the ordinary least squares fit.
‘huber’ modifies ‘squared_error’ to focus less on getting outliers
correct by switching from squared to linear loss past a distance of
epsilon. ‘epsilon_insensitive’ ignores errors less than epsilon and is
linear past that; this is the loss function used in SVR.
‘squared_epsilon_insensitive’ is the same but becomes squared loss past
a tolerance of epsilon.</p>
<p>More details about the losses formulas can be found in the
<span class="xref std std-ref">User Guide</span>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The loss ‘squared_loss’ was deprecated in v1.0 and will be removed
in version 1.2. Use <cite>loss=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>penalty</strong> (<em>{'l2'</em><em>, </em><em>'l1'</em><em>, </em><em>'elasticnet'}</em><em>, </em><em>default='l2'</em>) – The penalty (aka regularization term) to be used. Defaults to ‘l2’
which is the standard regularizer for linear SVM models. ‘l1’ and
‘elasticnet’ might bring sparsity to the model (feature selection)
not achievable with ‘l2’.</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – Constant that multiplies the regularization term. The higher the
value, the stronger the regularization.
Also used to compute the learning rate when set to <cite>learning_rate</cite> is
set to ‘optimal’.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.15</em>) – The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.
l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.
Only used if <cite>penalty</cite> is ‘elasticnet’.</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the intercept should be estimated or not. If False, the
data is assumed to be already centered.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=1000</em>) – <p>The maximum number of passes over the training data (aka epochs).
It only impacts the behavior in the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method, and not the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit()</span></code> method.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – <p>The stopping criterion. If it is not None, training will stop
when (loss &gt; best_loss - tol) for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive
epochs.
Convergence is checked against the training loss or the
validation loss depending on the <cite>early_stopping</cite> parameter.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not the training data should be shuffled after each epoch.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level.</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Epsilon in the epsilon-insensitive loss functions; only if <cite>loss</cite> is
‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.
For ‘huber’, determines the threshold at which it becomes less
important to get the prediction exactly right.
For epsilon-insensitive, any differences between the current prediction
and the correct label are ignored if they are less than this threshold.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Used for shuffling the data, when <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>learning_rate</strong> (<em>string</em><em>, </em><em>default='invscaling'</em>) – <p>The learning rate schedule:</p>
<ul>
<li><p>’constant’: <cite>eta = eta0</cite></p></li>
<li><p>’optimal’: <cite>eta = 1.0 / (alpha * (t + t0))</cite>
where t0 is chosen by a heuristic proposed by Leon Bottou.</p></li>
<li><p>’invscaling’: <cite>eta = eta0 / pow(t, power_t)</cite></p></li>
<li><p>’adaptive’: eta = eta0, as long as the training keeps decreasing.
Each time n_iter_no_change consecutive epochs fail to decrease the
training loss by tol or fail to increase validation score by tol if
early_stopping is True, the current learning rate is divided by 5.</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘adaptive’ option</p>
</div>
</div></blockquote>
</li>
</ul>
</p></li>
<li><p><strong>eta0</strong> (<em>double</em><em>, </em><em>default=0.01</em>) – The initial learning rate for the ‘constant’, ‘invscaling’ or
‘adaptive’ schedules. The default value is 0.01.</p></li>
<li><p><strong>power_t</strong> (<em>double</em><em>, </em><em>default=0.25</em>) – The exponent for inverse scaling learning rate.</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether to use early stopping to terminate training when validation
score is not improving. If set to True, it will automatically set aside
a fraction of training data as validation and terminate
training when validation score returned by the <cite>score</cite> method is not
improving by at least <cite>tol</cite> for <cite>n_iter_no_change</cite> consecutive
epochs.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘early_stopping’ option</p>
</div>
</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <cite>early_stopping</cite> is True.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘validation_fraction’ option</p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=5</em>) – <p>Number of iterations with no improvement to wait before stopping
fitting.
Convergence is checked against the training loss or the
validation loss depending on the <cite>early_stopping</cite> parameter.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20: </span>Added ‘n_iter_no_change’ option</p>
</div>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit as
initialization, otherwise, just erase the previous solution.
See <span class="xref std std-term">the Glossary</span>.</p>
<p>Repeatedly calling fit or partial_fit when warm_start is True can
result in a different solution than when calling fit a single time
because of the way the data is shuffled.
If a dynamic learning rate is used, the learning rate is adapted
depending on the number of samples already seen. Calling <code class="docutils literal notranslate"><span class="pre">fit</span></code> resets
this counter, while <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>  will result in increasing the
existing counter.</p>
</p></li>
<li><p><strong>average</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – When set to True, computes the averaged SGD weights accross all
updates and stores the result in the <code class="docutils literal notranslate"><span class="pre">coef_</span></code> attribute. If set to
an int greater than 1, averaging will begin once the total number of
samples seen reaches <cite>average</cite>. So <code class="docutils literal notranslate"><span class="pre">average=10</span></code> will begin
averaging after seeing 10 samples.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights assigned to the features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>The intercept term.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.average_coef_">
<span class="sig-name descname"><span class="pre">average_coef_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.average_coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Averaged weights assigned to the features. Only available
if <code class="docutils literal notranslate"><span class="pre">average=True</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.23: </span>Attribute <code class="docutils literal notranslate"><span class="pre">average_coef_</span></code> was deprecated
in version 0.23 and will be removed in 1.0 (renaming of 0.25).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.average_intercept_">
<span class="sig-name descname"><span class="pre">average_intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.average_intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>The averaged intercept term. Only available if <code class="docutils literal notranslate"><span class="pre">average=True</span></code>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.23: </span>Attribute <code class="docutils literal notranslate"><span class="pre">average_intercept_</span></code> was deprecated
in version 0.23 and will be removed in 1.0 (renaming of 0.25).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of iterations before reaching the stopping criterion.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDRegressor.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.linear_model.SGDRegressor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of weight updates performed during training.
Same as <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Always scale the input. The most convenient way is to use a pipeline.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                    <span class="n">SGDRegressor</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;sgdregressor&#39;, SGDRegressor())])</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.Ridge" title="sklearn.linear_model.Ridge"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Ridge</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="sklearn.svm.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.SVR</span></code></a></p>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.SquaredLoss">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">SquaredLoss</span></span><a class="headerlink" href="#sklearn.linear_model.SquaredLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._sgd_fast.Regression</span></code></p>
<p>Squared loss traditional used in linear regression.</p>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">TheilSenRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_subpopulation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10000.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_subsamples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">300</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._base.LinearModel</span></code></p>
<p>Theil-Sen Estimator: robust multivariate regression model.</p>
<p>The algorithm calculates least square solutions on subsets with size
n_subsamples of the samples in X. Any value of n_subsamples between the
number of features and samples leads to an estimator with a compromise
between robustness and efficiency. Since the number of least square
solutions is “n_samples choose n_subsamples”, it can be extremely large
and can therefore be limited with max_subpopulation. If this limit is
reached, the subsets are chosen randomly. In a final step, the spatial
median (or L1 median) is calculated of all least square solutions.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to calculate the intercept for this model. If set
to false, no intercept will be used in calculations.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>max_subpopulation</strong> (<em>int</em><em>, </em><em>default=1e4</em>) – Instead of computing with a set of cardinality ‘n choose k’, where n is
the number of samples and k is the number of subsamples (at least
number of features), consider only a stochastic subpopulation of a
given maximal size if ‘n choose k’ is larger than max_subpopulation.
For other than small problem sizes this parameter will determine
memory usage and runtime if n_subsamples is not changed.</p></li>
<li><p><strong>n_subsamples</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of samples to calculate the parameters. This is at least the
number of features (plus 1 if fit_intercept=True) and the number of
samples as a maximum. A lower number leads to a higher breakdown
point and a low efficiency while a high number leads to a low
breakdown point and a high efficiency. If None, take the
minimum number of subsamples leading to maximal robustness.
If n_subsamples is set to n_samples, Theil-Sen is identical to least
squares.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=300</em>) – Maximum number of iterations for the calculation of spatial median.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1.e-3</em>) – Tolerance when calculating spatial median.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – A random number generator instance to define the state of the random
permutations generator. Pass an int for reproducible output across
multiple function calls.
See <span class="xref std std-term">Glossary</span></p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Number of CPUs to use during the cross validation.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Verbose mode when fitting the model.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Coefficients of the regression model (median of distribution).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated intercept of regression model.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.breakdown_">
<span class="sig-name descname"><span class="pre">breakdown_</span></span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.breakdown_" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximated breakdown point.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of iterations needed for the spatial median.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.n_subpopulation_">
<span class="sig-name descname"><span class="pre">n_subpopulation_</span></span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.n_subpopulation_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of combinations taken into account from ‘n choose k’, where n is
the number of samples and k is the number of subsamples.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">TheilSenRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">4.0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">TheilSenRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9884...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="mi">1</span><span class="p">,])</span>
<span class="go">array([-31.5871...])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Theil-Sen Estimators in a Multiple Linear Regression Model, 2009
Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang
<a class="reference external" href="http://home.olemiss.edu/~xdang/papers/MTSE.pdf">http://home.olemiss.edu/~xdang/papers/MTSE.pdf</a></p></li>
</ul>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.TheilSenRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.TheilSenRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit linear model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns an instance of self.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.TweedieRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">TweedieRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fit_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">link</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.TweedieRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model._glm.glm.GeneralizedLinearRegressor</span></code></p>
<p>Generalized Linear Model with a Tweedie distribution.</p>
<p>This estimator can be used to model different GLMs depending on the
<code class="docutils literal notranslate"><span class="pre">power</span></code> parameter, which determines the underlying distribution.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>power</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>The power determines the underlying target distribution according
to the following table:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 23%" />
<col style="width: 77%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Power</p></th>
<th class="head"><p>Distribution</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>Normal</p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p>Poisson</p></td>
</tr>
<tr class="row-even"><td><p>(1,2)</p></td>
<td><p>Compound Poisson Gamma</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>Gamma</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>Inverse Gaussian</p></td>
</tr>
</tbody>
</table>
<p>For <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">power</span> <span class="pre">&lt;</span> <span class="pre">1</span></code>, no distribution exists.</p>
</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=1</em>) – Constant that multiplies the penalty term and thus determines the
regularization strength. <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">=</span> <span class="pre">0</span></code> is equivalent to unpenalized
GLMs. In this case, the design matrix <cite>X</cite> must have full column rank
(no collinearities).</p></li>
<li><p><strong>fit_intercept</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Specifies if a constant (a.k.a. bias or intercept) should be
added to the linear predictor (X &#64; coef + intercept).</p></li>
<li><p><strong>link</strong> (<em>{'auto'</em><em>, </em><em>'identity'</em><em>, </em><em>'log'}</em><em>, </em><em>default='auto'</em>) – <p>The link function of the GLM, i.e. mapping from linear predictor
<cite>X &#64; coeff + intercept</cite> to prediction <cite>y_pred</cite>. Option ‘auto’ sets
the link depending on the chosen family as follows:</p>
<ul>
<li><p>’identity’ for Normal distribution</p></li>
<li><p>’log’ for Poisson,  Gamma and Inverse Gaussian distributions</p></li>
</ul>
</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – The maximal number of iterations for the solver.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Stopping criterion. For the lbfgs solver,
the iteration will stop when <code class="docutils literal notranslate"><span class="pre">max{|g_j|,</span> <span class="pre">j</span> <span class="pre">=</span> <span class="pre">1,</span> <span class="pre">...,</span> <span class="pre">d}</span> <span class="pre">&lt;=</span> <span class="pre">tol</span></code>
where <code class="docutils literal notranslate"><span class="pre">g_j</span></code> is the j-th component of the gradient (derivative) of
the objective function.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to <code class="docutils literal notranslate"><span class="pre">fit</span></code>
as initialization for <code class="docutils literal notranslate"><span class="pre">coef_</span></code> and <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> .</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – For the lbfgs solver set verbose to any positive number for verbosity.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TweedieRegressor.coef_">
<span class="sig-name descname"><span class="pre">coef_</span></span><a class="headerlink" href="#sklearn.linear_model.TweedieRegressor.coef_" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimated coefficients for the linear predictor (<cite>X &#64; coef_ +
intercept_</cite>) in the GLM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TweedieRegressor.intercept_">
<span class="sig-name descname"><span class="pre">intercept_</span></span><a class="headerlink" href="#sklearn.linear_model.TweedieRegressor.intercept_" title="Permalink to this definition">¶</a></dt>
<dd><p>Intercept (a.k.a. bias) added to linear predictor.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.linear_model.TweedieRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.linear_model.TweedieRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>Actual number of iterations used in the solver.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">TweedieRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">TweedieRegressor()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.839...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">coef_</span>
<span class="go">array([0.599..., 0.299...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">intercept_</span>
<span class="go">1.600...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="go">array([2.500..., 4.599...])</span>
</pre></div>
</div>
<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.TweedieRegressor.family">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">family</span></span><a class="headerlink" href="#sklearn.linear_model.TweedieRegressor.family" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.enet_path">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">enet_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l1_ratio</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.enet_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute elastic net path with coordinate descent.</p>
<p>The elastic net optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||^</span><span class="mi">2_2</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
<span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">l1_ratio</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
<span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">l1_ratio</span><span class="p">)</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_Fro</span><span class="o">^</span><span class="mi">2</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values.</p></li>
<li><p><strong>l1_ratio</strong> (<em>float</em><em>, </em><em>default=0.5</em>) – Number between 0 and 1 passed to elastic net (scaling between
l1 and l2 penalties). <code class="docutils literal notranslate"><span class="pre">l1_ratio=1</span></code> corresponds to the Lasso.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code>.</p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path.</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If None alphas are set automatically.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If set to False, the input validation checks are skipped (including the
Gram matrix when provided). It is assumed that they are handled
by the caller.</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – Keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.
(Is returned when <code class="docutils literal notranslate"><span class="pre">return_n_iter</span></code> is set to True).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNet" title="sklearn.linear_model.MultiTaskElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.MultiTaskElasticNetCV" title="sklearn.linear_model.MultiTaskElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MultiTaskElasticNetCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNet" title="sklearn.linear_model.ElasticNet"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNet</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.ElasticNetCV" title="sklearn.linear_model.ElasticNetCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ElasticNetCV</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.lars_path">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">lars_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Gram</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lar'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_Gram</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.lars_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Least Angle Regression or Lasso path using LARS algorithm [1]</p>
<p>The optimization objective for the case method=’lasso’ is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>in the case of method=’lars’, the objective function is only known in
the form of an implicit equation (see discussion in [1])</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>None</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data. Note that if X is None then the Gram matrix must be
specified, i.e., cannot be None or False.</p></li>
<li><p><strong>y</strong> (<em>None</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Input targets.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em><em>,             </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>Gram</strong> (<em>None</em><em>, </em><em>'auto'</em><em>, </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,             </em><em>default=None</em>) – Precomputed Gram matrix (X’ * X), if <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, the Gram
matrix is precomputed from the given X, if there are more samples
than features.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform, set to infinity for no limit.</p></li>
<li><p><strong>alpha_min</strong> (<em>float</em><em>, </em><em>default=0</em>) – Minimum correlation along the path. It corresponds to the
regularization parameter alpha parameter in the Lasso.</p></li>
<li><p><strong>method</strong> (<em>{'lar'</em><em>, </em><em>'lasso'}</em><em>, </em><em>default='lar'</em>) – Specifies the returned model. Select <code class="docutils literal notranslate"><span class="pre">'lar'</span></code> for Least Angle
Regression, <code class="docutils literal notranslate"><span class="pre">'lasso'</span></code> for the Lasso.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">X</span></code> is overwritten.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_Gram</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">Gram</span></code> is overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls output verbosity.</p></li>
<li><p><strong>return_path</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">return_path==True</span></code> returns the entire path, else returns only the
last point of the path.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Restrict coefficients to be &gt;= 0.
This option is only allowed with method ‘lasso’. Note that the model
coefficients will not converge to the ordinary-least-squares solution
for small values of alpha. Only coefficients up to the smallest alpha
value (<code class="docutils literal notranslate"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span> <span class="pre">0.].min()</span></code> when fit_path=True) reached by
the stepwise Lars-Lasso algorithm are typically in congruence with the
solution of the coordinate descent lasso_path function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>array-like of shape (n_alphas + 1,)</em>) – Maximum of covariances (in absolute value) at each iteration.
<code class="docutils literal notranslate"><span class="pre">n_alphas</span></code> is either <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> or the
number of nodes in the path with <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">&gt;=</span> <span class="pre">alpha_min</span></code>, whichever
is smaller.</p></li>
<li><p><strong>active</strong> (<em>array-like of shape (n_alphas,)</em>) – Indices of active variables at the end of the path.</p></li>
<li><p><strong>coefs</strong> (<em>array-like of shape (n_features, n_alphas + 1)</em>) – Coefficients along the path</p></li>
<li><p><strong>n_iter</strong> (<em>int</em>) – Number of iterations run. Returned only if return_n_iter is set
to True.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path_gram" title="sklearn.linear_model.lars_path_gram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path_gram</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path_gram</span></code>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id8"><span class="brackets">1</span></dt>
<dd><p>“Least Angle Regression”, Efron et al.
<a class="reference external" href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf">http://statweb.stanford.edu/~tibs/ftp/lars.pdf</a></p>
</dd>
<dt class="label" id="id9"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Least-angle_regression">Wikipedia entry on the Least-angle regression</a></p>
</dd>
<dt class="label" id="id10"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Wikipedia entry on the Lasso</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.lars_path_gram">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">lars_path_gram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Xy</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Gram</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_samples</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">500</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha_min</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'lar'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2.220446049250313e-16</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_Gram</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.lars_path_gram" title="Permalink to this definition">¶</a></dt>
<dd><p>lars_path in the sufficient stats mode [1]</p>
<p>The optimization objective for the case method=’lasso’ is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>in the case of method=’lars’, the objective function is only known in
the form of an implicit equation (see discussion in [1])</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Xy = np.dot(X.T, y).</p></li>
<li><p><strong>Gram</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em>) – Gram = np.dot(X.T * X).</p></li>
<li><p><strong>n_samples</strong> (<em>int</em><em> or </em><em>float</em>) – Equivalent size of sample.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=500</em>) – Maximum number of iterations to perform, set to infinity for no limit.</p></li>
<li><p><strong>alpha_min</strong> (<em>float</em><em>, </em><em>default=0</em>) – Minimum correlation along the path. It corresponds to the
regularization parameter alpha parameter in the Lasso.</p></li>
<li><p><strong>method</strong> (<em>{'lar'</em><em>, </em><em>'lasso'}</em><em>, </em><em>default='lar'</em>) – Specifies the returned model. Select <code class="docutils literal notranslate"><span class="pre">'lar'</span></code> for Least Angle
Regression, <code class="docutils literal notranslate"><span class="pre">'lasso'</span></code> for the Lasso.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">X</span></code> is overwritten.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=np.finfo</em><em>(</em><em>float</em><em>)</em><em>eps</em>) – The machine-precision regularization in the computation of the
Cholesky diagonal factors. Increase this for very ill-conditioned
systems. Unlike the <code class="docutils literal notranslate"><span class="pre">tol</span></code> parameter in some iterative
optimization-based algorithms, this parameter does not control
the tolerance of the optimization.</p></li>
<li><p><strong>copy_Gram</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">Gram</span></code> is overwritten.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls output verbosity.</p></li>
<li><p><strong>return_path</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">return_path==True</span></code> returns the entire path, else returns only the
last point of the path.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return the number of iterations.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Restrict coefficients to be &gt;= 0.
This option is only allowed with method ‘lasso’. Note that the model
coefficients will not converge to the ordinary-least-squares solution
for small values of alpha. Only coefficients up to the smallest alpha
value (<code class="docutils literal notranslate"><span class="pre">alphas_[alphas_</span> <span class="pre">&gt;</span> <span class="pre">0.].min()</span></code> when fit_path=True) reached by
the stepwise Lars-Lasso algorithm are typically in congruence with the
solution of the coordinate descent lasso_path function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>array-like of shape (n_alphas + 1,)</em>) – Maximum of covariances (in absolute value) at each iteration.
<code class="docutils literal notranslate"><span class="pre">n_alphas</span></code> is either <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> or the
number of nodes in the path with <code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">&gt;=</span> <span class="pre">alpha_min</span></code>, whichever
is smaller.</p></li>
<li><p><strong>active</strong> (<em>array-like of shape (n_alphas,)</em>) – Indices of active variables at the end of the path.</p></li>
<li><p><strong>coefs</strong> (<em>array-like of shape (n_features, n_alphas + 1)</em>) – Coefficients along the path</p></li>
<li><p><strong>n_iter</strong> (<em>int</em>) – Number of iterations run. Returned only if return_n_iter is set
to True.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lasso_path" title="sklearn.linear_model.lasso_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">lasso_path_gram</span></code>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lars" title="sklearn.linear_model.Lars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LarsCV" title="sklearn.linear_model.LarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets">1</span></dt>
<dd><p>“Least Angle Regression”, Efron et al.
<a class="reference external" href="http://statweb.stanford.edu/~tibs/ftp/lars.pdf">http://statweb.stanford.edu/~tibs/ftp/lars.pdf</a></p>
</dd>
<dt class="label" id="id12"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Least-angle_regression">Wikipedia entry on the Least-angle regression</a></p>
</dd>
<dt class="label" id="id14"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Lasso_(statistics)">Wikipedia entry on the Lasso</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.lasso_path">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">lasso_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphas</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">positive</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.lasso_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Lasso path with coordinate descent</p>
<p>The Lasso optimization function varies for mono and multi-outputs.</p>
<p>For mono-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">y</span> <span class="o">-</span> <span class="n">Xw</span><span class="o">||^</span><span class="mi">2_2</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">w</span><span class="o">||</span><span class="n">_1</span>
</pre></div>
</div>
<p>For multi-output tasks it is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span> <span class="o">*</span> <span class="o">||</span><span class="n">Y</span> <span class="o">-</span> <span class="n">XW</span><span class="o">||^</span><span class="mi">2</span><span class="n">_Fro</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span>
</pre></div>
</div>
<p>Where:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">||</span><span class="n">W</span><span class="o">||</span><span class="n">_21</span> <span class="o">=</span> \<span class="n">sum_i</span> \<span class="n">sqrt</span><span class="p">{</span>\<span class="n">sum_j</span> <span class="n">w_</span><span class="p">{</span><span class="n">ij</span><span class="p">}</span><span class="o">^</span><span class="mi">2</span><span class="p">}</span>
</pre></div>
</div>
<p>i.e. the sum of norm of each row.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data. Pass directly as Fortran-contiguous data to avoid
unnecessary memory duplication. If <code class="docutils literal notranslate"><span class="pre">y</span></code> is mono-output then <code class="docutils literal notranslate"><span class="pre">X</span></code>
can be sparse.</p></li>
<li><p><strong>y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or         </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Target values</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-3</em>) – Length of the path. <code class="docutils literal notranslate"><span class="pre">eps=1e-3</span></code> means that
<code class="docutils literal notranslate"><span class="pre">alpha_min</span> <span class="pre">/</span> <span class="pre">alpha_max</span> <span class="pre">=</span> <span class="pre">1e-3</span></code></p></li>
<li><p><strong>n_alphas</strong> (<em>int</em><em>, </em><em>default=100</em>) – Number of alphas along the regularization path</p></li>
<li><p><strong>alphas</strong> (<em>ndarray</em><em>, </em><em>default=None</em>) – List of alphas where to compute the models.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> alphas are set automatically</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em>, </em><em>bool</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em><em>,                 </em><em>default='auto'</em>) – Whether to use a precomputed Gram matrix to speed up
calculations. If set to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> let us decide. The Gram
matrix can also be passed as argument.</p></li>
<li><p><strong>Xy</strong> (<em>array-like of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_outputs</em><em>)</em><em>,         </em><em>default=None</em>) – Xy = np.dot(X.T, y) that can be precomputed. It is useful
only when the Gram matrix is precomputed.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">True</span></code>, X will be copied; else, it may be overwritten.</p></li>
<li><p><strong>coef_init</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>)</em><em>, </em><em>default=None</em>) – The initial values of the coefficients.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em> or </em><em>int</em><em>, </em><em>default=False</em>) – Amount of verbosity.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – whether to return the number of iterations or not.</p></li>
<li><p><strong>positive</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If set to True, forces coefficients to be positive.
(Only allowed when <code class="docutils literal notranslate"><span class="pre">y.ndim</span> <span class="pre">==</span> <span class="pre">1</span></code>).</p></li>
<li><p><strong>**params</strong> (<em>kwargs</em>) – keyword arguments passed to the coordinate descent solver.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>alphas</strong> (<em>ndarray of shape (n_alphas,)</em>) – The alphas along the path where models are computed.</p></li>
<li><p><strong>coefs</strong> (<em>ndarray of shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)</em>) – Coefficients along the path.</p></li>
<li><p><strong>dual_gaps</strong> (<em>ndarray of shape (n_alphas,)</em>) – The dual gaps at the end of the optimization for each alpha.</p></li>
<li><p><strong>n_iters</strong> (<em>list of int</em>) – The number of iterations taken by the coordinate descent optimizer to
reach the specified tolerance for each alpha.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>For an example, see
<span class="xref std std-ref">examples/linear_model/plot_lasso_coordinate_descent_path.py</span>.</p>
<p>To avoid unnecessary memory duplication the X argument of the fit method
should be directly passed as a Fortran-contiguous numpy array.</p>
<p>Note that in certain cases, the Lars solver may be significantly
faster to implement this functionality. In particular, linear
interpolation can be used to retrieve model coefficients between the
values output by lars_path</p>
<p class="rubric">Examples</p>
<p>Comparing lasso_path and lars_path with interpolation:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.3</span><span class="p">,</span> <span class="mf">5.4</span><span class="p">,</span> <span class="mf">4.3</span><span class="p">]])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">3.1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Use lasso_path to compute a coefficient path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span><span class="p">,</span> <span class="n">coef_path</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">lasso_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path</span><span class="p">)</span>
<span class="go">[[0.         0.         0.46874778]</span>
<span class="go"> [0.2159048  0.4425765  0.23689075]]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Now use lars_path and 1D linear interpolation to compute the</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># same path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">lars_path</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">alphas</span><span class="p">,</span> <span class="n">active</span><span class="p">,</span> <span class="n">coef_path_lars</span> <span class="o">=</span> <span class="n">lars_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;lasso&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">interpolate</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">coef_path_continuous</span> <span class="o">=</span> <span class="n">interpolate</span><span class="o">.</span><span class="n">interp1d</span><span class="p">(</span><span class="n">alphas</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                                            <span class="n">coef_path_lars</span><span class="p">[:,</span> <span class="p">::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">coef_path_continuous</span><span class="p">([</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">]))</span>
<span class="go">[[0.         0.         0.46915237]</span>
<span class="go"> [0.2159048  0.4425765  0.23668876]]</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.Lasso" title="sklearn.linear_model.Lasso"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Lasso</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLars" title="sklearn.linear_model.LassoLars"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLars</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoCV" title="sklearn.linear_model.LassoCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoCV</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.LassoLarsCV" title="sklearn.linear_model.LassoLarsCV"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LassoLarsCV</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.orthogonal_mp">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">orthogonal_mp</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero_coefs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precompute</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_X</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.orthogonal_mp" title="Permalink to this definition">¶</a></dt>
<dd><p>Orthogonal Matching Pursuit (OMP).</p>
<p>Solves n_targets Orthogonal Matching Pursuit problems.
An instance of the problem has the form:</p>
<p>When parametrized by the number of non-zero coefficients using
<cite>n_nonzero_coefs</cite>:
argmin ||y - Xgamma||^2 subject to ||gamma||_0 &lt;= n_{nonzero coefs}</p>
<p>When parametrized by error using the parameter <cite>tol</cite>:
argmin ||gamma||_0 subject to ||y - Xgamma||^2 &lt;= tol</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data. Columns are assumed to have unit norm.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Input targets.</p></li>
<li><p><strong>n_nonzero_coefs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Desired number of non-zero entries in the solution. If None (by
default) this value is set to 10% of n_features.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=None</em>) – Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</p></li>
<li><p><strong>precompute</strong> (<em>'auto'</em><em> or </em><em>bool</em><em>, </em><em>default=False</em>) – Whether to perform precomputations. Improves performance when n_targets
or n_samples is very large.</p></li>
<li><p><strong>copy_X</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the design matrix X must be copied by the algorithm. A false
value is only helpful if X is already Fortran-ordered, otherwise a
copy is made anyway.</p></li>
<li><p><strong>return_path</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return every value of the nonzero coefficients along the
forward path. Useful for cross-validation.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether or not to return the number of iterations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>coef</strong> (<em>ndarray of shape (n_features,) or (n_features, n_targets)</em>) – Coefficients of the OMP solution. If <cite>return_path=True</cite>, this contains
the whole coefficient path. In this case its shape is
(n_features, n_features) or (n_features, n_targets, n_features) and
iterating over the last axis yields coefficients in increasing order
of active features.</p></li>
<li><p><strong>n_iters</strong> (<em>array-like or int</em>) – Number of active features across every target. Returned only if
<cite>return_n_iter</cite> is set to True.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuit</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.orthogonal_mp_gram" title="sklearn.linear_model.orthogonal_mp_gram"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp_gram</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(<a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf</a>)</p>
<p>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
<a class="reference external" href="https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.orthogonal_mp_gram">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">orthogonal_mp_gram</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">Gram</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Xy</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_nonzero_coefs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">norms_squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_Gram</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_Xy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.orthogonal_mp_gram" title="Permalink to this definition">¶</a></dt>
<dd><p>Gram Orthogonal Matching Pursuit (OMP).</p>
<p>Solves n_targets Orthogonal Matching Pursuit problems using only
the Gram matrix X.T * X and the product X.T * y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>Gram</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>, </em><em>n_features</em><em>)</em>) – Gram matrix of the input data: X.T * X.</p></li>
<li><p><strong>Xy</strong> (<em>ndarray of shape</em><em> (</em><em>n_features</em><em>,</em><em>) or </em><em>(</em><em>n_features</em><em>, </em><em>n_targets</em><em>)</em>) – Input targets multiplied by X: X.T * y.</p></li>
<li><p><strong>n_nonzero_coefs</strong> (<em>int</em><em>, </em><em>default=None</em>) – Desired number of non-zero entries in the solution. If None (by
default) this value is set to 10% of n_features.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=None</em>) – Maximum norm of the residual. If not None, overrides n_nonzero_coefs.</p></li>
<li><p><strong>norms_squared</strong> (<em>array-like of shape</em><em> (</em><em>n_targets</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Squared L2 norms of the lines of y. Required if tol is not None.</p></li>
<li><p><strong>copy_Gram</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the gram matrix must be copied by the algorithm. A false
value is only helpful if it is already Fortran-ordered, otherwise a
copy is made anyway.</p></li>
<li><p><strong>copy_Xy</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether the covariance vector Xy must be copied by the algorithm.
If False, it may be overwritten.</p></li>
<li><p><strong>return_path</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to return every value of the nonzero coefficients along the
forward path. Useful for cross-validation.</p></li>
<li><p><strong>return_n_iter</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether or not to return the number of iterations.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>coef</strong> (<em>ndarray of shape (n_features,) or (n_features, n_targets)</em>) – Coefficients of the OMP solution. If <cite>return_path=True</cite>, this contains
the whole coefficient path. In this case its shape is
(n_features, n_features) or (n_features, n_targets, n_features) and
iterating over the last axis yields coefficients in increasing order
of active features.</p></li>
<li><p><strong>n_iters</strong> (<em>array-like or int</em>) – Number of active features across every target. Returned only if
<cite>return_n_iter</cite> is set to True.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.linear_model.OrthogonalMatchingPursuit" title="sklearn.linear_model.OrthogonalMatchingPursuit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">OrthogonalMatchingPursuit</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.orthogonal_mp" title="sklearn.linear_model.orthogonal_mp"><code class="xref py py-obj docutils literal notranslate"><span class="pre">orthogonal_mp</span></code></a>, <a class="reference internal" href="#sklearn.linear_model.lars_path" title="sklearn.linear_model.lars_path"><code class="xref py py-obj docutils literal notranslate"><span class="pre">lars_path</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.decomposition.sparse_encode</span></code></p>
</div>
<p class="rubric">Notes</p>
<p>Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
Matching pursuits with time-frequency dictionaries, IEEE Transactions on
Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
(<a class="reference external" href="http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf">http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf</a>)</p>
<p>This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad,
M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal
Matching Pursuit Technical Report - CS Technion, April 2008.
<a class="reference external" href="https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf">https://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf</a></p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.linear_model.ridge_regression">
<span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">ridge_regression</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_intercept</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.ridge_regression" title="Permalink to this definition">¶</a></dt>
<dd><p>Solve the ridge equation by the method of normal equations.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix</em><em>, </em><em>LinearOperator} of shape</em><em>         (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_targets</em><em>)</em>) – Target values</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_targets</em><em>,</em><em>)</em>) – Regularization strength; must be a positive float. Regularization
improves the conditioning of the problem and reduces the variance of
the estimates. Larger values specify stronger regularization.
Alpha corresponds to <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(2C)</span></code> in other linear models such as
<a class="reference internal" href="#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a> or
<a class="reference internal" href="sklearn.svm.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearSVC</span></code></a>. If an array is passed, penalties are
assumed to be specific to the targets. Hence they must correspond in
number.</p></li>
<li><p><strong>sample_weight</strong> (<em>float</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Individual weights for each sample. If given a float, every sample
will have the same weight. If sample_weight is not None and
solver=’auto’, the solver will be set to ‘cholesky’.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
</p></li>
<li><p><strong>solver</strong> – <p>Solver to use in the computational routines:</p>
<ul>
<li><p>’auto’ chooses the solver automatically based on the type of data.</p></li>
<li><p>’svd’ uses a Singular Value Decomposition of X to compute the Ridge
coefficients. More stable for singular matrices than ‘cholesky’.</p></li>
<li><p>’cholesky’ uses the standard scipy.linalg.solve function to
obtain a closed-form solution via a Cholesky decomposition of
dot(X.T, X)</p></li>
<li><p>’sparse_cg’ uses the conjugate gradient solver as found in
scipy.sparse.linalg.cg. As an iterative algorithm, this solver is
more appropriate than ‘cholesky’ for large-scale data
(possibility to set <cite>tol</cite> and <cite>max_iter</cite>).</p></li>
<li><p>’lsqr’ uses the dedicated regularized least-squares routine
scipy.sparse.linalg.lsqr. It is the fastest and uses an iterative
procedure.</p></li>
<li><p>’sag’ uses a Stochastic Average Gradient descent, and ‘saga’ uses
its improved, unbiased version named SAGA. Both methods also use an
iterative procedure, and are often faster than other solvers when
both n_samples and n_features are large. Note that ‘sag’ and
‘saga’ fast convergence is only guaranteed on features with
approximately the same scale. You can preprocess the data with a
scaler from sklearn.preprocessing.</p></li>
</ul>
</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>max_iter<span class="classifier">int, default=None</span></dt><dd><p>Maximum number of iterations for conjugate gradient solver.
For the ‘sparse_cg’ and ‘lsqr’ solvers, the default value is determined
by scipy.sparse.linalg. For ‘sag’ and saga solver, the default value is
1000.</p>
</dd>
<dt>tol<span class="classifier">float, default=1e-3</span></dt><dd><p>Precision of the solution.</p>
</dd>
<dt>verbose<span class="classifier">int, default=0</span></dt><dd><p>Verbosity level. Setting verbose &gt; 0 will display additional
information depending on the solver used.</p>
</dd>
<dt>random_state<span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Used when <code class="docutils literal notranslate"><span class="pre">solver</span></code> == ‘sag’ or ‘saga’ to shuffle the data.
See <span class="xref std std-term">Glossary</span> for details.</p>
</dd>
<dt>return_n_iter<span class="classifier">bool, default=False</span></dt><dd><p>If True, the method also returns <cite>n_iter</cite>, the actual number of
iteration performed by the solver.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
</dd>
<dt>return_intercept<span class="classifier">bool, default=False</span></dt><dd><p>If True and if X is sparse, the method also returns the intercept,
and the solver is automatically changed to ‘sag’. This is only a
temporary fix for fitting the intercept with sparse data. For dense
data, use sklearn.linear_model._preprocess_data before your regression.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
</dd>
<dt>check_input<span class="classifier">bool, default=True</span></dt><dd><p>If False, the input arrays X and y will not be checked.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><p><strong>coef</strong> (<em>ndarray of shape (n_features,) or (n_targets, n_features)</em>) – Weight vector(s).</p></li>
<li><p><strong>n_iter</strong> (<em>int, optional</em>) – The actual number of iteration performed by the solver.
Only returned if <cite>return_n_iter</cite> is True.</p></li>
<li><p><strong>intercept</strong> (<em>float or ndarray of shape (n_targets,)</em>) – The intercept of the model. Only returned if <cite>return_intercept</cite>
is True and if X is a scipy sparse array.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This function won’t compute the intercept.</p>
</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Tommaso

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>