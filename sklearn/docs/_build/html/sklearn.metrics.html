

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.metrics package &mdash; Qsklearn  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Qsklearn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">qsklearn</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Qsklearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>sklearn.metrics package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/sklearn.metrics.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sklearn-metrics-package">
<h1>sklearn.metrics package<a class="headerlink" href="#sklearn-metrics-package" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sklearn.metrics.cluster.html">sklearn.metrics.cluster package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.cluster.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="sklearn.metrics.cluster.tests.html">sklearn.metrics.cluster.tests package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#module-sklearn.metrics.cluster.tests.test_bicluster">sklearn.metrics.cluster.tests.test_bicluster module</a></li>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#sklearn-metrics-cluster-tests-test-common-module">sklearn.metrics.cluster.tests.test_common module</a></li>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#sklearn-metrics-cluster-tests-test-supervised-module">sklearn.metrics.cluster.tests.test_supervised module</a></li>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#sklearn-metrics-cluster-tests-test-unsupervised-module">sklearn.metrics.cluster.tests.test_unsupervised module</a></li>
<li class="toctree-l4"><a class="reference internal" href="sklearn.metrics.cluster.tests.html#module-sklearn.metrics.cluster.tests">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.cluster.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.cluster.html#module-sklearn.metrics.cluster.setup">sklearn.metrics.cluster.setup module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.cluster.html#module-sklearn.metrics.cluster">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="sklearn.metrics.tests.html">sklearn.metrics.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-classification-module">sklearn.metrics.tests.test_classification module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-common-module">sklearn.metrics.tests.test_common module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-pairwise-module">sklearn.metrics.tests.test_pairwise module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-ranking-module">sklearn.metrics.tests.test_ranking module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-regression-module">sklearn.metrics.tests.test_regression module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#sklearn-metrics-tests-test-score-objects-module">sklearn.metrics.tests.test_score_objects module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.metrics.tests.html#module-sklearn.metrics.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-sklearn.metrics.pairwise">
<span id="sklearn-metrics-pairwise-module"></span><h2>sklearn.metrics.pairwise module<a class="headerlink" href="#module-sklearn.metrics.pairwise" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.additive_chi2_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">additive_chi2_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.additive_chi2_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the additive chi-squared kernel between observations in X and
Y.</p>
<p>The chi-squared kernel is computed between each pair of rows in X and Y.  X
and Y have to be non-negative. This kernel is most commonly applied to
histograms.</p>
<p>The chi-squared kernel is given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">k</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="o">-</span><span class="n">Sum</span> <span class="p">[(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)]</span>
</pre></div>
</div>
<p>It can be interpreted as a weighted difference per entry.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<p class="rubric">Notes</p>
<p>As the negative of a distance, this kernel is only conditionally positive
definite.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel_matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.chi2_kernel" title="sklearn.metrics.pairwise.chi2_kernel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">chi2_kernel</span></code></a></dt><dd><p>The exponentiated version of the kernel, which is usually preferable.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.kernel_approximation.AdditiveChi2Sampler</span></code></dt><dd><p>A Fourier approximation to this kernel.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.
Local features and kernels for classification of texture and object
categories: A comprehensive study
International Journal of Computer Vision 2007
<a class="reference external" href="https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf">https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf</a></p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.check_paired_arrays">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">check_paired_arrays</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.check_paired_arrays" title="Permalink to this definition">¶</a></dt>
<dd><p>Set X and Y appropriately and checks inputs for paired distances.</p>
<p>All paired distance metrics should use this function first to assert that
the given parameters are correct and safe to use.</p>
<p>Specifically, this function first ensures that both X and Y are arrays,
then checks that they are at least two dimensional while ensuring that
their elements are floats. Finally, the function checks that the size
of the dimensions of the two arrays are equal.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>safe_X</strong> (<em>{array-like, sparse matrix} of shape (n_samples_X, n_features)</em>) – An array equal to X, guaranteed to be a numpy array.</p></li>
<li><p><strong>safe_Y</strong> (<em>{array-like, sparse matrix} of shape (n_samples_Y, n_features)</em>) – An array equal to Y if Y was not None, guaranteed to be a numpy array.
If Y was None, safe_Y will be a pointer to X.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.check_pairwise_arrays">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">check_pairwise_arrays</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precomputed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accept_sparse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'csr'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_all_finite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.check_pairwise_arrays" title="Permalink to this definition">¶</a></dt>
<dd><p>Set X and Y appropriately and checks inputs.</p>
<p>If Y is None, it is set as a pointer to X (i.e. not a copy).
If Y is given, this does not happen.
All distance metrics should use this function first to assert that the
given parameters are correct and safe to use.</p>
<p>Specifically, this function first ensures that both X and Y are arrays,
then checks that they are at least two dimensional while ensuring that
their elements are floats (or dtype if provided). Finally, the function
checks that the size of the second dimension of the two arrays is equal, or
the equivalent check for a precomputed distance matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>precomputed</strong> (<em>bool</em><em>, </em><em>default=False</em>) – True if X is to be treated as precomputed distances to the samples in
Y.</p></li>
<li><p><strong>dtype</strong> (<em>str</em><em>, </em><em>type</em><em>, </em><em>list of type</em><em>, </em><em>default=None</em>) – <p>Data type required for X and Y. If None, the dtype will be an
appropriate float type selected by _return_float_dtype.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
<li><p><strong>accept_sparse</strong> (<em>str</em><em>, </em><em>bool</em><em> or </em><em>list/tuple of str</em><em>, </em><em>default='csr'</em>) – String[s] representing allowed sparse matrix formats, such as ‘csc’,
‘csr’, etc. If the input is sparse but not in the allowed format,
it will be converted to the first listed format. True allows the input
to be any format. False means that a sparse matrix input will
raise an error.</p></li>
<li><p><strong>force_all_finite</strong> (<em>bool</em><em> or </em><em>'allow-nan'</em><em>, </em><em>default=True</em>) – <p>Whether to raise an error on np.inf, np.nan, pd.NA in array. The
possibilities are:</p>
<ul>
<li><p>True: Force all values of array to be finite.</p></li>
<li><p>False: accepts np.inf, np.nan, pd.NA in array.</p></li>
<li><p>’allow-nan’: accepts only np.nan and pd.NA values in array. Values
cannot be infinite.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">force_all_finite</span></code> accepts the string <code class="docutils literal notranslate"><span class="pre">'allow-nan'</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.23: </span>Accepts <cite>pd.NA</cite> and converts it into <cite>np.nan</cite>.</p>
</div>
</p></li>
<li><p><strong>copy</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether a forced copy will be triggered. If copy=False, a copy might
be triggered by a conversion.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>safe_X</strong> (<em>{array-like, sparse matrix} of shape (n_samples_X, n_features)</em>) – An array equal to X, guaranteed to be a numpy array.</p></li>
<li><p><strong>safe_Y</strong> (<em>{array-like, sparse matrix} of shape (n_samples_Y, n_features)</em>) – An array equal to Y if Y was not None, guaranteed to be a numpy array.
If Y was None, safe_Y will be a pointer to X.</p></li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.chi2_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">chi2_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.chi2_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the exponential chi-squared kernel X and Y.</p>
<p>The chi-squared kernel is computed between each pair of rows in X and Y.  X
and Y have to be non-negative. This kernel is most commonly applied to
histograms.</p>
<p>The chi-squared kernel is given by:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">k</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="n">Sum</span> <span class="p">[(</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">^</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)])</span>
</pre></div>
</div>
<p>It can be interpreted as a weighted difference per entry.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=1.</em>) – Scaling parameter of the chi2 kernel.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel_matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.additive_chi2_kernel" title="sklearn.metrics.pairwise.additive_chi2_kernel"><code class="xref py py-obj docutils literal notranslate"><span class="pre">additive_chi2_kernel</span></code></a></dt><dd><p>The additive version of this kernel.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.kernel_approximation.AdditiveChi2Sampler</span></code></dt><dd><p>A Fourier approximation to the additive version of this kernel.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.
Local features and kernels for classification of texture and object
categories: A comprehensive study
International Journal of Computer Vision 2007
<a class="reference external" href="https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf">https://research.microsoft.com/en-us/um/people/manik/projects/trade-off/papers/ZhangIJCV06.pdf</a></p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.cosine_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">cosine_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.cosine_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine distance between samples in X and Y.</p>
<p>Cosine distance is defined as 1.0 minus the cosine similarity.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Matrix <cite>X</cite>.</p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>,             </em><em>default=None</em>) – Matrix <cite>Y</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distance matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.pairwise.cosine_similarity" title="sklearn.metrics.pairwise.cosine_similarity"><code class="xref py py-obj docutils literal notranslate"><span class="pre">cosine_similarity</span></code></a></p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">scipy.spatial.distance.cosine</span></code></dt><dd><p>Dense matrices only.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.cosine_similarity">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">cosine_similarity</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.cosine_similarity" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute cosine similarity between samples in X and Y.</p>
<p>Cosine similarity, or the cosine kernel, computes similarity as the
normalized dot product of X and Y:</p>
<blockquote>
<div><p>K(X, Y) = &lt;X, Y&gt; / (||X||*||Y||)</p>
</div></blockquote>
<p>On L2-normalized data, this function is equivalent to linear_kernel.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Input data.</p></li>
<li><p><strong>Y</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>,             </em><em>default=None</em>) – Input data. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the output will be the pairwise
similarities between all samples in <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
<li><p><strong>dense_output</strong> (<em>bool</em><em>, </em><em>default=True</em>) – <p>Whether to return dense output even when the input is sparse. If
<code class="docutils literal notranslate"><span class="pre">False</span></code>, the output is sparse if both input arrays are sparse.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>parameter <code class="docutils literal notranslate"><span class="pre">dense_output</span></code> for dense output.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.distance_metrics">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">distance_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.distance_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Valid metrics for pairwise_distances.</p>
<p>This function simply returns the valid pairwise distance metrics.
It exists to allow for a description of the mapping for
each of the valid strings.</p>
<p>The valid distance metrics, and the function they map to, are:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 73%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>metric</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>‘cityblock’</p></td>
<td><p>metrics.pairwise.manhattan_distances</p></td>
</tr>
<tr class="row-odd"><td><p>‘cosine’</p></td>
<td><p>metrics.pairwise.cosine_distances</p></td>
</tr>
<tr class="row-even"><td><p>‘euclidean’</p></td>
<td><p>metrics.pairwise.euclidean_distances</p></td>
</tr>
<tr class="row-odd"><td><p>‘haversine’</p></td>
<td><p>metrics.pairwise.haversine_distances</p></td>
</tr>
<tr class="row-even"><td><p>‘l1’</p></td>
<td><p>metrics.pairwise.manhattan_distances</p></td>
</tr>
<tr class="row-odd"><td><p>‘l2’</p></td>
<td><p>metrics.pairwise.euclidean_distances</p></td>
</tr>
<tr class="row-even"><td><p>‘manhattan’</p></td>
<td><p>metrics.pairwise.manhattan_distances</p></td>
</tr>
<tr class="row-odd"><td><p>‘nan_euclidean’</p></td>
<td><p>metrics.pairwise.nan_euclidean_distances</p></td>
</tr>
</tbody>
</table>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.euclidean_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">euclidean_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y_norm_squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_norm_squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Considering the rows of X (and Y=X) as vectors, compute the
distance matrix between each pair of vectors.</p>
<p>For efficiency reasons, the euclidean distance between a pair of row
vector x and y is computed as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>This formulation has two advantages over other ways of computing distances.
First, it is computationally efficient when dealing with sparse data.
Second, if one argument varies but the other remains unchanged, then
<cite>dot(x, x)</cite> and/or <cite>dot(y, y)</cite> can be pre-computed.</p>
<p>However, this is not the most precise way of doing this computation,
because this equation potentially suffers from “catastrophic cancellation”.
Also, the distance matrix returned by this function may not be exactly
symmetric as required by, e.g., <code class="docutils literal notranslate"><span class="pre">scipy.spatial.distance</span></code> functions.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>,             </em><em>default=None</em>) – </p></li>
<li><p><strong>Y_norm_squared</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>,</em><em>) or </em><em>(</em><em>n_samples_Y</em><em>, </em><em>1</em><em>)             or </em><em>(</em><em>1</em><em>, </em><em>n_samples_Y</em><em>)</em><em>, </em><em>default=None</em>) – Pre-computed dot-products of vectors in Y (e.g.,
<code class="docutils literal notranslate"><span class="pre">(Y**2).sum(axis=1)</span></code>)
May be ignored in some cases, see the note below.</p></li>
<li><p><strong>squared</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Return squared Euclidean distances.</p></li>
<li><p><strong>X_norm_squared</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>,</em><em>) or </em><em>(</em><em>n_samples_X</em><em>, </em><em>1</em><em>)             or </em><em>(</em><em>1</em><em>, </em><em>n_samples_X</em><em>)</em><em>, </em><em>default=None</em>) – Pre-computed dot-products of vectors in X (e.g.,
<code class="docutils literal notranslate"><span class="pre">(X**2).sum(axis=1)</span></code>)
May be ignored in some cases, see the note below.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>To achieve better accuracy, <cite>X_norm_squared</cite> and <cite>Y_norm_squared</cite> may be
unused if they are passed as <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>distances</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.paired_distances" title="sklearn.metrics.pairwise.paired_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></a></dt><dd><p>Distances betweens pairs of elements of X and Y.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># distance between rows of X</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="go">array([[0., 1.],</span>
<span class="go">       [1., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get distance to origin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[1.        ],</span>
<span class="go">       [1.41421356]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.haversine_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">haversine_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.haversine_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Haversine distance between samples in X and Y.</p>
<p>The Haversine (or great circle) distance is the angular distance between
two points on the surface of a sphere. The first coordinate of each point
is assumed to be the latitude, the second is the longitude, given
in radians. The dimension of the data must be 2.</p>
<div class="math notranslate nohighlight">
\[D(x, y) = 2\arcsin[\sqrt{\sin^2((x1 - y1) / 2)
                         + \cos(x1)\cos(y1)\sin^2((x2 - y2) / 2)}]\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>2</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>2</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distance</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>As the Earth is nearly spherical, the haversine formula provides a good
approximation of the distance between two points of the Earth surface, with
a less than 1% error on average.</p>
<p class="rubric">Examples</p>
<p>We want to calculate the distance between the Ezeiza Airport
(Buenos Aires, Argentina) and the Charles de Gaulle Airport (Paris,
France).</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">haversine_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">radians</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsas</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">34.83333</span><span class="p">,</span> <span class="o">-</span><span class="mf">58.5166646</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paris</span> <span class="o">=</span> <span class="p">[</span><span class="mf">49.0083899664</span><span class="p">,</span> <span class="mf">2.53844117956</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bsas_in_radians</span> <span class="o">=</span> <span class="p">[</span><span class="n">radians</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">bsas</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paris_in_radians</span> <span class="o">=</span> <span class="p">[</span><span class="n">radians</span><span class="p">(</span><span class="n">_</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">paris</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">=</span> <span class="n">haversine_distances</span><span class="p">([</span><span class="n">bsas_in_radians</span><span class="p">,</span> <span class="n">paris_in_radians</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">result</span> <span class="o">*</span> <span class="mi">6371000</span><span class="o">/</span><span class="mi">1000</span>  <span class="c1"># multiply by Earth radius to get kilometers</span>
<span class="go">array([[    0.        , 11099.54035582],</span>
<span class="go">       [11099.54035582,     0.        ]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.kernel_metrics">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">kernel_metrics</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.kernel_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Valid metrics for pairwise_kernels.</p>
<p>This function simply returns the valid pairwise distance metrics.
It exists, however, to allow for a verbose description of the mapping for
each of the valid strings.</p>
<dl>
<dt>The valid distance metrics, and the function they map to, are:</dt><dd><table class="docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 73%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>metric</p></th>
<th class="head"><p>Function</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>‘additive_chi2’</p></td>
<td><p>sklearn.pairwise.additive_chi2_kernel</p></td>
</tr>
<tr class="row-odd"><td><p>‘chi2’</p></td>
<td><p>sklearn.pairwise.chi2_kernel</p></td>
</tr>
<tr class="row-even"><td><p>‘linear’</p></td>
<td><p>sklearn.pairwise.linear_kernel</p></td>
</tr>
<tr class="row-odd"><td><p>‘poly’</p></td>
<td><p>sklearn.pairwise.polynomial_kernel</p></td>
</tr>
<tr class="row-even"><td><p>‘polynomial’</p></td>
<td><p>sklearn.pairwise.polynomial_kernel</p></td>
</tr>
<tr class="row-odd"><td><p>‘rbf’</p></td>
<td><p>sklearn.pairwise.rbf_kernel</p></td>
</tr>
<tr class="row-even"><td><p>‘laplacian’</p></td>
<td><p>sklearn.pairwise.laplacian_kernel</p></td>
</tr>
<tr class="row-odd"><td><p>‘sigmoid’</p></td>
<td><p>sklearn.pairwise.sigmoid_kernel</p></td>
</tr>
<tr class="row-even"><td><p>‘cosine’</p></td>
<td><p>sklearn.pairwise.cosine_similarity</p></td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.laplacian_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">laplacian_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.laplacian_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the laplacian kernel between X and Y.</p>
<p>The laplacian kernel is defined as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">K</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">||</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="o">||</span><span class="n">_1</span><span class="p">)</span>
</pre></div>
</div>
<p>for each pair of rows x in X and y in Y.
Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=None</em>) – If None, defaults to 1.0 / n_features.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel_matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.linear_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">linear_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dense_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.linear_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the linear kernel between X and Y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>dense_output</strong> (<em>bool</em><em>, </em><em>default=True</em>) – <p>Whether to return dense output even when the input is sparse. If
<code class="docutils literal notranslate"><span class="pre">False</span></code>, the output is sparse if both input arrays are sparse.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Gram matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.manhattan_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">manhattan_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sum_over_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.manhattan_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the L1 distances between the vectors in X and Y.</p>
<p>With sum_over_features equal to False it returns the componentwise
distances.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>sum_over_features</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True the function returns the pairwise distance matrix
else it returns the componentwise L1 pairwise-distances.
Not supported for sparse matrix inputs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>D</strong> – If sum_over_features is False shape is
(n_samples_X * n_samples_Y, n_features) and D contains the
componentwise L1 pairwise-distances (ie. absolute difference),
else shape is (n_samples_X, n_samples_Y) and D contains
the pairwise L1 distances.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X * n_samples_Y, n_features) or             (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>When X and/or Y are CSR sparse matrices and they are not already
in canonical format, this function modifies them in-place to
make them canonical.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">manhattan_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manhattan_distances</span><span class="p">([[</span><span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">]])</span>
<span class="go">array([[0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manhattan_distances</span><span class="p">([[</span><span class="mi">3</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">2</span><span class="p">]])</span>
<span class="go">array([[1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manhattan_distances</span><span class="p">([[</span><span class="mi">2</span><span class="p">]],</span> <span class="p">[[</span><span class="mi">3</span><span class="p">]])</span>
<span class="go">array([[1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manhattan_distances</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span>         <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">]])</span>
<span class="go">array([[0., 2.],</span>
<span class="go">       [4., 4.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="mf">2.</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">manhattan_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sum_over_features</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">array([[1., 1.],</span>
<span class="go">       [1., 1.]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.nan_euclidean_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">nan_euclidean_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">nan</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.nan_euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the euclidean distances in the presence of missing values.</p>
<p>Compute the euclidean distance between each pair of samples in X and Y,
where Y=X is assumed if Y=None. When calculating the distance between a
pair of samples, this formulation ignores feature coordinates with a
missing value in either sample and scales up the weight of the remaining
coordinates:</p>
<blockquote>
<div><p>dist(x,y) = sqrt(weight * sq. distance from present coordinates)
where,
weight = Total # of coordinates / # of present coordinates</p>
</div></blockquote>
<p>For example, the distance between <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">na,</span> <span class="pre">na,</span> <span class="pre">6]</span></code> and <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">na,</span> <span class="pre">4,</span> <span class="pre">5]</span></code>
is:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\sqrt{\frac{4}{2}((3-1)^2 + (6-5)^2)}\]</div>
</div></blockquote>
<p>If all the coordinates are missing or if there are no common present
coordinates then NaN is returned for that pair.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape=</em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape=</em><em>(</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>squared</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Return squared Euclidean distances.</p></li>
<li><p><strong>missing_values</strong> (<em>np.nan</em><em> or </em><em>int</em><em>, </em><em>default=np.nan</em>) – Representation of missing value.</p></li>
<li><p><strong>copy</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Make and use a deep copy of X and Y (if Y exists).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.paired_distances" title="sklearn.metrics.pairwise.paired_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></a></dt><dd><p>Distances between pairs of elements of X and Y.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">nan_euclidean_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;NaN&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">nan</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan_euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># distance between rows of X</span>
<span class="go">array([[0.        , 1.41421356],</span>
<span class="go">       [1.41421356, 0.        ]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># get distance to origin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan_euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[1.        ],</span>
<span class="go">       [1.41421356]])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>John K. Dixon, “Pattern Recognition with Partly Missing Data”,
IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:
10, pp. 617 - 621, Oct. 1979.
<a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/4310090/">http://ieeexplore.ieee.org/abstract/document/4310090/</a></p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.paired_cosine_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">paired_cosine_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.paired_cosine_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the paired cosine distances between X and Y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The cosine distance is equivalent to the half the squared
euclidean distance if each sample is normalized to unit norm.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.paired_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">paired_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.paired_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the paired distances between X and Y.</p>
<p>Computes the distances between (X[0], Y[0]), (X[1], Y[1]), etc…</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Array 1 for distance computation.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Array 2 for distance computation.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=&quot;euclidean&quot;</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
specified in PAIRED_DISTANCES, including “euclidean”,
“manhattan”, or “cosine”.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.pairwise_distances" title="sklearn.metrics.pairwise.pairwise_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pairwise_distances</span></code></a></dt><dd><p>Computes the distance between every pair of samples.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">paired_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">paired_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">array([0., 1.])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.paired_euclidean_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">paired_euclidean_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.paired_euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the paired euclidean distances between X and Y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.paired_manhattan_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">paired_manhattan_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.paired_manhattan_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the L1 distances between the vectors in X and Y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.pairwise_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_all_finite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.pairwise_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the distance matrix from a vector array X and optional Y.</p>
<p>This method takes either a vector array or a distance matrix, and returns
a distance matrix. If the input is a vector array, the distances are
computed. If the input is a distances matrix, it is returned instead.</p>
<p>This method provides a safe way to take a distance matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.</p>
<p>If Y is given (default is None), then the returned matrix is the pairwise
distance between the arrays from both X and Y.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]. These metrics support sparse matrix
inputs.
[‘nan_euclidean’] but it does not yet support sparse matrices.</p></li>
<li><p>From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’,
‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,
‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]
See the documentation for scipy.spatial.distance for details on these
metrics. These metrics do not support sparse matrix inputs.</p></li>
</ul>
<p>Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are
valid scipy.spatial.distance metrics), the scikit-learn implementation
will be used, which is faster and has support for sparse matrices (except
for ‘cityblock’). For a verbose description of the metrics from
scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics
function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise distances between samples, or a feature array.
The shape of the array should be (n_samples_X, n_samples_X) if
metric == “precomputed” and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – An optional second feature array. Only allowed if
metric != “precomputed”.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by scipy.spatial.distance.pdist for its metric parameter, or
a metric listed in <code class="docutils literal notranslate"><span class="pre">pairwise.PAIRWISE_DISTANCE_FUNCTIONS</span></code>.
If metric is “precomputed”, X is assumed to be a distance matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>force_all_finite</strong> (<em>bool</em><em> or </em><em>'allow-nan'</em><em>, </em><em>default=True</em>) – <p>Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
for a metric listed in <code class="docutils literal notranslate"><span class="pre">pairwise.PAIRWISE_DISTANCE_FUNCTIONS</span></code>. The
possibilities are:</p>
<ul>
<li><p>True: Force all values of array to be finite.</p></li>
<li><p>False: accepts np.inf, np.nan, pd.NA in array.</p></li>
<li><p>’allow-nan’: accepts only np.nan and pd.NA values in array. Values
cannot be infinite.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">force_all_finite</span></code> accepts the string <code class="docutils literal notranslate"><span class="pre">'allow-nan'</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.23: </span>Accepts <cite>pd.NA</cite> and converts it into <cite>np.nan</cite>.</p>
</div>
</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>D</strong> – A distance matrix D such that D_{i, j} is the distance between the
ith and jth vectors of the given matrix X, if Y is None.
If Y is not None, then D_{i, j} is the distance between the ith array
from X and the jth array from Y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.pairwise_distances_chunked" title="sklearn.metrics.pairwise.pairwise_distances_chunked"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pairwise_distances_chunked</span></code></a></dt><dd><p>Performs the same calculation as this function, but returns a generator of chunks of the distance matrix, in order to limit memory usage.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.pairwise.paired_distances" title="sklearn.metrics.pairwise.paired_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></a></dt><dd><p>Computes the distances between corresponding elements of two arrays.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.pairwise_distances_argmin">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_argmin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.pairwise_distances_argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute minimum distances between one point and a set of points.</p>
<p>This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance).</p>
<p>This is mostly equivalent to calling:</p>
<blockquote>
<div><p>pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)</p>
</div></blockquote>
<p>but uses much less memory, and is faster for large arrays.</p>
<p>This function works with dense 2D arrays only.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – Arrays containing points.</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>default=1</em>) – Axis along which the argmin and distances are to be computed.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=&quot;euclidean&quot;</em>) – <p>Metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>metric_kwargs</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Keyword arguments to pass to specified metric function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>argmin</strong> – Y[argmin[i], :] is the row in Y that is closest to X[i, :].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances</span></code></a>, <a class="reference internal" href="#sklearn.metrics.pairwise_distances_argmin_min" title="sklearn.metrics.pairwise_distances_argmin_min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances_argmin_min</span></code></a></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.pairwise_distances_argmin_min">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_argmin_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.pairwise_distances_argmin_min" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute minimum distances between one point and a set of points.</p>
<p>This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance). The minimal distances are
also returned.</p>
<p>This is mostly equivalent to calling:</p>
<blockquote>
<div><dl class="simple">
<dt>(pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),</dt><dd><p>pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))</p>
</dd>
</dl>
</div></blockquote>
<p>but uses much less memory, and is faster for large arrays.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>default=1</em>) – Axis along which the argmin and distances are to be computed.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – <p>Metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>metric_kwargs</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Keyword arguments to pass to specified metric function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>argmin</strong> (<em>ndarray</em>) – Y[argmin[i], :] is the row in Y that is closest to X[i, :].</p></li>
<li><p><strong>distances</strong> (<em>ndarray</em>) – distances[i] is the distance between the i-th row in X and the
argmin[i]-th row in Y.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances</span></code></a>, <a class="reference internal" href="#sklearn.metrics.pairwise_distances_argmin" title="sklearn.metrics.pairwise_distances_argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances_argmin</span></code></a></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.pairwise_distances_chunked">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_chunked</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">working_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.pairwise_distances_chunked" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a distance matrix chunk by chunk with optional reduction.</p>
<p>In cases where not all of a pairwise distance matrix needs to be stored at
once, this is used to calculate pairwise distances in
<code class="docutils literal notranslate"><span class="pre">working_memory</span></code>-sized chunks.  If <code class="docutils literal notranslate"><span class="pre">reduce_func</span></code> is given, it is run
on each chunk and its return values are concatenated into lists, arrays
or sparse matrices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise distances between samples, or a feature array.
The shape the array should be (n_samples_X, n_samples_X) if
metric=’precomputed’ and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – An optional second feature array. Only allowed if
metric != “precomputed”.</p></li>
<li><p><strong>reduce_func</strong> (<em>callable</em><em>, </em><em>default=None</em>) – <p>The function which is applied on each chunk of the distance matrix,
reducing it to needed values.  <code class="docutils literal notranslate"><span class="pre">reduce_func(D_chunk,</span> <span class="pre">start)</span></code>
is called repeatedly, where <code class="docutils literal notranslate"><span class="pre">D_chunk</span></code> is a contiguous vertical
slice of the pairwise distance matrix, starting at row <code class="docutils literal notranslate"><span class="pre">start</span></code>.
It should return one of: None; an array, a list, or a sparse matrix
of length <code class="docutils literal notranslate"><span class="pre">D_chunk.shape[0]</span></code>; or a tuple of such objects. Returning
None is useful for in-place operations, rather than reductions.</p>
<p>If None, pairwise_distances_chunked returns a generator of vertical
chunks of the distance matrix.</p>
</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by scipy.spatial.distance.pdist for its metric parameter, or
a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
If metric is “precomputed”, X is assumed to be a distance matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>working_memory</strong> (<em>int</em><em>, </em><em>default=None</em>) – The sought maximum memory for temporary distance matrix chunks.
When None (default), the value of
<code class="docutils literal notranslate"><span class="pre">sklearn.get_config()['working_memory']</span></code> is used.</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>D_chunk</strong> (<em>{ndarray, sparse matrix}</em>) – A contiguous slice of distance matrix, optionally processed by
<code class="docutils literal notranslate"><span class="pre">reduce_func</span></code>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Without reduce_func:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances_chunked</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D_chunk</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D_chunk</span>
<span class="go">array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],</span>
<span class="go">       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],</span>
<span class="go">       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],</span>
<span class="go">       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],</span>
<span class="go">       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])</span>
</pre></div>
</div>
<p>Retrieve all neighbors and average distance within radius r:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="mf">.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">reduce_func</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">neigh</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">D_chunk</span><span class="p">]</span>
<span class="gp">... </span>    <span class="n">avg_dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">D_chunk</span> <span class="o">*</span> <span class="p">(</span><span class="n">D_chunk</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">neigh</span><span class="p">,</span> <span class="n">avg_dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="p">,</span> <span class="n">avg_dist</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span>
<span class="go">[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">avg_dist</span>
<span class="go">array([0.039..., 0.        , 0.        , 0.039..., 0.        ])</span>
</pre></div>
</div>
<p>Where r is defined per sample, we need to make use of <code class="docutils literal notranslate"><span class="pre">start</span></code>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.2</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">reduce_func</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">neigh</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="gp">... </span>             <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">)]</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">neigh</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span>
<span class="go">[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]</span>
</pre></div>
</div>
<p>Force row-by-row generation by reducing <code class="docutils literal notranslate"><span class="pre">working_memory</span></code>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">working_memory</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="go">[array([0, 3])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="go">[array([0, 1])]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.pairwise_kernels">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">pairwise_kernels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.pairwise_kernels" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the kernel between arrays X and optional array Y.</p>
<p>This method takes either a vector array or a kernel matrix, and returns
a kernel matrix. If the input is a vector array, the kernels are
computed. If the input is a kernel matrix, it is returned instead.</p>
<p>This method provides a safe way to take a kernel matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.</p>
<p>If Y is given (default is None), then the returned matrix is the pairwise
kernel between the arrays from both X and Y.</p>
<dl class="simple">
<dt>Valid values for metric are:</dt><dd><p>[‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’,
‘laplacian’, ‘sigmoid’, ‘cosine’]</p>
</dd>
</dl>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise kernels between samples, or a feature array.
The shape of the array should be (n_samples_X, n_samples_X) if
metric == “precomputed” and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – A second feature array only if X has shape (n_samples_X, n_features).</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=&quot;linear&quot;</em>) – The metric to use when calculating kernel between instances in a
feature array. If metric is a string, it must be one of the metrics
in pairwise.PAIRWISE_KERNEL_FUNCTIONS.
If metric is “precomputed”, X is assumed to be a kernel matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two rows from X as input and return the corresponding
kernel value as a single number. This means that callables from
<a class="reference internal" href="#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> are not allowed, as they operate on
matrices, not single samples. Use the string identifying the kernel
instead.</p></li>
<li><p><strong>filter_params</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to filter invalid parameters or not.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the kernel function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – A kernel matrix K such that K_{i, j} is the kernel between the
ith and jth vectors of the given matrix X, if Y is None.
If Y is not None, then K_{i, j} is the kernel between the ith array
from X and the jth array from Y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If metric is ‘precomputed’, Y is ignored and X is returned.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.polynomial_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">polynomial_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">degree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.polynomial_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the polynomial kernel between X and Y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="o">=</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">&lt;</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">&gt;</span> <span class="o">+</span> <span class="n">coef0</span><span class="p">)</span><span class="o">^</span><span class="n">degree</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>degree</strong> (<em>int</em><em>, </em><em>default=3</em>) – </p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=None</em>) – If None, defaults to 1.0 / n_features.</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=1</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Gram matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.rbf_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">rbf_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.rbf_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the rbf (gaussian) kernel between X and Y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">K</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">||</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="o">||^</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>for each pair of rows x in X and y in Y.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=None</em>) – If None, defaults to 1.0 / n_features.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kernel_matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise.sigmoid_kernel">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.pairwise.</span></span><span class="sig-name descname"><span class="pre">sigmoid_kernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gamma</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">coef0</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise.sigmoid_kernel" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the sigmoid kernel between X and Y:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">K</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> <span class="o">=</span> <span class="n">tanh</span><span class="p">(</span><span class="n">gamma</span> <span class="o">&lt;</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="o">&gt;</span> <span class="o">+</span> <span class="n">coef0</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>gamma</strong> (<em>float</em><em>, </em><em>default=None</em>) – If None, defaults to 1.0 / n_features.</p></li>
<li><p><strong>coef0</strong> (<em>float</em><em>, </em><em>default=1</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>Gram matrix</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-sklearn.metrics.setup">
<span id="sklearn-metrics-setup-module"></span><h2>sklearn.metrics.setup module<a class="headerlink" href="#module-sklearn.metrics.setup" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.setup.configuration">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.setup.</span></span><span class="sig-name descname"><span class="pre">configuration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parent_package</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.setup.configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-sklearn.metrics">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sklearn.metrics" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#module-sklearn.metrics" title="sklearn.metrics"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics</span></code></a> module includes score functions, performance metrics
and pairwise metrics and distance computations.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">ConfusionMatrixDisplay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">confusion_matrix</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Confusion Matrix visualization.</p>
<p>It is recommend to use
<a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="sklearn.metrics.ConfusionMatrixDisplay.from_estimator"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_estimator()</span></code></a> or
<a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_predictions()</span></code></a> to
create a <a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a>. All parameters are stored as
attributes.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>confusion_matrix</strong> (<em>ndarray of shape</em><em> (</em><em>n_classes</em><em>, </em><em>n_classes</em><em>)</em>) – Confusion matrix.</p></li>
<li><p><strong>display_labels</strong> (<em>ndarray of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Display labels for plot. If None, display labels are set from 0 to
<cite>n_classes - 1</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.im_">
<span class="sig-name descname"><span class="pre">im_</span></span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.im_" title="Permalink to this definition">¶</a></dt>
<dd><p>Image representing the confusion matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib AxesImage</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.text_">
<span class="sig-name descname"><span class="pre">text_</span></span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.text_" title="Permalink to this definition">¶</a></dt>
<dd><p>Array of matplotlib axes. <cite>None</cite> if <cite>include_values</cite> is false.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_classes), dtype=matplotlib Text,             or None</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.ax_">
<span class="sig-name descname"><span class="pre">ax_</span></span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.ax_" title="Permalink to this definition">¶</a></dt>
<dd><p>Axes with confusion matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.figure_">
<span class="sig-name descname"><span class="pre">figure_</span></span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.figure_" title="Permalink to this definition">¶</a></dt>
<dd><p>Figure containing the confusion matrix.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Figure</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a></dt><dd><p>Compute Confusion Matrix to evaluate the accuracy of a classification.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="sklearn.metrics.ConfusionMatrixDisplay.from_estimator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_estimator</span></code></a></dt><dd><p>Plot the confusion matrix given an estimator, the data, and the label.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_predictions</span></code></a></dt><dd><p>Plot the confusion matrix given the true and predicted labels.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span>                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">disp</span> <span class="o">=</span> <span class="n">ConfusionMatrixDisplay</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span>
<span class="gp">... </span>                              <span class="n">display_labels</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span> 
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.from_estimator">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">from_estimator</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xticks_rotation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'horizontal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'viridis'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colorbar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot Confusion Matrix given an estimator and some data.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 1.0.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator instance</em>) – Fitted classifier or a fitted <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>
in which the last estimator is a classifier.</p></li>
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input values.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – List of labels to index the confusion matrix. This may be used to
reorder or select a subset of labels. If <cite>None</cite> is given, those
that appear at least once in <cite>y_true</cite> or <cite>y_pred</cite> are used in
sorted order.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>normalize</strong> (<em>{'true'</em><em>, </em><em>'pred'</em><em>, </em><em>'all'}</em><em>, </em><em>default=None</em>) – <p>Either to normalize the counts display in the matrix:</p>
<ul>
<li><p>if <cite>‘true’</cite>, the confusion matrix is normalized over the true
conditions (e.g. rows);</p></li>
<li><p>if <cite>‘pred’</cite>, the confusion matrix is normalized over the
predicted conditions (e.g. columns);</p></li>
<li><p>if <cite>‘all’</cite>, the confusion matrix is normalized by the total
number of samples;</p></li>
<li><p>if <cite>None</cite> (default), the confusion matrix will not be normalized.</p></li>
</ul>
</p></li>
<li><p><strong>display_labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Target names used for plotting. By default, <cite>labels</cite> will be used
if it is defined, otherwise the unique labels of <cite>y_true</cite> and
<cite>y_pred</cite> will be used.</p></li>
<li><p><strong>include_values</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Includes values in confusion matrix.</p></li>
<li><p><strong>xticks_rotation</strong> (<em>{'vertical'</em><em>, </em><em>'horizontal'}</em><em> or </em><em>float</em><em>,                 </em><em>default='horizontal'</em>) – Rotation of xtick labels.</p></li>
<li><p><strong>values_format</strong> (<em>str</em><em>, </em><em>default=None</em>) – Format specification for values in confusion matrix. If <cite>None</cite>, the
format specification is ‘d’ or ‘.2g’ whichever is shorter.</p></li>
<li><p><strong>cmap</strong> (<em>str</em><em> or </em><em>matplotlib Colormap</em><em>, </em><em>default='viridis'</em>) – Colormap recognized by matplotlib.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib Axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>colorbar</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to add a colorbar to the plot.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_predictions</span></code></a></dt><dd><p>Plot the confusion matrix given the true and predicted labels.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ConfusionMatrixDisplay</span><span class="o">.</span><span class="n">from_estimator</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.from_predictions">
<em class="property"><span class="pre">classmethod</span> </em><span class="sig-name descname"><span class="pre">from_predictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xticks_rotation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'horizontal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'viridis'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colorbar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot Confusion Matrix given true and predicted labels.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True labels.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The predicted labels given by the method <cite>predict</cite> of an
classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – List of labels to index the confusion matrix. This may be used to
reorder or select a subset of labels. If <cite>None</cite> is given, those
that appear at least once in <cite>y_true</cite> or <cite>y_pred</cite> are used in
sorted order.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>normalize</strong> (<em>{'true'</em><em>, </em><em>'pred'</em><em>, </em><em>'all'}</em><em>, </em><em>default=None</em>) – <p>Either to normalize the counts display in the matrix:</p>
<ul>
<li><p>if <cite>‘true’</cite>, the confusion matrix is normalized over the true
conditions (e.g. rows);</p></li>
<li><p>if <cite>‘pred’</cite>, the confusion matrix is normalized over the
predicted conditions (e.g. columns);</p></li>
<li><p>if <cite>‘all’</cite>, the confusion matrix is normalized by the total
number of samples;</p></li>
<li><p>if <cite>None</cite> (default), the confusion matrix will not be normalized.</p></li>
</ul>
</p></li>
<li><p><strong>display_labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Target names used for plotting. By default, <cite>labels</cite> will be used
if it is defined, otherwise the unique labels of <cite>y_true</cite> and
<cite>y_pred</cite> will be used.</p></li>
<li><p><strong>include_values</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Includes values in confusion matrix.</p></li>
<li><p><strong>xticks_rotation</strong> (<em>{'vertical'</em><em>, </em><em>'horizontal'}</em><em> or </em><em>float</em><em>,                 </em><em>default='horizontal'</em>) – Rotation of xtick labels.</p></li>
<li><p><strong>values_format</strong> (<em>str</em><em>, </em><em>default=None</em>) – Format specification for values in confusion matrix. If <cite>None</cite>, the
format specification is ‘d’ or ‘.2g’ whichever is shorter.</p></li>
<li><p><strong>cmap</strong> (<em>str</em><em> or </em><em>matplotlib Colormap</em><em>, </em><em>default='viridis'</em>) – Colormap recognized by matplotlib.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib Axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>colorbar</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to add a colorbar to the plot.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="sklearn.metrics.ConfusionMatrixDisplay.from_estimator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_estimator</span></code></a></dt><dd><p>Plot the confusion matrix given an estimator, the data, and the label.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ConfusionMatrixDisplay</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ConfusionMatrixDisplay</span><span class="o">.</span><span class="n">from_predictions</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.ConfusionMatrixDisplay.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'viridis'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xticks_rotation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'horizontal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colorbar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.ConfusionMatrixDisplay.plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot visualization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>include_values</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Includes values in confusion matrix.</p></li>
<li><p><strong>cmap</strong> (<em>str</em><em> or </em><em>matplotlib Colormap</em><em>, </em><em>default='viridis'</em>) – Colormap recognized by matplotlib.</p></li>
<li><p><strong>xticks_rotation</strong> (<em>{'vertical'</em><em>, </em><em>'horizontal'}</em><em> or </em><em>float</em><em>,                          </em><em>default='horizontal'</em>) – Rotation of xtick labels.</p></li>
<li><p><strong>values_format</strong> (<em>str</em><em>, </em><em>default=None</em>) – Format specification for values in confusion matrix. If <cite>None</cite>,
the format specification is ‘d’ or ‘.2g’ whichever is shorter.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>colorbar</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to add a colorbar to the plot.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.metrics.DetCurveDisplay">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">DetCurveDisplay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fpr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fnr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.DetCurveDisplay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>DET curve visualization.</p>
<p>It is recommend to use <a class="reference internal" href="#sklearn.metrics.plot_det_curve" title="sklearn.metrics.plot_det_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_det_curve()</span></code></a> to create a
visualizer. All parameters are stored as attributes.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fpr</strong> (<em>ndarray</em>) – False positive rate.</p></li>
<li><p><strong>fnr</strong> (<em>ndarray</em>) – False negative rate.</p></li>
<li><p><strong>estimator_name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of estimator. If None, the estimator name is not shown.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – The label of the positive class.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.DetCurveDisplay.line_">
<span class="sig-name descname"><span class="pre">line_</span></span><a class="headerlink" href="#sklearn.metrics.DetCurveDisplay.line_" title="Permalink to this definition">¶</a></dt>
<dd><p>DET Curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Artist</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.DetCurveDisplay.ax_">
<span class="sig-name descname"><span class="pre">ax_</span></span><a class="headerlink" href="#sklearn.metrics.DetCurveDisplay.ax_" title="Permalink to this definition">¶</a></dt>
<dd><p>Axes with DET Curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.DetCurveDisplay.figure_">
<span class="sig-name descname"><span class="pre">figure_</span></span><a class="headerlink" href="#sklearn.metrics.DetCurveDisplay.figure_" title="Permalink to this definition">¶</a></dt>
<dd><p>Figure containing the curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Figure</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.det_curve" title="sklearn.metrics.det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det_curve</span></code></a></dt><dd><p>Compute error rates for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.plot_det_curve" title="sklearn.metrics.plot_det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_det_curve</span></code></a></dt><dd><p>Plot detection error tradeoff (DET) curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">det_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">display</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">DetCurveDisplay</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">fpr</span><span class="o">=</span><span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="o">=</span><span class="n">fnr</span><span class="p">,</span> <span class="n">estimator_name</span><span class="o">=</span><span class="s1">&#39;example estimator&#39;</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">display</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>      
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.DetCurveDisplay.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.DetCurveDisplay.plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot visualization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of DET curve for labeling. If <cite>None</cite>, use the name of the
estimator.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">DetCurveDisplay</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.metrics.PrecisionRecallDisplay">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">PrecisionRecallDisplay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">precision</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recall</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.PrecisionRecallDisplay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Precision Recall visualization.</p>
<p>It is recommend to use <a class="reference internal" href="#sklearn.metrics.plot_precision_recall_curve" title="sklearn.metrics.plot_precision_recall_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_precision_recall_curve()</span></code></a>
to create a visualizer. All parameters are stored as attributes.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>precision</strong> (<em>ndarray</em>) – Precision values.</p></li>
<li><p><strong>recall</strong> (<em>ndarray</em>) – Recall values.</p></li>
<li><p><strong>average_precision</strong> (<em>float</em><em>, </em><em>default=None</em>) – Average precision. If None, the average precision is not shown.</p></li>
<li><p><strong>estimator_name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of estimator. If None, then the estimator name is not shown.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – <p>The class considered as the positive class. If None, the class will not
be shown in the legend.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.PrecisionRecallDisplay.line_">
<span class="sig-name descname"><span class="pre">line_</span></span><a class="headerlink" href="#sklearn.metrics.PrecisionRecallDisplay.line_" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision recall curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Artist</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.PrecisionRecallDisplay.ax_">
<span class="sig-name descname"><span class="pre">ax_</span></span><a class="headerlink" href="#sklearn.metrics.PrecisionRecallDisplay.ax_" title="Permalink to this definition">¶</a></dt>
<dd><p>Axes with precision recall curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.PrecisionRecallDisplay.figure_">
<span class="sig-name descname"><span class="pre">figure_</span></span><a class="headerlink" href="#sklearn.metrics.PrecisionRecallDisplay.figure_" title="Permalink to this definition">¶</a></dt>
<dd><p>Figure containing the curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Figure</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a></dt><dd><p>Compute precision-recall pairs for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.plot_precision_recall_curve" title="sklearn.metrics.plot_precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_precision_recall_curve</span></code></a></dt><dd><p>Plot Precision Recall Curve for binary classifiers.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="p">(</span><span class="n">precision_recall_curve</span><span class="p">,</span>
<span class="gp">... </span>                             <span class="n">PrecisionRecallDisplay</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span>                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">disp</span> <span class="o">=</span> <span class="n">PrecisionRecallDisplay</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="o">=</span><span class="n">recall</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">disp</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span> 
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.PrecisionRecallDisplay.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.PrecisionRecallDisplay.plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot visualization.</p>
<p>Extra keyword arguments will be passed to matplotlib’s <cite>plot</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ax</strong> (<em>Matplotlib Axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of precision recall curve for labeling. If <cite>None</cite>, use the
name of the estimator.</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em>) – Keyword arguments to be passed to matplotlib’s <cite>plot</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.PrecisionRecallDisplay" title="sklearn.metrics.PrecisionRecallDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">PrecisionRecallDisplay</span></code></a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.metrics.RocCurveDisplay">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">RocCurveDisplay</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fpr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tpr</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">roc_auc</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.RocCurveDisplay" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>ROC Curve visualization.</p>
<p>It is recommend to use <a class="reference internal" href="#sklearn.metrics.plot_roc_curve" title="sklearn.metrics.plot_roc_curve"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_roc_curve()</span></code></a> to create a
visualizer. All parameters are stored as attributes.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fpr</strong> (<em>ndarray</em>) – False positive rate.</p></li>
<li><p><strong>tpr</strong> (<em>ndarray</em>) – True positive rate.</p></li>
<li><p><strong>roc_auc</strong> (<em>float</em><em>, </em><em>default=None</em>) – Area under ROC curve. If None, the roc_auc score is not shown.</p></li>
<li><p><strong>estimator_name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of estimator. If None, the estimator name is not shown.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – <p>The class considered as the positive class when computing the roc auc
metrics. By default, <cite>estimators.classes_[1]</cite> is considered
as the positive class.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.RocCurveDisplay.line_">
<span class="sig-name descname"><span class="pre">line_</span></span><a class="headerlink" href="#sklearn.metrics.RocCurveDisplay.line_" title="Permalink to this definition">¶</a></dt>
<dd><p>ROC Curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Artist</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.RocCurveDisplay.ax_">
<span class="sig-name descname"><span class="pre">ax_</span></span><a class="headerlink" href="#sklearn.metrics.RocCurveDisplay.ax_" title="Permalink to this definition">¶</a></dt>
<dd><p>Axes with ROC Curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Axes</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.metrics.RocCurveDisplay.figure_">
<span class="sig-name descname"><span class="pre">figure_</span></span><a class="headerlink" href="#sklearn.metrics.RocCurveDisplay.figure_" title="Permalink to this definition">¶</a></dt>
<dd><p>Figure containing the curve.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>matplotlib Figure</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a></dt><dd><p>Compute Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.plot_roc_curve" title="sklearn.metrics.plot_roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_roc_curve</span></code></a></dt><dd><p>Plot Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></dt><dd><p>Compute the area under the ROC curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">display</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">RocCurveDisplay</span><span class="p">(</span><span class="n">fpr</span><span class="o">=</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="o">=</span><span class="n">tpr</span><span class="p">,</span> <span class="n">roc_auc</span><span class="o">=</span><span class="n">roc_auc</span><span class="p">,</span>                                          <span class="n">estimator_name</span><span class="o">=</span><span class="s1">&#39;example estimator&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">display</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>      
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.metrics.RocCurveDisplay.plot">
<span class="sig-name descname"><span class="pre">plot</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.RocCurveDisplay.plot" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot visualization</p>
<p>Extra keyword arguments will be passed to matplotlib’s <code class="docutils literal notranslate"><span class="pre">plot</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of ROC Curve for labeling. If <cite>None</cite>, use the name of the
estimator.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">RocCurveDisplay</span></code></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.accuracy_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">accuracy_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.accuracy_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Accuracy classification score.</p>
<p>In multilabel classification, this function computes subset accuracy:
the set of labels predicted for a sample must <em>exactly</em> match the
corresponding set of labels in y_true.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) labels.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Predicted labels, as returned by a classifier.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, return the number of correctly classified samples.
Otherwise, return the fraction of correctly classified samples.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>score</strong> – If <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">True</span></code>, return the fraction of correctly
classified samples (float), else returns the number of correctly
classified samples (int).</p>
<p>The best performance is 1 with <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">True</span></code> and the number
of samples with <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">False</span></code>.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.jaccard_score" title="sklearn.metrics.jaccard_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_loss</span></code></a>, <a class="reference internal" href="#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_one_loss</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>In binary and multiclass classification, this function is equal
to the <code class="docutils literal notranslate"><span class="pre">jaccard_score</span></code> function.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">2</span>
</pre></div>
</div>
<p>In the multilabel case with binary label indicators:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.adjusted_mutual_info_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">adjusted_mutual_info_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'arithmetic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.adjusted_mutual_info_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Adjusted Mutual Information between two clusterings.</p>
<p>Adjusted Mutual Information (AMI) is an adjustment of the Mutual
Information (MI) score to account for chance. It accounts for the fact that
the MI is generally higher for two clusterings with a larger number of
clusters, regardless of whether there is actually more information shared.
For two clusterings <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>, the AMI is given as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">AMI</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span> <span class="o">=</span> <span class="p">[</span><span class="n">MI</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span> <span class="o">-</span> <span class="n">E</span><span class="p">(</span><span class="n">MI</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">))]</span> <span class="o">/</span> <span class="p">[</span><span class="n">avg</span><span class="p">(</span><span class="n">H</span><span class="p">(</span><span class="n">U</span><span class="p">),</span> <span class="n">H</span><span class="p">(</span><span class="n">V</span><span class="p">))</span> <span class="o">-</span> <span class="n">E</span><span class="p">(</span><span class="n">MI</span><span class="p">(</span><span class="n">U</span><span class="p">,</span> <span class="n">V</span><span class="p">))]</span>
</pre></div>
</div>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is furthermore symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with
<code class="docutils literal notranslate"><span class="pre">label_pred</span></code> will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.</p>
<p>Be mindful that this function is an order of magnitude slower than other
metrics, such as the Adjusted Rand Index.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>labels_pred</strong> (<em>int array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>average_method</strong> (<em>str</em><em>, </em><em>default='arithmetic'</em>) – <p>How to compute the normalizer in the denominator. Possible options
are ‘min’, ‘geometric’, ‘arithmetic’, and ‘max’.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">average_method</span></code> changed from ‘max’ to
‘arithmetic’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>ami</strong> – The AMI returns a value of 1 when the two partitions are identical
(ie perfectly matched). Random partitions (independent labellings) have
an expected AMI around 0 on average hence can be negative.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float (upperlimited by 1.0)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_rand_score</span></code></a></dt><dd><p>Adjusted Rand Index.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.mutual_info_score" title="sklearn.metrics.mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">mutual_info_score</span></code></a></dt><dd><p>Mutual Information (not adjusted for chance).</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Perfect labelings are both homogeneous and complete, hence have
score 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">adjusted_mutual_info_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the AMI is null:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">0.0</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf">Vinh, Epps, and Bailey, (2010). Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance, JMLR</a></p>
</dd>
<dt class="label" id="id2"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Adjusted_Mutual_Information">Wikipedia entry for the Adjusted Mutual Information</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.adjusted_rand_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">adjusted_rand_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.adjusted_rand_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Rand index adjusted for chance.</p>
<p>The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings.</p>
<p>The raw RI score is then “adjusted for chance” into the ARI score
using the following scheme:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ARI</span> <span class="o">=</span> <span class="p">(</span><span class="n">RI</span> <span class="o">-</span> <span class="n">Expected_RI</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">RI</span><span class="p">)</span> <span class="o">-</span> <span class="n">Expected_RI</span><span class="p">)</span>
</pre></div>
</div>
<p>The adjusted Rand index is thus ensured to have a value close to
0.0 for random labeling independently of the number of clusters and
samples and exactly 1.0 when the clusterings are identical (up to
a permutation).</p>
<p>ARI is a symmetric measure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">==</span> <span class="n">adjusted_rand_score</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – Ground truth class labels to be used as a reference</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Cluster labels to evaluate</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>ARI</strong> – Similarity score between -1.0 and 1.0. Random labelings have an ARI
close to 0.0. 1.0 stands for perfect match.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Perfectly matching labelings have a score of 1 even</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">adjusted_rand_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.57...</span>
</pre></div>
</div>
<p>ARI is symmetric, so labelings that have pure clusters with members
coming from the same classes but unnecessary splits are penalized:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="go">0.57...</span>
</pre></div>
</div>
<p>If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the ARI is very low:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">adjusted_rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">0.0</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="citation">
<dt class="label" id="hubert1985"><span class="brackets">Hubert1985</span></dt>
<dd><p>L. Hubert and P. Arabie, Comparing Partitions,
Journal of Classification 1985
<a class="reference external" href="https://link.springer.com/article/10.1007%2FBF01908075">https://link.springer.com/article/10.1007%2FBF01908075</a></p>
</dd>
<dt class="label" id="steinley2004"><span class="brackets">Steinley2004</span></dt>
<dd><p>D. Steinley, Properties of the Hubert-Arabie
adjusted Rand index, Psychological Methods 2004</p>
</dd>
<dt class="label" id="wk"><span class="brackets">wk</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index</a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a></dt><dd><p>Adjusted Mutual Information.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.auc">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">auc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.auc" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Area Under the Curve (AUC) using the trapezoidal rule.</p>
<p>This is a general function, given points on a curve.  For computing the
area under the ROC-curve, see <a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">roc_auc_score()</span></code></a>.  For an alternative
way to summarize a precision-recall curve, see
<a class="reference internal" href="#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision_score()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>ndarray of shape</em><em> (</em><em>n</em><em>,</em><em>)</em>) – x coordinates. These must be either monotonic increasing or monotonic
decreasing.</p></li>
<li><p><strong>y</strong> (<em>ndarray of shape</em><em>, </em><em>(</em><em>n</em><em>,</em><em>)</em>) – y coordinates.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>auc</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></dt><dd><p>Compute the area under the ROC curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">average_precision_score</span></code></a></dt><dd><p>Compute average precision from prediction scores.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a></dt><dd><p>Compute precision-recall pairs for different probability thresholds.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
<span class="go">0.75</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.average_precision_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">average_precision_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'macro'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.average_precision_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute average precision (AP) from prediction scores.</p>
<p>AP summarizes a precision-recall curve as the weighted mean of precisions
achieved at each threshold, with the increase in recall from the previous
threshold used as the weight:</p>
<div class="math notranslate nohighlight">
\[\text{AP} = \sum_n (R_n - R_{n-1}) P_n\]</div>
<p>where <span class="math notranslate nohighlight">\(P_n\)</span> and <span class="math notranslate nohighlight">\(R_n\)</span> are the precision and recall at the nth
threshold <a href="#id63"><span class="problematic" id="id3">[1]_</span></a>. This implementation is not interpolated and is different
from computing the area under the precision-recall curve with the
trapezoidal rule, which uses linear interpolation and can be too
optimistic.</p>
<p>Note: this implementation is restricted to the binary classification task
or multilabel classification task.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – True binary labels or binary label indicators.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by <span class="xref std std-term">decision_function</span> on some classifiers).</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'</em><em>, </em><em>'macro'}</em><em> or </em><em>None</em><em>,             </em><em>default='macro'</em>) – <p>If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise,
this determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by considering each element of the label
indicator matrix as a label.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average, weighted
by support (the number of true instances for each label).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average.</p>
</dd>
</dl>
<p>Will be ignored when <code class="docutils literal notranslate"><span class="pre">y_true</span></code> is binary.</p>
</p></li>
<li><p><strong>pos_label</strong> (<em>int</em><em> or </em><em>str</em><em>, </em><em>default=1</em>) – The label of the positive class. Only applied to binary <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.
For multilabel-indicator <code class="docutils literal notranslate"><span class="pre">y_true</span></code>, <code class="docutils literal notranslate"><span class="pre">pos_label</span></code> is fixed to 1.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>average_precision</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></dt><dd><p>Compute the area under the ROC curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a></dt><dd><p>Compute precision-recall pairs for different probability thresholds.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>Instead of linearly interpolating between operating points, precisions
are weighted by the change in recall since the last operating point.</p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/w/index.php?title=Information_retrieval&amp;oldid=793358396#Average_precision">Wikipedia entry for the Average precision</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">average_precision_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="go">0.83...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.balanced_accuracy_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">balanced_accuracy_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">adjusted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.balanced_accuracy_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the balanced accuracy.</p>
<p>The balanced accuracy in binary and multiclass classification problems to
deal with imbalanced datasets. It is defined as the average of recall
obtained on each class.</p>
<p>The best value is 1 and the worst value is 0 when <code class="docutils literal notranslate"><span class="pre">adjusted=False</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>adjusted</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When true, the result is adjusted for chance, so that random
performance would score 0, while keeping perfect performance at a score
of 1.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>balanced_accuracy</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.recall_score" title="sklearn.metrics.recall_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">recall_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>Some literature promotes alternative definitions of balanced accuracy. Our
definition is equivalent to <a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a> with class-balanced
sample weights, and shares desirable properties with the binary case.
See the <span class="xref std std-ref">User Guide</span>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets">1</span></dt>
<dd><p>Brodersen, K.H.; Ong, C.S.; Stephan, K.E.; Buhmann, J.M. (2010).
The balanced accuracy and its posterior distribution.
Proceedings of the 20th International Conference on Pattern
Recognition, 3121-24.</p>
</dd>
<dt class="label" id="id6"><span class="brackets">2</span></dt>
<dd><p>John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, (2015).
<a class="reference external" href="https://mitpress.mit.edu/books/fundamentals-machine-learning-predictive-data-analytics">Fundamentals of Machine Learning for Predictive Data Analytics:
Algorithms, Worked Examples, and Case Studies</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">balanced_accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.625</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.brier_score_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">brier_score_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_prob</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.brier_score_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Brier score loss.</p>
<p>The smaller the Brier score loss, the better, hence the naming with “loss”.
The Brier score measures the mean squared difference between the predicted
probability and the actual outcome. The Brier score always
takes on a value between zero and one, since this is the largest
possible difference between a predicted probability (which must be
between zero and one) and the actual outcome (which can take on values
of only 0 and 1). It can be decomposed is the sum of refinement loss and
calibration loss.</p>
<p>The Brier score is appropriate for binary and categorical outcomes that
can be structured as true or false, but is inappropriate for ordinal
variables which can take on three or more values (this is because the
Brier score assumes that all possible outcomes are equivalently
“distant” from one another). Which label is considered to be the positive
label is controlled via the parameter <cite>pos_label</cite>, which defaults to
the greater label unless <cite>y_true</cite> is all 0 or all -1, in which case
<cite>pos_label</cite> defaults to 1.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True targets.</p></li>
<li><p><strong>y_prob</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Probabilities of the positive class.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>pos_label</strong> (<em>int</em><em> or </em><em>str</em><em>, </em><em>default=None</em>) – <p>Label of the positive class. <cite>pos_label</cite> will be infered in the
following manner:</p>
<ul>
<li><p>if <cite>y_true</cite> in {-1, 1} or {0, 1}, <cite>pos_label</cite> defaults to 1;</p></li>
<li><p>else if <cite>y_true</cite> contains string, an error will be raised and
<cite>pos_label</cite> should be explicitely specified;</p></li>
<li><p>otherwise, <cite>pos_label</cite> defaults to the greater label,
i.e. <cite>np.unique(y_true)[-1]</cite>.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – Brier score loss.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">brier_score_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true_categorical</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_prob</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">)</span>
<span class="go">0.037...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">y_prob</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="go">0.037...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true_categorical</span><span class="p">,</span> <span class="n">y_prob</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s2">&quot;ham&quot;</span><span class="p">)</span>
<span class="go">0.037...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">brier_score_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_prob</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Brier_score">Wikipedia entry for the Brier score</a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.calinski_harabasz_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">calinski_harabasz_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.calinski_harabasz_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Calinski and Harabasz score.</p>
<p>It is also known as the Variance Ratio Criterion.</p>
<p>The score is defined as ratio between the within-cluster dispersion and
the between-cluster dispersion.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – A list of <code class="docutils literal notranslate"><span class="pre">n_features</span></code>-dimensional data points. Each row corresponds
to a single data point.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Predicted labels for each sample.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The resulting Calinski-Harabasz score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id8"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://www.tandfonline.com/doi/abs/10.1080/03610927408827101">T. Calinski and J. Harabasz, 1974. “A dendrite method for cluster
analysis”. Communications in Statistics</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.check_scoring">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">check_scoring</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_none</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.check_scoring" title="Permalink to this definition">¶</a></dt>
<dd><p>Determine scorer from user options.</p>
<p>A TypeError will be thrown if the estimator cannot be scored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator object implementing 'fit'</em>) – The object to use to fit the data.</p></li>
<li><p><strong>scoring</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=None</em>) – A string (see model evaluation documentation) or
a scorer callable object / function with signature
<code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</p></li>
<li><p><strong>allow_none</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If no scoring is specified and the estimator has no score function, we
can either return None or raise an exception.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scoring</strong> – A scorer callable object / function with signature
<code class="docutils literal notranslate"><span class="pre">scorer(estimator,</span> <span class="pre">X,</span> <span class="pre">y)</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.classification_report">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">classification_report</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">digits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_dict</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.classification_report" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a text report showing the main classification metrics.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_labels</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Optional list of label indices to include in the report.</p></li>
<li><p><strong>target_names</strong> (<em>list of str of shape</em><em> (</em><em>n_labels</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Optional display names matching the labels (same order).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>digits</strong> (<em>int</em><em>, </em><em>default=2</em>) – Number of digits for formatting output floating point values.
When <code class="docutils literal notranslate"><span class="pre">output_dict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, this will be ignored and the
returned values will not be rounded.</p></li>
<li><p><strong>output_dict</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If True, return output as dict.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division. If set to
“warn”, this acts as 0, but warnings are also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>report</strong> – Text summary of the precision, recall, F1 score for each class.
Dictionary returned if output_dict is True. Dictionary has the
following structure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;label 1&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;precision&#39;</span><span class="p">:</span><span class="mf">0.5</span><span class="p">,</span>
             <span class="s1">&#39;recall&#39;</span><span class="p">:</span><span class="mf">1.0</span><span class="p">,</span>
             <span class="s1">&#39;f1-score&#39;</span><span class="p">:</span><span class="mf">0.67</span><span class="p">,</span>
             <span class="s1">&#39;support&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">},</span>
 <span class="s1">&#39;label 2&#39;</span><span class="p">:</span> <span class="p">{</span> <span class="o">...</span> <span class="p">},</span>
  <span class="o">...</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The reported averages include macro average (averaging the unweighted
mean per label), weighted average (averaging the support-weighted mean
per label), and sample average (only for multilabel classification).
Micro average (averaging the total true positives, false negatives and
false positives) is only shown for multi-label or multi-class
with a subset of classes, because it corresponds to accuracy
otherwise and would be the same for all metrics.
See also <a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-func docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code></a> for more details
on averages.</p>
<p>Note that in binary classification, recall of the positive class
is also known as “sensitivity”; recall of the negative class is
“specificity”.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string / dict</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">target_names</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;class 0&#39;</span><span class="p">,</span> <span class="s1">&#39;class 1&#39;</span><span class="p">,</span> <span class="s1">&#39;class 2&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">target_names</span><span class="p">))</span>
<span class="go">              precision    recall  f1-score   support</span>

<span class="go">     class 0       0.50      1.00      0.67         1</span>
<span class="go">     class 1       0.00      0.00      0.00         1</span>
<span class="go">     class 2       1.00      0.67      0.80         3</span>

<span class="go">    accuracy                           0.60         5</span>
<span class="go">   macro avg       0.50      0.56      0.49         5</span>
<span class="go">weighted avg       0.70      0.60      0.61         5</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">              precision    recall  f1-score   support</span>

<span class="go">           1       1.00      0.67      0.80         3</span>
<span class="go">           2       0.00      0.00      0.00         0</span>
<span class="go">           3       0.00      0.00      0.00         0</span>

<span class="go">   micro avg       1.00      0.67      0.80         3</span>
<span class="go">   macro avg       0.33      0.22      0.27         3</span>
<span class="go">weighted avg       1.00      0.67      0.80         3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.cohen_kappa_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">cohen_kappa_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y2</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.cohen_kappa_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Cohen’s kappa: a statistic that measures inter-annotator agreement.</p>
<p>This function computes Cohen’s kappa <a href="#id64"><span class="problematic" id="id9">[1]_</span></a>, a score that expresses the level
of agreement between two annotators on a classification problem. It is
defined as</p>
<div class="math notranslate nohighlight">
\[\kappa = (p_o - p_e) / (1 - p_e)\]</div>
<p>where <span class="math notranslate nohighlight">\(p_o\)</span> is the empirical probability of agreement on the label
assigned to any sample (the observed agreement ratio), and <span class="math notranslate nohighlight">\(p_e\)</span> is
the expected agreement when both annotators assign labels randomly.
<span class="math notranslate nohighlight">\(p_e\)</span> is estimated using a per-annotator empirical prior over the
class labels <a href="#id65"><span class="problematic" id="id10">[2]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y1</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Labels assigned by the first annotator.</p></li>
<li><p><strong>y2</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Labels assigned by the second annotator. The kappa statistic is
symmetric, so swapping <code class="docutils literal notranslate"><span class="pre">y1</span></code> and <code class="docutils literal notranslate"><span class="pre">y2</span></code> doesn’t change the value.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – List of labels to index the matrix. This may be used to select a
subset of labels. If None, all labels that appear at least once in
<code class="docutils literal notranslate"><span class="pre">y1</span></code> or <code class="docutils literal notranslate"><span class="pre">y2</span></code> are used.</p></li>
<li><p><strong>weights</strong> (<em>{'linear'</em><em>, </em><em>'quadratic'}</em><em>, </em><em>default=None</em>) – Weighting type to calculate the score. None means no weighted;
“linear” means linear weighted; “quadratic” means quadratic weighted.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>kappa</strong> – The kappa statistic, which is a number between -1 and 1. The maximum
value means complete agreement; zero or lower means chance agreement.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets">1</span></dt>
<dd><p>J. Cohen (1960). “A coefficient of agreement for nominal scales”.
Educational and Psychological Measurement 20(1):37-46.
doi:10.1177/001316446002000104.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://www.mitpressjournals.org/doi/pdf/10.1162/coli.07-034-R2">R. Artstein and M. Poesio (2008). “Inter-coder agreement for
computational linguistics”. Computational Linguistics 34(4):555-596</a>.</p>
</dd>
<dt class="label" id="id13"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Cohen%27s_kappa">Wikipedia entry for the Cohen’s kappa</a>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.completeness_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">completeness_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.completeness_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Completeness metric of a cluster labeling given a ground truth.</p>
<p>A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is not symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with <code class="docutils literal notranslate"><span class="pre">label_pred</span></code>
will return the <a class="reference internal" href="#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">homogeneity_score()</span></code></a> which will be different in
general.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – ground truth class labels to be used as a reference</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – cluster labels to evaluate</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>completeness</strong> – score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id14"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://aclweb.org/anthology/D/D07/D07-1043.pdf">Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
conditional entropy-based external cluster evaluation measure</a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">homogeneity_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_measure_score</span></code></a></p>
</div>
<p class="rubric">Examples</p>
<p>Perfect labelings are complete:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">completeness_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">completeness_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Non-perfect labelings that assign all classes members to the same clusters
are still complete:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">completeness_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">completeness_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">0.999...</span>
</pre></div>
</div>
<p>If classes members are split across different clusters, the
assignment cannot be complete:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">completeness_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">completeness_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">0.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute confusion matrix to evaluate the accuracy of a classification.</p>
<p>By definition a confusion matrix <span class="math notranslate nohighlight">\(C\)</span> is such that <span class="math notranslate nohighlight">\(C_{i, j}\)</span>
is equal to the number of observations known to be in group <span class="math notranslate nohighlight">\(i\)</span> and
predicted to be in group <span class="math notranslate nohighlight">\(j\)</span>.</p>
<p>Thus in binary classification, the count of true negatives is
<span class="math notranslate nohighlight">\(C_{0,0}\)</span>, false negatives is <span class="math notranslate nohighlight">\(C_{1,0}\)</span>, true positives is
<span class="math notranslate nohighlight">\(C_{1,1}\)</span> and false positives is <span class="math notranslate nohighlight">\(C_{0,1}\)</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>)</em><em>, </em><em>default=None</em>) – List of labels to index the matrix. This may be used to reorder
or select a subset of labels.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> is given, those that appear at least once
in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> or <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
<li><p><strong>normalize</strong> (<em>{'true'</em><em>, </em><em>'pred'</em><em>, </em><em>'all'}</em><em>, </em><em>default=None</em>) – Normalizes confusion matrix over the true (rows), predicted (columns)
conditions or all the population. If None, confusion matrix will not be
normalized.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>C</strong> – Confusion matrix whose i-th row and j-th
column entry indicates the number of
samples with true label being i-th class
and predicted label being j-th class.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes, n_classes)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="sklearn.metrics.ConfusionMatrixDisplay.from_estimator"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_estimator</span></code></a></dt><dd><p>Plot the confusion matrix given an estimator, the data, and the label.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay.from_predictions</span></code></a></dt><dd><p>Plot the confusion matrix given the true and predicted labels.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></dt><dd><p>Confusion Matrix visualization.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id15"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Confusion_matrix">Wikipedia entry for the Confusion matrix</a>
(Wikipedia and other references may use a different
convention for axes).</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([[2, 0, 0],</span>
<span class="go">       [0, 0, 1],</span>
<span class="go">       [1, 0, 2]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">])</span>
<span class="go">array([[2, 0, 0],</span>
<span class="go">       [0, 0, 1],</span>
<span class="go">       [1, 0, 2]])</span>
</pre></div>
</div>
<p>In the binary case, we can extract true positives, etc as follows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">(</span><span class="n">tn</span><span class="p">,</span> <span class="n">fp</span><span class="p">,</span> <span class="n">fn</span><span class="p">,</span> <span class="n">tp</span><span class="p">)</span>
<span class="go">(0, 2, 1, 1)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.consensus_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">consensus_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">b</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">similarity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'jaccard'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.consensus_score" title="Permalink to this definition">¶</a></dt>
<dd><p>The similarity of two sets of biclusters.</p>
<p>Similarity between individual biclusters is computed. Then the
best matching between sets is found using the Hungarian algorithm.
The final score is the sum of similarities divided by the size of
the larger set.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>(</em><em>rows</em><em>, </em><em>columns</em><em>)</em>) – Tuple of row and column indicators for a set of biclusters.</p></li>
<li><p><strong>b</strong> (<em>(</em><em>rows</em><em>, </em><em>columns</em><em>)</em>) – Another set of biclusters like <code class="docutils literal notranslate"><span class="pre">a</span></code>.</p></li>
<li><p><strong>similarity</strong> (<em>'jaccard'</em><em> or </em><em>callable</em><em>, </em><em>default='jaccard'</em>) – May be the string “jaccard” to use the Jaccard coefficient, or
any function that takes four arguments, each of which is a 1d
indicator vector: (a_rows, a_columns, b_rows, b_columns).</p></li>
</ul>
</dd>
</dl>
<p class="rubric">References</p>
<ul class="simple">
<li><p>Hochreiter, Bodenhofer, et. al., 2010. <a class="reference external" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881408/">FABIA: factor analysis
for bicluster acquisition</a>.</p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.coverage_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">coverage_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.coverage_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Coverage error measure.</p>
<p>Compute how far we need to go through the ranked scores to cover all
true labels. The best value is equal to the average number
of labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> per sample.</p>
<p>Ties in <code class="docutils literal notranslate"><span class="pre">y_scores</span></code> are broken by giving maximal rank that would have
been assigned to all tied values.</p>
<p>Note: Our implementation’s score is 1 greater than the one given in
Tsoumakas et al., 2010. This extends it to handle the degenerate case
in which an instance has 0 true labels.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – True binary labels in binary indicator format.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by “decision_function” on some classifiers).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>coverage_error</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id16"><span class="brackets">1</span></dt>
<dd><p>Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010).
Mining multi-label data. In Data mining and knowledge discovery
handbook (pp. 667-685). Springer US.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.davies_bouldin_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">davies_bouldin_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.davies_bouldin_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the Davies-Bouldin score.</p>
<p>The score is defined as the average similarity measure of each cluster with
its most similar cluster, where similarity is the ratio of within-cluster
distances to between-cluster distances. Thus, clusters which are farther
apart and less dispersed will result in a better score.</p>
<p>The minimum score is zero, with lower values indicating better clustering.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – A list of <code class="docutils literal notranslate"><span class="pre">n_features</span></code>-dimensional data points. Each row corresponds
to a single data point.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Predicted labels for each sample.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The resulting Davies-Bouldin score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id17"><span class="brackets">1</span></dt>
<dd><p>Davies, David L.; Bouldin, Donald W. (1979).
<a class="reference external" href="https://ieeexplore.ieee.org/document/4766909">“A Cluster Separation Measure”</a>.
IEEE Transactions on Pattern Analysis and Machine Intelligence.
PAMI-1 (2): 224-227</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.dcg_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">dcg_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">log_base</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_ties</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.dcg_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Discounted Cumulative Gain.</p>
<p>Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount.</p>
<p>This ranking metric yields a high value if true labels are ranked high by
<code class="docutils literal notranslate"><span class="pre">y_score</span></code>.</p>
<p>Usually the Normalized Discounted Cumulative Gain (NDCG, computed by
ndcg_score) is preferred.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – True targets of multilabel classification, or true scores of entities
to be ranked.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – Target scores, can either be probability estimates, confidence values,
or non-thresholded measure of decisions (as returned by
“decision_function” on some classifiers).</p></li>
<li><p><strong>k</strong> (<em>int</em><em>, </em><em>default=None</em>) – Only consider the highest k scores in the ranking. If None, use all
outputs.</p></li>
<li><p><strong>log_base</strong> (<em>float</em><em>, </em><em>default=2</em>) – Base of the logarithm used for the discount. A low value means a
sharper discount (top results are more important).</p></li>
<li><p><strong>sample_weight</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, all samples are given the same weight.</p></li>
<li><p><strong>ignore_ties</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Assume that there are no ties in y_score (which is likely to be the
case if y_score is continuous) for efficiency gains.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>discounted_cumulative_gain</strong> – The averaged sample DCG scores.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.ndcg_score" title="sklearn.metrics.ndcg_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ndcg_score</span></code></a></dt><dd><p>The Discounted Cumulative Gain divided by the Ideal Discounted Cumulative Gain (the DCG obtained for a perfect ranking), in order to have a score between 0 and 1.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">Wikipedia entry for Discounted Cumulative Gain</a>.</p>
<p>Jarvelin, K., &amp; Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.</p>
<p>Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp; Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013).</p>
<p>McSherry, F., &amp; Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">dcg_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we have groud-truth relevance of some answers to a query:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">true_relevance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we predict scores for the answers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">70</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="go">9.49...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we can set k to truncate the sum; only top k answers contribute</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">5.63...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># now we have some ties in our prediction</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># by default ties are averaged, so here we get the average true</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># relevance of our top predictions: (10 + 5) / 2 = 7.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">7.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we can choose to ignore ties for faster results, but only</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># if we know there aren&#39;t ties in our scores, otherwise we get</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># wrong results:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span>
<span class="gp">... </span>          <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ignore_ties</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">5.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.det_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">det_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.det_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute error rates for different probability thresholds.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This metric is used for evaluation of ranking and error tradeoffs of
a binary classification task.</p>
</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True binary labels. If labels are not either {-1, 1} or {0, 1}, then
pos_label should be explicitly given.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape of</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by “decision_function” on some classifiers).</p></li>
<li><p><strong>pos_label</strong> (<em>int</em><em> or </em><em>str</em><em>, </em><em>default=None</em>) – The label of the positive class.
When <code class="docutils literal notranslate"><span class="pre">pos_label=None</span></code>, if <cite>y_true</cite> is in {-1, 1} or {0, 1},
<code class="docutils literal notranslate"><span class="pre">pos_label</span></code> is set to 1, otherwise an error will be raised.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>fpr</strong> (<em>ndarray of shape (n_thresholds,)</em>) – False positive rate (FPR) such that element i is the false positive
rate of predictions with score &gt;= thresholds[i]. This is occasionally
referred to as false acceptance propability or fall-out.</p></li>
<li><p><strong>fnr</strong> (<em>ndarray of shape (n_thresholds,)</em>) – False negative rate (FNR) such that element i is the false negative
rate of predictions with score &gt;= thresholds[i]. This is occasionally
referred to as false rejection or miss rate.</p></li>
<li><p><strong>thresholds</strong> (<em>ndarray of shape (n_thresholds,)</em>) – Decreasing score values.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.plot_det_curve" title="sklearn.metrics.plot_det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_det_curve</span></code></a></dt><dd><p>Plot detection error tradeoff (DET) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.DetCurveDisplay" title="sklearn.metrics.DetCurveDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DetCurveDisplay</span></code></a></dt><dd><p>DET curve visualization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a></dt><dd><p>Compute Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a></dt><dd><p>Compute precision-recall curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">det_curve</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">fnr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">det_curve</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span>
<span class="go">array([0.5, 0.5, 0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fnr</span>
<span class="go">array([0. , 0.5, 0.5])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">thresholds</span>
<span class="go">array([0.35, 0.4 , 0.8 ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.euclidean_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">euclidean_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y_norm_squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_norm_squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Considering the rows of X (and Y=X) as vectors, compute the
distance matrix between each pair of vectors.</p>
<p>For efficiency reasons, the euclidean distance between a pair of row
vector x and y is computed as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">dist</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>This formulation has two advantages over other ways of computing distances.
First, it is computationally efficient when dealing with sparse data.
Second, if one argument varies but the other remains unchanged, then
<cite>dot(x, x)</cite> and/or <cite>dot(y, y)</cite> can be pre-computed.</p>
<p>However, this is not the most precise way of doing this computation,
because this equation potentially suffers from “catastrophic cancellation”.
Also, the distance matrix returned by this function may not be exactly
symmetric as required by, e.g., <code class="docutils literal notranslate"><span class="pre">scipy.spatial.distance</span></code> functions.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>,             </em><em>default=None</em>) – </p></li>
<li><p><strong>Y_norm_squared</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>,</em><em>) or </em><em>(</em><em>n_samples_Y</em><em>, </em><em>1</em><em>)             or </em><em>(</em><em>1</em><em>, </em><em>n_samples_Y</em><em>)</em><em>, </em><em>default=None</em>) – Pre-computed dot-products of vectors in Y (e.g.,
<code class="docutils literal notranslate"><span class="pre">(Y**2).sum(axis=1)</span></code>)
May be ignored in some cases, see the note below.</p></li>
<li><p><strong>squared</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Return squared Euclidean distances.</p></li>
<li><p><strong>X_norm_squared</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>,</em><em>) or </em><em>(</em><em>n_samples_X</em><em>, </em><em>1</em><em>)             or </em><em>(</em><em>1</em><em>, </em><em>n_samples_X</em><em>)</em><em>, </em><em>default=None</em>) – Pre-computed dot-products of vectors in X (e.g.,
<code class="docutils literal notranslate"><span class="pre">(X**2).sum(axis=1)</span></code>)
May be ignored in some cases, see the note below.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>To achieve better accuracy, <cite>X_norm_squared</cite> and <cite>Y_norm_squared</cite> may be
unused if they are passed as <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>distances</strong></p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></dt><dd><p>Distances betweens pairs of elements of X and Y.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">euclidean_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># distance between rows of X</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="go">array([[0., 1.],</span>
<span class="go">       [1., 0.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get distance to origin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[1.        ],</span>
<span class="go">       [1.41421356]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.explained_variance_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">explained_variance_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.explained_variance_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Explained variance regression score function.</p>
<p>Best possible score is 1.0, lower values are worse.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'</em><em>, </em><em>'variance_weighted'}</em><em> or             </em><em>array-like of shape</em><em> (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output scores.
Array-like value defines weights used to average scores.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of scores in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Scores of all outputs are averaged with uniform weight.</p>
</dd>
<dt>’variance_weighted’ :</dt><dd><p>Scores of all outputs are averaged, weighted by the variances
of each individual output.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The explained variance or ndarray if ‘multioutput’ is ‘raw_values’.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is not a symmetric function.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">explained_variance_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.957...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">explained_variance_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;uniform_average&#39;</span><span class="p">)</span>
<span class="go">0.983...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.f1_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">f1_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.f1_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure.</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <cite>labels</cite> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,</em><em>'weighted'</em><em>, </em><em>'binary'}</em><em> or </em><em>None</em><em>,             </em><em>default='binary'</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division, i.e. when all
predictions and labels are negative. If set to “warn”, this acts as 0,
but warnings are also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.fbeta_score" title="sklearn.metrics.fbeta_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fbeta_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="#sklearn.metrics.jaccard_score" title="sklearn.metrics.jaccard_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id18"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1.0...</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code>, precision is undefined.
When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, recall is undefined.
In such cases, by default the metric will be set to 0, as will f-score,
and <code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code> will be raised. This behavior can be
modified with <code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.fbeta_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">fbeta_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.fbeta_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F-beta score.</p>
<p>The F-beta score is the weighted harmonic mean of precision and recall,
reaching its optimal value at 1 and its worst value at 0.</p>
<p>The <cite>beta</cite> parameter determines the weight of recall in the combined
score. <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> lends more weight to precision, while <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>
favors recall (<code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">-&gt;</span> <span class="pre">0</span></code> considers only precision, <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">-&gt;</span> <span class="pre">+inf</span></code>
only recall).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>beta</strong> (<em>float</em>) – Determines the weight of recall in the combined score.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <cite>labels</cite> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'</em><em>, </em><em>'binary'}</em><em> or </em><em>None             default='binary'</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division, i.e. when all
predictions and labels are negative. If set to “warn”, this acts as 0,
but warnings are also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>fbeta_score</strong> – F-beta score of the positive class in binary classification or weighted
average of the F-beta score of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float (if average is not None) or array of float, shape =        [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>. This behavior can be
modified with <code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id19"><span class="brackets">1</span></dt>
<dd><p>R. Baeza-Yates and B. Ribeiro-Neto (2011).
Modern Information Retrieval. Addison Wesley, pp. 327-328.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">fbeta_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.23...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">0.23...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fbeta_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="go">array([0.71..., 0.        , 0.        ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.fowlkes_mallows_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">fowlkes_mallows_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.fowlkes_mallows_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Measure the similarity of two clusterings of a set of points.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<p>The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of
the precision and recall:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FMI</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="n">sqrt</span><span class="p">((</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">))</span>
</pre></div>
</div>
<p>Where <code class="docutils literal notranslate"><span class="pre">TP</span></code> is the number of <strong>True Positive</strong> (i.e. the number of pair of
points that belongs in the same clusters in both <code class="docutils literal notranslate"><span class="pre">labels_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">labels_pred</span></code>), <code class="docutils literal notranslate"><span class="pre">FP</span></code> is the number of <strong>False Positive</strong> (i.e. the
number of pair of points that belongs in the same clusters in
<code class="docutils literal notranslate"><span class="pre">labels_true</span></code> and not in <code class="docutils literal notranslate"><span class="pre">labels_pred</span></code>) and <code class="docutils literal notranslate"><span class="pre">FN</span></code> is the number of
<strong>False Negative</strong> (i.e the number of pair of points that belongs in the
same clusters in <code class="docutils literal notranslate"><span class="pre">labels_pred</span></code> and not in <code class="docutils literal notranslate"><span class="pre">labels_True</span></code>).</p>
<p>The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (int array, shape = (<code class="docutils literal notranslate"><span class="pre">n_samples</span></code>,)) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>labels_pred</strong> (array, shape = (<code class="docutils literal notranslate"><span class="pre">n_samples</span></code>, )) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>sparse</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Compute contingency matrix internally with sparse matrix.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The resulting Fowlkes-Mallows score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Perfect labelings are both homogeneous and complete, hence have
score 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">fowlkes_mallows_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fowlkes_mallows_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fowlkes_mallows_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>If classes members are completely split across different clusters,
the assignment is totally random, hence the FMI is null:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">fowlkes_mallows_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="go">0.0</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id22"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf">E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical
Association</a></p>
</dd>
<dt class="label" id="id23"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Fowlkes-Mallows_index">Wikipedia entry for the Fowlkes-Mallows Index</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.get_scorer">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">get_scorer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scoring</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.get_scorer" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a scorer from string.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>scoring</strong> (<em>str</em><em> or </em><em>callable</em>) – Scoring method as string. If callable it is returned as is.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scorer</strong> – The scorer.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.hamming_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">hamming_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.hamming_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the average Hamming loss.</p>
<p>The Hamming loss is the fraction of labels that are incorrectly predicted.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) labels.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Predicted labels, as returned by a classifier.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – Return the average Hamming loss between element of <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or int</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.jaccard_score" title="sklearn.metrics.jaccard_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.zero_one_loss" title="sklearn.metrics.zero_one_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">zero_one_loss</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>In multiclass classification, the Hamming loss corresponds to the Hamming
distance between <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> which is equivalent to the
subset <code class="docutils literal notranslate"><span class="pre">zero_one_loss</span></code> function, when <cite>normalize</cite> parameter is set to
True.</p>
<p>In multilabel classification, the Hamming loss is different from the
subset zero-one loss. The zero-one loss considers the entire set of labels
for a given sample incorrect if it does not entirely match the true set of
labels. Hamming loss is more forgiving in that it penalizes only the
individual labels.</p>
<p>The Hamming loss is upperbounded by the subset zero-one loss, when
<cite>normalize</cite> parameter is set to True. It is always between 0 and 1,
lower being better.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id24"><span class="brackets">1</span></dt>
<dd><p>Grigorios Tsoumakas, Ioannis Katakis. Multi-Label Classification:
An Overview. International Journal of Data Warehousing &amp; Mining,
3(3), 1-13, July-September 2007.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Hamming_distance">Wikipedia entry on the Hamming distance</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">hamming_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.25</span>
</pre></div>
</div>
<p>In the multilabel case with binary label indicators:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hamming_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.75</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.hinge_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">hinge_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pred_decision</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.hinge_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Average hinge loss (non-regularized).</p>
<p>In binary class case, assuming labels in y_true are encoded with +1 and -1,
when a prediction mistake is made, <code class="docutils literal notranslate"><span class="pre">margin</span> <span class="pre">=</span> <span class="pre">y_true</span> <span class="pre">*</span> <span class="pre">pred_decision</span></code> is
always negative (since the signs disagree), implying <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">margin</span></code> is
always greater than 1.  The cumulated hinge loss is therefore an upper
bound of the number of mistakes made by the classifier.</p>
<p>In multiclass case, the function expects that either all the labels are
included in y_true or an optional labels argument is provided which
contains all the labels. The multilabel margin is calculated according
to Crammer-Singer’s method. As in the binary case, the cumulated hinge loss
is an upper bound of the number of mistakes made by the classifier.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True target, consisting of integers of two values. The positive label
must be greater than the negative label.</p></li>
<li><p><strong>pred_decision</strong> (<em>array of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – Predicted decisions, as output by decision_function (floats).</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – Contains all the labels for the problem. Used in multiclass hinge loss.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id26"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Hinge_loss">Wikipedia entry on the Hinge loss</a>.</p>
</dd>
<dt class="label" id="id27"><span class="brackets">2</span></dt>
<dd><p>Koby Crammer, Yoram Singer. On the Algorithmic
Implementation of Multiclass Kernel-based Vector
Machines. Journal of Machine Learning Research 2,
(2001), 265-292.</p>
</dd>
<dt class="label" id="id28"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="http://www.ttic.edu/sigml/symposium2011/papers/Moore+DeNero_Regularization.pdf">L1 AND L2 Regularization for Multiclass Hinge Loss Models
by Robert C. Moore, John DeNero</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">hinge_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">LinearSVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span>
<span class="go">array([-2.18...,  2.36...,  0.09...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hinge_loss</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">pred_decision</span><span class="p">)</span>
<span class="go">0.30...</span>
</pre></div>
</div>
<p>In the multiclass case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">LinearSVC()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pred_decision</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">hinge_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">pred_decision</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
<span class="go">0.56...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.homogeneity_completeness_v_measure">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">homogeneity_completeness_v_measure</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.homogeneity_completeness_v_measure" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the homogeneity and completeness and V-Measure scores at once.</p>
<p>Those metrics are based on normalized conditional entropy measures of
the clustering labeling to evaluate given the knowledge of a Ground
Truth class labels of the same samples.</p>
<p>A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.</p>
<p>A clustering result satisfies completeness if all the data points
that are members of a given class are elements of the same cluster.</p>
<p>Both scores have positive values between 0.0 and 1.0, larger values
being desirable.</p>
<p>Those 3 metrics are independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score values in any way.</p>
<p>V-Measure is furthermore symmetric: swapping <code class="docutils literal notranslate"><span class="pre">labels_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">label_pred</span></code> will give the same score. This does not hold for
homogeneity and completeness. V-Measure is identical to
<a class="reference internal" href="#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">normalized_mutual_info_score()</span></code></a> with the arithmetic averaging
method.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – ground truth class labels to be used as a reference</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – cluster labels to evaluate</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Ratio of weight attributed to <code class="docutils literal notranslate"><span class="pre">homogeneity</span></code> vs <code class="docutils literal notranslate"><span class="pre">completeness</span></code>.
If <code class="docutils literal notranslate"><span class="pre">beta</span></code> is greater than 1, <code class="docutils literal notranslate"><span class="pre">completeness</span></code> is weighted more
strongly in the calculation. If <code class="docutils literal notranslate"><span class="pre">beta</span></code> is less than 1,
<code class="docutils literal notranslate"><span class="pre">homogeneity</span></code> is weighted more strongly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>homogeneity</strong> (<em>float</em>) – score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling</p></li>
<li><p><strong>completeness</strong> (<em>float</em>) – score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling</p></li>
<li><p><strong>v_measure</strong> (<em>float</em>) – harmonic mean of the first two</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">homogeneity_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">completeness_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_measure_score</span></code></a></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.homogeneity_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">homogeneity_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.homogeneity_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Homogeneity metric of a cluster labeling given a ground truth.</p>
<p>A clustering result satisfies homogeneity if all of its clusters
contain only data points which are members of a single class.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is not symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with <code class="docutils literal notranslate"><span class="pre">label_pred</span></code>
will return the <a class="reference internal" href="#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">completeness_score()</span></code></a> which will be different in
general.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – ground truth class labels to be used as a reference</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – cluster labels to evaluate</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>homogeneity</strong> – score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id29"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://aclweb.org/anthology/D/D07/D07-1043.pdf">Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
conditional entropy-based external cluster evaluation measure</a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">completeness_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_measure_score</span></code></a></p>
</div>
<p class="rubric">Examples</p>
<p>Perfect labelings are homogeneous:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">homogeneity_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">homogeneity_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Non-perfect labelings that further split classes into more clusters can be
perfectly homogeneous:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">homogeneity_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">1.000000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">homogeneity_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">1.000000</span>
</pre></div>
</div>
<p>Clusters that include samples from different classes do not make for an
homogeneous labeling:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">homogeneity_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">0.0...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">homogeneity_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="go">0.0...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.jaccard_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">jaccard_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.jaccard_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Jaccard similarity coefficient score.</p>
<p>The Jaccard index [1], or Jaccard similarity coefficient, defined as
the size of the intersection divided by the size of the union of two label
sets, is used to compare set of predicted labels for a sample to the
corresponding set of labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) labels.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Predicted labels, as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{None</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'</em><em>,             </em><em>'binary'}</em><em>, </em><em>default='binary'</em>) – <p>If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average, weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>{0.0</em><em>, </em><em>1.0}</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division, i.e. when there
there are no negative values in predictions and labels. If set to
“warn”, this acts like 0, but a warning is also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float (if average is not None) or array of floats, shape =            [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_score</span></code></a>, <code class="xref py py-obj docutils literal notranslate"><span class="pre">f_score</span></code>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p><a class="reference internal" href="#sklearn.metrics.jaccard_score" title="sklearn.metrics.jaccard_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">jaccard_score()</span></code></a> may be a poor metric if there are no
positives for some samples or classes. Jaccard is undefined if there are
no true or predicted labels, and our implementation will return a score
of 0 with a warning.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id31"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Jaccard_index">Wikipedia entry for the Jaccard index</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">jaccard_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
</pre></div>
</div>
<p>In the binary case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="go">0.6666...</span>
</pre></div>
</div>
<p>In the multilabel case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;samples&#39;</span><span class="p">)</span>
<span class="go">0.5833...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">0.6666...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.5, 0.5, 1. ])</span>
</pre></div>
</div>
<p>In the multiclass case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">jaccard_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([1. , 0. , 0.33...])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.label_ranking_average_precision_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">label_ranking_average_precision_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.label_ranking_average_precision_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute ranking-based average precision.</p>
<p>Label ranking average precision (LRAP) is the average over each ground
truth label assigned to each sample, of the ratio of true vs. total
labels with lower score.</p>
<p>This metric is used in multilabel ranking problem, where the goal
is to give better rank to the labels associated to each sample.</p>
<p>The obtained score is always strictly greater than 0 and
the best value is 1.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – True binary labels in binary indicator format.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by “decision_function” on some classifiers).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">label_ranking_average_precision_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.75</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">label_ranking_average_precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span>
<span class="go">0.416...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.label_ranking_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">label_ranking_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.label_ranking_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Ranking loss measure.</p>
<p>Compute the average number of label pairs that are incorrectly ordered
given y_score weighted by the size of the label set and the number of
labels not in the label set.</p>
<p>This is similar to the error set size, but weighted by the number of
relevant and irrelevant labels. The best performance is achieved with
a ranking loss of zero.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>A function <em>label_ranking_loss</em></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – True binary labels in binary indicator format.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by “decision_function” on some classifiers).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id32"><span class="brackets">1</span></dt>
<dd><p>Tsoumakas, G., Katakis, I., &amp; Vlahavas, I. (2010).
Mining multi-label data. In Data mining and knowledge discovery
handbook (pp. 667-685). Springer US.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.log_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">log_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">eps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-15</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.log_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Log loss, aka logistic loss or cross-entropy loss.</p>
<p>This is the loss function used in (multinomial) logistic regression
and extensions of it such as neural networks, defined as the negative
log-likelihood of a logistic model that returns <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> probabilities
for its training data <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.
The log loss is only defined for two or more labels.
For a single sample with true label <span class="math notranslate nohighlight">\(y \in \{0,1\}\)</span> and
and a probability estimate <span class="math notranslate nohighlight">\(p = \operatorname{Pr}(y = 1)\)</span>, the log
loss is:</p>
<div class="math notranslate nohighlight">
\[L_{\log}(y, p) = -(y \log (p) + (1 - y) \log (1 - p))\]</div>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like</em><em> or </em><em>label indicator matrix</em>) – Ground truth (correct) labels for n_samples samples.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of float</em><em>, </em><em>shape =</em><em> (</em><em>n_samples</em><em>, </em><em>n_classes</em><em>) or </em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – Predicted probabilities, as returned by a classifier’s
predict_proba method. If <code class="docutils literal notranslate"><span class="pre">y_pred.shape</span> <span class="pre">=</span> <span class="pre">(n_samples,)</span></code>
the probabilities provided are assumed to be that of the
positive class. The labels in <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are assumed to be
ordered alphabetically, as done by
<code class="xref py py-class docutils literal notranslate"><span class="pre">preprocessing.LabelBinarizer</span></code>.</p></li>
<li><p><strong>eps</strong> (<em>float</em><em>, </em><em>default=1e-15</em>) – Log loss is undefined for p=0 or p=1, so probabilities are
clipped to max(eps, min(1 - eps, p)).</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If true, return the mean loss per sample.
Otherwise, return the sum of the per-sample losses.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – <p>If not provided, labels will be inferred from y_true. If <code class="docutils literal notranslate"><span class="pre">labels</span></code>
is <code class="docutils literal notranslate"><span class="pre">None</span></code> and <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> has shape (n_samples,) the labels are
assumed to be binary and are inferred from <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The logarithm used is the natural logarithm (base-e).</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">log_loss</span><span class="p">([</span><span class="s2">&quot;spam&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;ham&quot;</span><span class="p">,</span> <span class="s2">&quot;spam&quot;</span><span class="p">],</span>
<span class="gp">... </span>         <span class="p">[[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.9</span><span class="p">],</span> <span class="p">[</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">.8</span><span class="p">,</span> <span class="mf">.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">.35</span><span class="p">,</span> <span class="mf">.65</span><span class="p">]])</span>
<span class="go">0.21616...</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p>C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,
p. 209.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.make_scorer">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">make_scorer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">score_func</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">greater_is_better</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">needs_proba</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">needs_threshold</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.make_scorer" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a scorer from a performance metric or loss function.</p>
<p>This factory function wraps scoring functions for use in
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code></a> and
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">cross_val_score()</span></code></a>.
It takes a score function, such as <a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>,
<a class="reference internal" href="#sklearn.metrics.mean_squared_error" title="sklearn.metrics.mean_squared_error"><code class="xref py py-func docutils literal notranslate"><span class="pre">mean_squared_error()</span></code></a>,
<code class="xref py py-func docutils literal notranslate"><span class="pre">adjusted_rand_index()</span></code> or
<code class="xref py py-func docutils literal notranslate"><span class="pre">average_precision()</span></code>
and returns a callable that scores an estimator’s output.
The signature of the call is <cite>(estimator, X, y)</cite> where <cite>estimator</cite>
is the model to be evaluated, <cite>X</cite> is the data and <cite>y</cite> is the
ground truth labeling (or <cite>None</cite> in the case of unsupervised models).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>score_func</strong> (<em>callable</em>) – Score function (or loss function) with signature
<code class="docutils literal notranslate"><span class="pre">score_func(y,</span> <span class="pre">y_pred,</span> <span class="pre">**kwargs)</span></code>.</p></li>
<li><p><strong>greater_is_better</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether score_func is a score function (default), meaning high is good,
or a loss function, meaning low is good. In the latter case, the
scorer object will sign-flip the outcome of the score_func.</p></li>
<li><p><strong>needs_proba</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether score_func requires predict_proba to get probability estimates
out of a classifier.</p>
<p>If True, for binary <cite>y_true</cite>, the score function is supposed to accept
a 1D <cite>y_pred</cite> (i.e., probability of the positive class, shape
<cite>(n_samples,)</cite>).</p>
</p></li>
<li><p><strong>needs_threshold</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>Whether score_func takes a continuous decision certainty.
This only works for binary classification using estimators that
have either a decision_function or predict_proba method.</p>
<p>If True, for binary <cite>y_true</cite>, the score function is supposed to accept
a 1D <cite>y_pred</cite> (i.e., probability of the positive class or the decision
function, shape <cite>(n_samples,)</cite>).</p>
<p>For example <code class="docutils literal notranslate"><span class="pre">average_precision</span></code> or the area under the roc curve
can not be computed using discrete predictions alone.</p>
</p></li>
<li><p><strong>**kwargs</strong> (<em>additional arguments</em>) – Additional parameters to be passed to score_func.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scorer</strong> – Callable object that returns a scalar score; greater is better.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>callable</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">fbeta_score</span><span class="p">,</span> <span class="n">make_scorer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ftwo_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">fbeta_score</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ftwo_scorer</span>
<span class="go">make_scorer(fbeta_score, beta=2)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LinearSVC</span><span class="p">(),</span> <span class="n">param_grid</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]},</span>
<span class="gp">... </span>                    <span class="n">scoring</span><span class="o">=</span><span class="n">ftwo_scorer</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>If <cite>needs_proba=False</cite> and <cite>needs_threshold=False</cite>, the score
function is supposed to accept the output of <span class="xref std std-term">predict</span>. If
<cite>needs_proba=True</cite>, the score function is supposed to accept the
output of <span class="xref std std-term">predict_proba</span> (For binary <cite>y_true</cite>, the score function is
supposed to accept probability of the positive class). If
<cite>needs_threshold=True</cite>, the score function is supposed to accept the
output of <span class="xref std std-term">decision_function</span>.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.matthews_corrcoef">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">matthews_corrcoef</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.matthews_corrcoef" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Matthews correlation coefficient (MCC).</p>
<p>The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary and multiclass classifications. It takes
into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are of
very different sizes. The MCC is in essence a correlation coefficient value
between -1 and +1. A coefficient of +1 represents a perfect prediction, 0
an average random prediction and -1 an inverse prediction.  The statistic
is also known as the phi coefficient. [source: Wikipedia]</p>
<p>Binary and multiclass labels are supported.  Only in the binary case does
this relate to information about true and false positives and negatives.
See references below.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>mcc</strong> – The Matthews correlation coefficient (+1 represents a perfect
prediction, 0 an average random prediction and -1 and inverse
prediction).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id33"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://doi.org/10.1093/bioinformatics/16.5.412">Baldi, Brunak, Chauvin, Andersen and Nielsen, (2000). Assessing the
accuracy of prediction algorithms for classification: an overview</a>.</p>
</dd>
<dt class="label" id="id34"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Matthews_correlation_coefficient">Wikipedia entry for the Matthews Correlation Coefficient</a>.</p>
</dd>
<dt class="label" id="id35"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S1476927104000799">Gorodkin, (2004). Comparing two K-category assignments by a
K-category correlation coefficient</a>.</p>
</dd>
<dt class="label" id="id36"><span class="brackets">4</span></dt>
<dd><p><a class="reference external" href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0041882">Jurman, Riccadonna, Furlanello, (2012). A Comparison of MCC and CEN
Error Measures in MultiClass Prediction</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">matthews_corrcoef</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">matthews_corrcoef</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">-0.33...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.max_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">max_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.max_error" title="Permalink to this definition">¶</a></dt>
<dd><p>max_error metric calculates the maximum residual error.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated target values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>max_error</strong> – A positive floating point value (the best value is 0.0).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">max_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_absolute_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_absolute_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_absolute_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean absolute error regression loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em>  or </em><em>array-like of shape</em><em>             (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>loss</strong> – If multioutput is ‘raw_values’, then mean absolute error is returned
for each output separately.
If multioutput is ‘uniform_average’ or an ndarray of weights, then the
weighted average of all output errors is returned.</p>
<p>MAE output is non-negative floating point. The best value is 0.0.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="go">array([0.5, 1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="go">0.85...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_absolute_percentage_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_absolute_percentage_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_absolute_percentage_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean absolute percentage error regression loss.</p>
<p>Note here that we do not represent the output as a percentage in range
[0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the
<span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em> or </em><em>array-like</em>) – <p>Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.
If input is list then the shape must be (n_outputs,).</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>loss</strong> – If multioutput is ‘raw_values’, then mean absolute percentage error
is returned for each output separately.
If multioutput is ‘uniform_average’ or an ndarray of weights, then the
weighted average of all output errors is returned.</p>
<p>MAPE output is non-negative floating point. The best value is 0.0.
But note the fact that bad predictions can lead to arbitarily large
MAPE values, especially if some y_true values are very close to zero.
Note that we return a large value instead of <cite>inf</cite> when y_true is zero.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats in the range [0, 1/eps]</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_absolute_percentage_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.3273...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5515...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_absolute_percentage_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="go">0.6198...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_gamma_deviance">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_gamma_deviance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_gamma_deviance" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Gamma deviance regression loss.</p>
<p>Gamma deviance is equivalent to the Tweedie deviance with
the power parameter <cite>power=2</cite>. It is invariant to scaling of
the target variable, and measures relative errors.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values. Requires y_true &gt; 0.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated target values. Requires y_pred &gt; 0.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A non-negative floating point value (the best value is 0.0).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_gamma_deviance</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_gamma_deviance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0568...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_pinball_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_pinball_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_pinball_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Pinball loss for quantile regression.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>alpha</strong> (<em>double</em><em>, </em><em>slope of the pinball loss</em><em>, </em><em>default=0.5</em><em>,</em>) – this loss is equivalent to <span class="xref std std-ref">mean_absolute_error</span> when <cite>alpha=0.5</cite>,
<cite>alpha=0.95</cite> is minimized by estimators of the 95th percentile.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em>  or </em><em>array-like of shape</em><em>             (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>loss</strong> – If multioutput is ‘raw_values’, then mean absolute error is returned
for each output separately.
If multioutput is ‘uniform_average’ or an ndarray of weights, then the
weighted average of all output errors is returned.</p>
<p>The pinball loss output is a non-negative floating point. The best
value is 0.0.</p>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_pinball_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="go">0.03...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="go">0.3...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="go">0.3...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="go">0.03...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_pinball_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="go">0.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_poisson_deviance">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_poisson_deviance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_poisson_deviance" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Poisson deviance regression loss.</p>
<p>Poisson deviance is equivalent to the Tweedie deviance with
the power parameter <cite>power=1</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values. Requires y_true &gt;= 0.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated target values. Requires y_pred &gt; 0.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A non-negative floating point value (the best value is 0.0).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_poisson_deviance</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_poisson_deviance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.4260...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_squared_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_squared_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_squared_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean squared error regression loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em> or </em><em>array-like of shape</em><em>             (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
<li><p><strong>squared</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If True returns MSE value, if False returns RMSE value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A non-negative floating point value (the best value is 0.0), or an
array of floating point values, one for each individual target.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.375</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">0.612...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.708...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">0.822...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="go">array([0.41666667, 1.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="go">0.825...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_squared_log_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_squared_log_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_squared_log_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean squared logarithmic error regression loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em> or </em><em>array-like of shape</em><em>             (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output values.
Array-like value defines weights used to average errors.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors when the input is of multioutput
format.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A non-negative floating point value (the best value is 0.0), or an
array of floating point values, one for each individual target.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_log_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.039...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.044...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="go">array([0.00462428, 0.08377444])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_log_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="go">0.060...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mean_tweedie_deviance">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mean_tweedie_deviance</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mean_tweedie_deviance" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Tweedie deviance regression loss.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>power</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>Tweedie power parameter. Either power &lt;= 0 or power &gt;= 1.</p>
<p>The higher <cite>p</cite> the less weight is given to extreme
deviations between true and predicted targets.</p>
<ul>
<li><p>power &lt; 0: Extreme stable distribution. Requires: y_pred &gt; 0.</p></li>
<li><p>power = 0 : Normal distribution, output corresponds to
mean_squared_error. y_true and y_pred can be any real numbers.</p></li>
<li><p>power = 1 : Poisson distribution. Requires: y_true &gt;= 0 and
y_pred &gt; 0.</p></li>
<li><p>1 &lt; p &lt; 2 : Compound Poisson distribution. Requires: y_true &gt;= 0
and y_pred &gt; 0.</p></li>
<li><p>power = 2 : Gamma distribution. Requires: y_true &gt; 0 and y_pred &gt; 0.</p></li>
<li><p>power = 3 : Inverse Gaussian distribution. Requires: y_true &gt; 0
and y_pred &gt; 0.</p></li>
<li><p>otherwise : Positive stable distribution. Requires: y_true &gt; 0
and y_pred &gt; 0.</p></li>
</ul>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – A non-negative floating point value (the best value is 0.0).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_tweedie_deviance</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_tweedie_deviance</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">power</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">1.4260...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.median_absolute_error">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">median_absolute_error</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.median_absolute_error" title="Permalink to this definition">¶</a></dt>
<dd><p>Median absolute error regression loss.</p>
<p>Median absolute error output is non-negative floating point. The best value
is 0.0. Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'}</em><em> or </em><em>array-like of shape</em><em>             (</em><em>n_outputs</em><em>,</em><em>)</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output values. Array-like value defines
weights used to average errors.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of errors in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Errors of all outputs are averaged with uniform weight.</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – If multioutput is ‘raw_values’, then mean absolute error is returned
for each output separately.
If multioutput is ‘uniform_average’ or an ndarray of weights, then the
weighted average of all output errors is returned.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">median_absolute_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.5</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;raw_values&#39;</span><span class="p">)</span>
<span class="go">array([0.5, 1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">multioutput</span><span class="o">=</span><span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">])</span>
<span class="go">0.85</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.multilabel_confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">multilabel_confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samplewise</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.multilabel_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a confusion matrix for each class or sample.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<p>Compute class-wise (default) or sample-wise (samplewise=True) multilabel
confusion matrix to evaluate the accuracy of a classification, and output
confusion matrices for each class or sample.</p>
<p>In multilabel confusion matrix <span class="math notranslate nohighlight">\(MCM\)</span>, the count of true negatives
is <span class="math notranslate nohighlight">\(MCM_{:,0,0}\)</span>, false negatives is <span class="math notranslate nohighlight">\(MCM_{:,1,0}\)</span>,
true positives is <span class="math notranslate nohighlight">\(MCM_{:,1,1}\)</span> and false positives is
<span class="math notranslate nohighlight">\(MCM_{:,0,1}\)</span>.</p>
<p>Multiclass data will be treated as if binarized under a one-vs-rest
transformation. Returned confusion matrices will be in the order of
sorted unique labels in the union of (y_true, y_pred).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>) or             </em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>) or             </em><em>(</em><em>n_samples</em><em>,</em><em>)</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – A list of classes or column indices to select some (or to force
inclusion of classes absent from the data).</p></li>
<li><p><strong>samplewise</strong> (<em>bool</em><em>, </em><em>default=False</em>) – In the multilabel case, this calculates a confusion matrix per sample.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>multi_confusion</strong> – A 2x2 confusion matrix corresponding to each output in the input.
When calculating class-wise multi_confusion (default), then
n_outputs = n_labels; when calculating sample-wise multi_confusion
(samplewise=True), n_outputs = n_samples. If <code class="docutils literal notranslate"><span class="pre">labels</span></code> is defined,
the results will be returned in the order specified in <code class="docutils literal notranslate"><span class="pre">labels</span></code>,
otherwise the results will be returned in sorted order by default.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_outputs, 2, 2)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The multilabel_confusion_matrix calculates class-wise or sample-wise
multilabel confusion matrices, and in multiclass tasks, labels are
binarized under a one-vs-rest way; while confusion_matrix calculates
one confusion matrix for confusion between every two classes.</p>
<p class="rubric">Examples</p>
<p>Multilabel-indicator case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">multilabel_confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>                   <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multilabel_confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">array([[[1, 0],</span>
<span class="go">        [0, 1]],</span>

<span class="go">       [[1, 0],</span>
<span class="go">        [0, 1]],</span>

<span class="go">       [[0, 1],</span>
<span class="go">        [1, 0]]])</span>
</pre></div>
</div>
<p>Multiclass case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multilabel_confusion_matrix</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
<span class="gp">... </span>                            <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;ant&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">])</span>
<span class="go">array([[[3, 1],</span>
<span class="go">        [0, 2]],</span>

<span class="go">       [[5, 0],</span>
<span class="go">        [1, 0]],</span>

<span class="go">       [[2, 1],</span>
<span class="go">        [1, 2]]])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.mutual_info_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">mutual_info_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contingency</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.mutual_info_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Mutual Information between two clusterings.</p>
<p>The Mutual Information is a measure of the similarity between two labels of
the same data. Where <span class="math notranslate nohighlight">\(|U_i|\)</span> is the number of the samples
in cluster <span class="math notranslate nohighlight">\(U_i\)</span> and <span class="math notranslate nohighlight">\(|V_j|\)</span> is the number of the
samples in cluster <span class="math notranslate nohighlight">\(V_j\)</span>, the Mutual Information
between clusterings <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> is given as:</p>
<div class="math notranslate nohighlight">
\[MI(U,V)=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i\cap V_j|}{N}
\log\frac{N|U_i \cap V_j|}{|U_i||V_j|}\]</div>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is furthermore symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with
<code class="docutils literal notranslate"><span class="pre">label_pred</span></code> will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>labels_pred</strong> (<em>int array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>contingency</strong> (<em>{ndarray</em><em>, </em><em>sparse matrix} of shape</em><em>             (</em><em>n_classes_true</em><em>, </em><em>n_classes_pred</em><em>)</em><em>, </em><em>default=None</em>) – A contingency matrix given by the <code class="xref py py-func docutils literal notranslate"><span class="pre">contingency_matrix()</span></code> function.
If value is <code class="docutils literal notranslate"><span class="pre">None</span></code>, it will be computed, otherwise the given value is
used, with <code class="docutils literal notranslate"><span class="pre">labels_true</span></code> and <code class="docutils literal notranslate"><span class="pre">labels_pred</span></code> ignored.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>mi</strong> – Mutual information, a non-negative value</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>The logarithm used is the natural logarithm (base-e).</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a></dt><dd><p>Adjusted against chance Mutual Information.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normalized_mutual_info_score</span></code></a></dt><dd><p>Normalized Mutual Information.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.nan_euclidean_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">nan_euclidean_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">squared</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">missing_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">nan</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.nan_euclidean_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculate the euclidean distances in the presence of missing values.</p>
<p>Compute the euclidean distance between each pair of samples in X and Y,
where Y=X is assumed if Y=None. When calculating the distance between a
pair of samples, this formulation ignores feature coordinates with a
missing value in either sample and scales up the weight of the remaining
coordinates:</p>
<blockquote>
<div><p>dist(x,y) = sqrt(weight * sq. distance from present coordinates)
where,
weight = Total # of coordinates / # of present coordinates</p>
</div></blockquote>
<p>For example, the distance between <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">na,</span> <span class="pre">na,</span> <span class="pre">6]</span></code> and <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">na,</span> <span class="pre">4,</span> <span class="pre">5]</span></code>
is:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\sqrt{\frac{4}{2}((3-1)^2 + (6-5)^2)}\]</div>
</div></blockquote>
<p>If all the coordinates are missing or if there are no common present
coordinates then NaN is returned for that pair.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape=</em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – </p></li>
<li><p><strong>Y</strong> (<em>array-like of shape=</em><em>(</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – </p></li>
<li><p><strong>squared</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Return squared Euclidean distances.</p></li>
<li><p><strong>missing_values</strong> (<em>np.nan</em><em> or </em><em>int</em><em>, </em><em>default=np.nan</em>) – Representation of missing value.</p></li>
<li><p><strong>copy</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Make and use a deep copy of X and Y (if Y exists).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>distances</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></dt><dd><p>Distances between pairs of elements of X and Y.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.pairwise</span> <span class="kn">import</span> <span class="n">nan_euclidean_distances</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s2">&quot;NaN&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">nan</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan_euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span> <span class="c1"># distance between rows of X</span>
<span class="go">array([[0.        , 1.41421356],</span>
<span class="go">       [1.41421356, 0.        ]])</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># get distance to origin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nan_euclidean_distances</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([[1.        ],</span>
<span class="go">       [1.41421356]])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<ul class="simple">
<li><p>John K. Dixon, “Pattern Recognition with Partly Missing Data”,
IEEE Transactions on Systems, Man, and Cybernetics, Volume: 9, Issue:
10, pp. 617 - 621, Oct. 1979.
<a class="reference external" href="http://ieeexplore.ieee.org/abstract/document/4310090/">http://ieeexplore.ieee.org/abstract/document/4310090/</a></p></li>
</ul>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.ndcg_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">ndcg_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore_ties</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.ndcg_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Normalized Discounted Cumulative Gain.</p>
<p>Sum the true scores ranked in the order induced by the predicted scores,
after applying a logarithmic discount. Then divide by the best possible
score (Ideal DCG, obtained for a perfect ranking) to obtain a score between
0 and 1.</p>
<p>This ranking metric yields a high value if true labels are ranked high by
<code class="docutils literal notranslate"><span class="pre">y_score</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – True targets of multilabel classification, or true scores of entities
to be ranked.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_labels</em><em>)</em>) – Target scores, can either be probability estimates, confidence values,
or non-thresholded measure of decisions (as returned by
“decision_function” on some classifiers).</p></li>
<li><p><strong>k</strong> (<em>int</em><em>, </em><em>default=None</em>) – Only consider the highest k scores in the ranking. If None, use all
outputs.</p></li>
<li><p><strong>sample_weight</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, all samples are given the same weight.</p></li>
<li><p><strong>ignore_ties</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Assume that there are no ties in y_score (which is likely to be the
case if y_score is continuous) for efficiency gains.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>normalized_discounted_cumulative_gain</strong> – The averaged NDCG scores for all samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float in [0., 1.]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.dcg_score" title="sklearn.metrics.dcg_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">dcg_score</span></code></a></dt><dd><p>Discounted Cumulative Gain (not normalized).</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Discounted_cumulative_gain">Wikipedia entry for Discounted Cumulative Gain</a></p>
<p>Jarvelin, K., &amp; Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.</p>
<p>Wang, Y., Wang, L., Li, Y., He, D., Chen, W., &amp; Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013)</p>
<p>McSherry, F., &amp; Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">ndcg_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we have groud-truth relevance of some answers to a query:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">true_relevance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we predict some scores (relevance) for the answers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">70</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="go">0.69...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mf">.05</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.0</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span>
<span class="go">0.49...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we can set k to truncate the sum; only top k answers contribute.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="go">0.35...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># the normalization takes k into account so a perfect answer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># would still get 1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">true_relevance</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># now we have some ties in our prediction</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># by default ties are averaged, so here we get the average (normalized)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># true relevance of our top predictions: (10 / 10 + 5 / 10) / 2 = .75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># we can choose to ignore ties for faster results, but only</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># if we know there aren&#39;t ties in our scores, otherwise we get</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># wrong results:</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ndcg_score</span><span class="p">(</span><span class="n">true_relevance</span><span class="p">,</span>
<span class="gp">... </span>          <span class="n">scores</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ignore_ties</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.normalized_mutual_info_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">normalized_mutual_info_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'arithmetic'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.normalized_mutual_info_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Normalized Mutual Information between two clusterings.</p>
<p>Normalized Mutual Information (NMI) is a normalization of the Mutual
Information (MI) score to scale the results between 0 (no mutual
information) and 1 (perfect correlation). In this function, mutual
information is normalized by some generalized mean of <code class="docutils literal notranslate"><span class="pre">H(labels_true)</span></code>
and <code class="docutils literal notranslate"><span class="pre">H(labels_pred))</span></code>, defined by the <cite>average_method</cite>.</p>
<p>This measure is not adjusted for chance. Therefore
<a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">adjusted_mutual_info_score()</span></code></a> might be preferred.</p>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is furthermore symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with
<code class="docutils literal notranslate"><span class="pre">label_pred</span></code> will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>labels_pred</strong> (<em>int array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – A clustering of the data into disjoint subsets.</p></li>
<li><p><strong>average_method</strong> (<em>str</em><em>, </em><em>default='arithmetic'</em>) – <p>How to compute the normalizer in the denominator. Possible options
are ‘min’, ‘geometric’, ‘arithmetic’, and ‘max’.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">average_method</span></code> changed from ‘geometric’ to
‘arithmetic’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>nmi</strong> – score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.v_measure_score" title="sklearn.metrics.v_measure_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">v_measure_score</span></code></a></dt><dd><p>V-Measure (NMI with arithmetic mean option).</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_rand_score</span></code></a></dt><dd><p>Adjusted Rand Index.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a></dt><dd><p>Adjusted Mutual Information (adjusted against chance).</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Perfect labelings are both homogeneous and complete, hence have
score 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">normalized_mutual_info_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>If classes members are completely split across different clusters,
the assignment is totally in-complete, hence the NMI is null:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">normalized_mutual_info_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="gp">... </span>
<span class="go">0.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pair_confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pair_confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pair_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>Pair confusion matrix arising from two clusterings.</p>
<p>The pair confusion matrix <span class="math notranslate nohighlight">\(C\)</span> computes a 2 by 2 similarity matrix
between two clusterings by considering all pairs of samples and counting
pairs that are assigned into the same or into different clusters under
the true and predicted clusterings.</p>
<p>Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
<span class="math notranslate nohighlight">\(C_{00}\)</span>, false negatives is <span class="math notranslate nohighlight">\(C_{10}\)</span>, true positives is
<span class="math notranslate nohighlight">\(C_{11}\)</span> and false positives is <span class="math notranslate nohighlight">\(C_{01}\)</span>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>dtype=integral</em>) – Ground truth class labels to be used as a reference.</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>dtype=integral</em>) – Cluster labels to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>C</strong> – The contingency matrix.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (2, 2), dtype=np.int64</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.rand_score" title="sklearn.metrics.rand_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">rand_score</span></code></a></dt><dd><p>Rand Score</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_rand_score</span></code></a></dt><dd><p>Adjusted Rand Score</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a></dt><dd><p>Adjusted Mutual Information</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">pair_confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pair_confusion_matrix</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">array([[8, 0],</span>
<span class="go">       [0, 4]]...</span>
</pre></div>
</div>
<p>Labelings that assign all classes members to the same clusters
are complete but may be not always pure, hence penalized, and
have some off-diagonal non-zero entries:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pair_confusion_matrix</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">array([[8, 2],</span>
<span class="go">       [0, 2]]...</span>
</pre></div>
</div>
<p>Note that the matrix is not symmetric.</p>
<p class="rubric">References</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise_distances">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">force_all_finite</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise_distances" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the distance matrix from a vector array X and optional Y.</p>
<p>This method takes either a vector array or a distance matrix, and returns
a distance matrix. If the input is a vector array, the distances are
computed. If the input is a distances matrix, it is returned instead.</p>
<p>This method provides a safe way to take a distance matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.</p>
<p>If Y is given (default is None), then the returned matrix is the pairwise
distance between the arrays from both X and Y.</p>
<p>Valid values for metric are:</p>
<ul class="simple">
<li><p>From scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]. These metrics support sparse matrix
inputs.
[‘nan_euclidean’] but it does not yet support sparse matrices.</p></li>
<li><p>From scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’, ‘mahalanobis’,
‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’,
‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’]
See the documentation for scipy.spatial.distance for details on these
metrics. These metrics do not support sparse matrix inputs.</p></li>
</ul>
<p>Note that in the case of ‘cityblock’, ‘cosine’ and ‘euclidean’ (which are
valid scipy.spatial.distance metrics), the scikit-learn implementation
will be used, which is faster and has support for sparse matrices (except
for ‘cityblock’). For a verbose description of the metrics from
scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics
function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise distances between samples, or a feature array.
The shape of the array should be (n_samples_X, n_samples_X) if
metric == “precomputed” and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – An optional second feature array. Only allowed if
metric != “precomputed”.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by scipy.spatial.distance.pdist for its metric parameter, or
a metric listed in <code class="docutils literal notranslate"><span class="pre">pairwise.PAIRWISE_DISTANCE_FUNCTIONS</span></code>.
If metric is “precomputed”, X is assumed to be a distance matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>force_all_finite</strong> (<em>bool</em><em> or </em><em>'allow-nan'</em><em>, </em><em>default=True</em>) – <p>Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignored
for a metric listed in <code class="docutils literal notranslate"><span class="pre">pairwise.PAIRWISE_DISTANCE_FUNCTIONS</span></code>. The
possibilities are:</p>
<ul>
<li><p>True: Force all values of array to be finite.</p></li>
<li><p>False: accepts np.inf, np.nan, pd.NA in array.</p></li>
<li><p>’allow-nan’: accepts only np.nan and pd.NA values in array. Values
cannot be infinite.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22: </span><code class="docutils literal notranslate"><span class="pre">force_all_finite</span></code> accepts the string <code class="docutils literal notranslate"><span class="pre">'allow-nan'</span></code>.</p>
</div>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.23: </span>Accepts <cite>pd.NA</cite> and converts it into <cite>np.nan</cite>.</p>
</div>
</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>D</strong> – A distance matrix D such that D_{i, j} is the distance between the
ith and jth vectors of the given matrix X, if Y is None.
If Y is not None, then D_{i, j} is the distance between the ith array
from X and the jth array from Y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.pairwise_distances_chunked" title="sklearn.metrics.pairwise_distances_chunked"><code class="xref py py-obj docutils literal notranslate"><span class="pre">pairwise_distances_chunked</span></code></a></dt><dd><p>Performs the same calculation as this function, but returns a generator of chunks of the distance matrix, in order to limit memory usage.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">paired_distances</span></code></dt><dd><p>Computes the distances between corresponding elements of two arrays.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise_distances_argmin">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_argmin</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise_distances_argmin" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute minimum distances between one point and a set of points.</p>
<p>This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance).</p>
<p>This is mostly equivalent to calling:</p>
<blockquote>
<div><p>pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)</p>
</div></blockquote>
<p>but uses much less memory, and is faster for large arrays.</p>
<p>This function works with dense 2D arrays only.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>Y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – Arrays containing points.</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>default=1</em>) – Axis along which the argmin and distances are to be computed.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=&quot;euclidean&quot;</em>) – <p>Metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>metric_kwargs</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Keyword arguments to pass to specified metric function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>argmin</strong> – Y[argmin[i], :] is the row in Y that is closest to X[i, :].</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>numpy.ndarray</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances</span></code></a>, <a class="reference internal" href="#sklearn.metrics.pairwise_distances_argmin_min" title="sklearn.metrics.pairwise_distances_argmin_min"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances_argmin_min</span></code></a></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise_distances_argmin_min">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_argmin_min</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">axis</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_kwargs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise_distances_argmin_min" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute minimum distances between one point and a set of points.</p>
<p>This function computes for each row in X, the index of the row of Y which
is closest (according to the specified distance). The minimal distances are
also returned.</p>
<p>This is mostly equivalent to calling:</p>
<blockquote>
<div><dl class="simple">
<dt>(pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),</dt><dd><p>pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))</p>
</dd>
</dl>
</div></blockquote>
<p>but uses much less memory, and is faster for large arrays.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>Y</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em>) – Array containing points.</p></li>
<li><p><strong>axis</strong> (<em>int</em><em>, </em><em>default=1</em>) – Axis along which the argmin and distances are to be computed.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – <p>Metric to use for distance computation. Any metric from scikit-learn
or scipy.spatial.distance can be used.</p>
<p>If metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays as input and return one value indicating the
distance between them. This works for Scipy’s metrics, but is less
efficient than passing the metric name as a string.</p>
<p>Distance matrices are not supported.</p>
<p>Valid values for metric are:</p>
<ul>
<li><p>from scikit-learn: [‘cityblock’, ‘cosine’, ‘euclidean’, ‘l1’, ‘l2’,
‘manhattan’]</p></li>
<li><p>from scipy.spatial.distance: [‘braycurtis’, ‘canberra’, ‘chebyshev’,
‘correlation’, ‘dice’, ‘hamming’, ‘jaccard’, ‘kulsinski’,
‘mahalanobis’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’,
‘seuclidean’, ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’,
‘yule’]</p></li>
</ul>
<p>See the documentation for scipy.spatial.distance for details on these
metrics.</p>
</p></li>
<li><p><strong>metric_kwargs</strong> (<em>dict</em><em>, </em><em>default=None</em>) – Keyword arguments to pass to specified metric function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>argmin</strong> (<em>ndarray</em>) – Y[argmin[i], :] is the row in Y that is closest to X[i, :].</p></li>
<li><p><strong>distances</strong> (<em>ndarray</em>) – distances[i] is the distance between the i-th row in X and the
argmin[i]-th row in Y.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.pairwise_distances" title="sklearn.metrics.pairwise_distances"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances</span></code></a>, <a class="reference internal" href="#sklearn.metrics.pairwise_distances_argmin" title="sklearn.metrics.pairwise_distances_argmin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise_distances_argmin</span></code></a></p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise_distances_chunked">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_distances_chunked</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">working_memory</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise_distances_chunked" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a distance matrix chunk by chunk with optional reduction.</p>
<p>In cases where not all of a pairwise distance matrix needs to be stored at
once, this is used to calculate pairwise distances in
<code class="docutils literal notranslate"><span class="pre">working_memory</span></code>-sized chunks.  If <code class="docutils literal notranslate"><span class="pre">reduce_func</span></code> is given, it is run
on each chunk and its return values are concatenated into lists, arrays
or sparse matrices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise distances between samples, or a feature array.
The shape the array should be (n_samples_X, n_samples_X) if
metric=’precomputed’ and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – An optional second feature array. Only allowed if
metric != “precomputed”.</p></li>
<li><p><strong>reduce_func</strong> (<em>callable</em><em>, </em><em>default=None</em>) – <p>The function which is applied on each chunk of the distance matrix,
reducing it to needed values.  <code class="docutils literal notranslate"><span class="pre">reduce_func(D_chunk,</span> <span class="pre">start)</span></code>
is called repeatedly, where <code class="docutils literal notranslate"><span class="pre">D_chunk</span></code> is a contiguous vertical
slice of the pairwise distance matrix, starting at row <code class="docutils literal notranslate"><span class="pre">start</span></code>.
It should return one of: None; an array, a list, or a sparse matrix
of length <code class="docutils literal notranslate"><span class="pre">D_chunk.shape[0]</span></code>; or a tuple of such objects. Returning
None is useful for in-place operations, rather than reductions.</p>
<p>If None, pairwise_distances_chunked returns a generator of vertical
chunks of the distance matrix.</p>
</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by scipy.spatial.distance.pdist for its metric parameter, or
a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.
If metric is “precomputed”, X is assumed to be a distance matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two arrays from X as input and return a value indicating
the distance between them.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>working_memory</strong> (<em>int</em><em>, </em><em>default=None</em>) – The sought maximum memory for temporary distance matrix chunks.
When None (default), the value of
<code class="docutils literal notranslate"><span class="pre">sklearn.get_config()['working_memory']</span></code> is used.</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>D_chunk</strong> (<em>{ndarray, sparse matrix}</em>) – A contiguous slice of distance matrix, optionally processed by
<code class="docutils literal notranslate"><span class="pre">reduce_func</span></code>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Without reduce_func:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">pairwise_distances_chunked</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D_chunk</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">D_chunk</span>
<span class="go">array([[0.  ..., 0.29..., 0.41..., 0.19..., 0.57...],</span>
<span class="go">       [0.29..., 0.  ..., 0.57..., 0.41..., 0.76...],</span>
<span class="go">       [0.41..., 0.57..., 0.  ..., 0.44..., 0.90...],</span>
<span class="go">       [0.19..., 0.41..., 0.44..., 0.  ..., 0.51...],</span>
<span class="go">       [0.57..., 0.76..., 0.90..., 0.51..., 0.  ...]])</span>
</pre></div>
</div>
<p>Retrieve all neighbors and average distance within radius r:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="mf">.2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">reduce_func</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">neigh</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">)</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">D_chunk</span><span class="p">]</span>
<span class="gp">... </span>    <span class="n">avg_dist</span> <span class="o">=</span> <span class="p">(</span><span class="n">D_chunk</span> <span class="o">*</span> <span class="p">(</span><span class="n">D_chunk</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">neigh</span><span class="p">,</span> <span class="n">avg_dist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span><span class="p">,</span> <span class="n">avg_dist</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span>
<span class="go">[array([0, 3]), array([1]), array([2]), array([0, 3]), array([4])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">avg_dist</span>
<span class="go">array([0.039..., 0.        , 0.        , 0.039..., 0.        ])</span>
</pre></div>
</div>
<p>Where r is defined per sample, we need to make use of <code class="docutils literal notranslate"><span class="pre">start</span></code>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="p">[</span><span class="mf">.2</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">reduce_func</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">):</span>
<span class="gp">... </span>    <span class="n">neigh</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">d</span> <span class="o">&lt;</span> <span class="n">r</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="gp">... </span>             <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">D_chunk</span><span class="p">,</span> <span class="n">start</span><span class="p">)]</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">neigh</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">neigh</span>
<span class="go">[array([0, 3]), array([0, 1]), array([2]), array([0, 3]), array([4])]</span>
</pre></div>
</div>
<p>Force row-by-row generation by reducing <code class="docutils literal notranslate"><span class="pre">working_memory</span></code>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gen</span> <span class="o">=</span> <span class="n">pairwise_distances_chunked</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">reduce_func</span><span class="o">=</span><span class="n">reduce_func</span><span class="p">,</span>
<span class="gp">... </span>                                 <span class="n">working_memory</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="go">[array([0, 3])]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">next</span><span class="p">(</span><span class="n">gen</span><span class="p">)</span>
<span class="go">[array([0, 1])]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.pairwise_kernels">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">pairwise_kernels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filter_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.pairwise_kernels" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the kernel between arrays X and optional array Y.</p>
<p>This method takes either a vector array or a kernel matrix, and returns
a kernel matrix. If the input is a vector array, the kernels are
computed. If the input is a kernel matrix, it is returned instead.</p>
<p>This method provides a safe way to take a kernel matrix as input, while
preserving compatibility with many other algorithms that take a vector
array.</p>
<p>If Y is given (default is None), then the returned matrix is the pairwise
kernel between the arrays from both X and Y.</p>
<dl class="simple">
<dt>Valid values for metric are:</dt><dd><p>[‘additive_chi2’, ‘chi2’, ‘linear’, ‘poly’, ‘polynomial’, ‘rbf’,
‘laplacian’, ‘sigmoid’, ‘cosine’]</p>
</dd>
</dl>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_X</em><em>, </em><em>n_samples_X</em><em>) or             </em><em>(</em><em>n_samples_X</em><em>, </em><em>n_features</em><em>)</em>) – Array of pairwise kernels between samples, or a feature array.
The shape of the array should be (n_samples_X, n_samples_X) if
metric == “precomputed” and (n_samples_X, n_features) otherwise.</p></li>
<li><p><strong>Y</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples_Y</em><em>, </em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – A second feature array only if X has shape (n_samples_X, n_features).</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default=&quot;linear&quot;</em>) – The metric to use when calculating kernel between instances in a
feature array. If metric is a string, it must be one of the metrics
in pairwise.PAIRWISE_KERNEL_FUNCTIONS.
If metric is “precomputed”, X is assumed to be a kernel matrix.
Alternatively, if metric is a callable function, it is called on each
pair of instances (rows) and the resulting value recorded. The callable
should take two rows from X as input and return the corresponding
kernel value as a single number. This means that callables from
<a class="reference internal" href="#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a> are not allowed, as they operate on
matrices, not single samples. Use the string identifying the kernel
instead.</p></li>
<li><p><strong>filter_params</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to filter invalid parameters or not.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to use for the computation. This works by breaking
down the pairwise matrix into n_jobs even slices and computing them in
parallel.</p>
<p><code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the kernel function.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>K</strong> – A kernel matrix K such that K_{i, j} is the kernel between the
ith and jth vectors of the given matrix X, if Y is None.
If Y is not None, then K_{i, j} is the kernel between the ith array
from X and the jth array from Y.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples_X, n_samples_X) or             (n_samples_X, n_samples_Y)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>If metric is ‘precomputed’, Y is ignored and X is returned.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.plot_confusion_matrix">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">plot_confusion_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">display_labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">include_values</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">xticks_rotation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'horizontal'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">values_format</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cmap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'viridis'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colorbar</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.plot_confusion_matrix" title="Permalink to this definition">¶</a></dt>
<dd><p>DEPRECATED: Function plot_confusion_matrix is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.</p>
<p>Plot Confusion Matrix.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span><cite>plot_confusion_matrix</cite> is deprecated in 1.0 and will be removed in
1.2. Use one of the following class methods:
<a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_predictions" title="sklearn.metrics.ConfusionMatrixDisplay.from_predictions"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_predictions()</span></code></a> or
<a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay.from_estimator" title="sklearn.metrics.ConfusionMatrixDisplay.from_estimator"><code class="xref py py-func docutils literal notranslate"><span class="pre">from_estimator()</span></code></a>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator instance</em>) – Fitted classifier or a fitted <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>
in which the last estimator is a classifier.</p></li>
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input values.</p></li>
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – List of labels to index the matrix. This may be used to reorder or
select a subset of labels. If <cite>None</cite> is given, those that appear at
least once in <cite>y_true</cite> or <cite>y_pred</cite> are used in sorted order.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>normalize</strong> (<em>{'true'</em><em>, </em><em>'pred'</em><em>, </em><em>'all'}</em><em>, </em><em>default=None</em>) – <p>Either to normalize the counts display in the matrix:</p>
<blockquote>
<div><ul>
<li><p>if <cite>‘true’</cite>, the confusion matrix is normalized over the true
conditions (e.g. rows);</p></li>
<li><p>if <cite>‘pred’</cite>, the confusion matrix is normalized over the
predicted conditions (e.g. columns);</p></li>
<li><p>if <cite>‘all’</cite>, the confusion matrix is normalized by the total
number of samples;</p></li>
<li><p>if <cite>None</cite> (default), the confusion matrix will not be normalized.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>display_labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Target names used for plotting. By default, <cite>labels</cite> will be used if
it is defined, otherwise the unique labels of <cite>y_true</cite> and <cite>y_pred</cite>
will be used.</p></li>
<li><p><strong>include_values</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Includes values in confusion matrix.</p></li>
<li><p><strong>xticks_rotation</strong> (<em>{'vertical'</em><em>, </em><em>'horizontal'}</em><em> or </em><em>float</em><em>,                         </em><em>default='horizontal'</em>) – Rotation of xtick labels.</p></li>
<li><p><strong>values_format</strong> (<em>str</em><em>, </em><em>default=None</em>) – Format specification for values in confusion matrix. If <cite>None</cite>,
the format specification is ‘d’ or ‘.2g’ whichever is shorter.</p></li>
<li><p><strong>cmap</strong> (<em>str</em><em> or </em><em>matplotlib Colormap</em><em>, </em><em>default='viridis'</em>) – Colormap recognized by matplotlib.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib Axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is
created.</p></li>
<li><p><strong>colorbar</strong> (<em>bool</em><em>, </em><em>default=True</em>) – <p>Whether or not to add a colorbar to the plot.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.confusion_matrix" title="sklearn.metrics.confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">confusion_matrix</span></code></a></dt><dd><p>Compute Confusion Matrix to evaluate the accuracy of a classification.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.ConfusionMatrixDisplay" title="sklearn.metrics.ConfusionMatrixDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code></a></dt><dd><p>Confusion Matrix visualization.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">plot_confusion_matrix</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>  
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.plot_det_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">plot_det_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.plot_det_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot detection error tradeoff (DET) curve.</p>
<p>Extra keyword arguments will be passed to matplotlib’s <cite>plot</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator instance</em>) – Fitted classifier or a fitted <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>
in which the last estimator is a classifier.</p></li>
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input values.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>response_method</strong> (<em>{'predict_proba'</em><em>, </em><em>'decision_function'</em><em>, </em><em>'auto'}             default='auto'</em>) – Specifies whether to use <span class="xref std std-term">predict_proba</span> or
<span class="xref std std-term">decision_function</span> as the predicted target response. If set to
‘auto’, <span class="xref std std-term">predict_proba</span> is tried first and if it does not exist
<span class="xref std std-term">decision_function</span> is tried next.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of DET curve for labeling. If <cite>None</cite>, use the name of the
estimator.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is created.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – The label of the positive class.
When <cite>pos_label=None</cite>, if <cite>y_true</cite> is in {-1, 1} or {0, 1},
<cite>pos_label</cite> is set to 1, otherwise an error will be raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.DetCurveDisplay" title="sklearn.metrics.DetCurveDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">DetCurveDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.det_curve" title="sklearn.metrics.det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det_curve</span></code></a></dt><dd><p>Compute error rates for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.DetCurveDisplay" title="sklearn.metrics.DetCurveDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DetCurveDisplay</span></code></a></dt><dd><p>DET curve visualization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.plot_roc_curve" title="sklearn.metrics.plot_roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_roc_curve</span></code></a></dt><dd><p>Plot Receiver operating characteristic (ROC) curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">plot_det_curve</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>                                   
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.plot_precision_recall_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">plot_precision_recall_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.plot_precision_recall_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot Precision Recall Curve for binary classifiers.</p>
<p>Extra keyword arguments will be passed to matplotlib’s <cite>plot</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator instance</em>) – Fitted classifier or a fitted <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>
in which the last estimator is a classifier.</p></li>
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input values.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Binary target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>response_method</strong> (<em>{'predict_proba'</em><em>, </em><em>'decision_function'</em><em>, </em><em>'auto'}</em><em>,                       </em><em>default='auto'</em>) – Specifies whether to use <span class="xref std std-term">predict_proba</span> or
<span class="xref std std-term">decision_function</span> as the target response. If set to ‘auto’,
<span class="xref std std-term">predict_proba</span> is tried first and if it does not exist
<span class="xref std std-term">decision_function</span> is tried next.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name for labeling curve. If <cite>None</cite>, the name of the
estimator is used.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is created.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – <p>The class considered as the positive class when computing the precision
and recall metrics. By default, <cite>estimators.classes_[1]</cite> is considered
as the positive class.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
<li><p><strong>**kwargs</strong> (<em>dict</em>) – Keyword arguments to be passed to matplotlib’s <cite>plot</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.PrecisionRecallDisplay" title="sklearn.metrics.PrecisionRecallDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">PrecisionRecallDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.precision_recall_curve" title="sklearn.metrics.precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_curve</span></code></a></dt><dd><p>Compute precision-recall pairs for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.PrecisionRecallDisplay" title="sklearn.metrics.PrecisionRecallDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PrecisionRecallDisplay</span></code></a></dt><dd><p>Precision Recall visualization.</p>
</dd>
</dl>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.plot_roc_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">plot_roc_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimator</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_intermediate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">response_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.plot_roc_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot Receiver operating characteristic (ROC) curve.</p>
<p>Extra keyword arguments will be passed to matplotlib’s <cite>plot</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimator</strong> (<em>estimator instance</em>) – Fitted classifier or a fitted <code class="xref py py-class docutils literal notranslate"><span class="pre">Pipeline</span></code>
in which the last estimator is a classifier.</p></li>
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input values.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>drop_intermediate</strong> (<em>boolean</em><em>, </em><em>default=True</em>) – Whether to drop some suboptimal thresholds which would not appear
on a plotted ROC curve. This is useful in order to create lighter
ROC curves.</p></li>
<li><p><strong>response_method</strong> (<em>{'predict_proba'</em><em>, </em><em>'decision_function'</em><em>, </em><em>'auto'}     default='auto'</em>) – Specifies whether to use <span class="xref std std-term">predict_proba</span> or
<span class="xref std std-term">decision_function</span> as the target response. If set to ‘auto’,
<span class="xref std std-term">predict_proba</span> is tried first and if it does not exist
<span class="xref std std-term">decision_function</span> is tried next.</p></li>
<li><p><strong>name</strong> (<em>str</em><em>, </em><em>default=None</em>) – Name of ROC Curve for labeling. If <cite>None</cite>, use the name of the
estimator.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib axes</em><em>, </em><em>default=None</em>) – Axes object to plot on. If <cite>None</cite>, a new figure and axes is created.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=None</em>) – <p>The class considered as the positive class when computing the roc auc
metrics. By default, <cite>estimators.classes_[1]</cite> is considered
as the positive class.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>display</strong> – Object that stores computed values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.metrics.RocCurveDisplay" title="sklearn.metrics.RocCurveDisplay"><code class="xref py py-class docutils literal notranslate"><span class="pre">RocCurveDisplay</span></code></a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a></dt><dd><p>Compute Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.RocCurveDisplay" title="sklearn.metrics.RocCurveDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RocCurveDisplay</span></code></a></dt><dd><p>ROC Curve visualization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></dt><dd><p>Compute the area under the ROC curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">model_selection</span><span class="p">,</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">SVC(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">metrics</span><span class="o">.</span><span class="n">plot_roc_curve</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>                                   
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.precision_recall_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">precision_recall_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">probas_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.precision_recall_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute precision-recall pairs for different probability thresholds.</p>
<p>Note: this implementation is restricted to the binary classification task.</p>
<p>The precision is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fp)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fp</span></code> the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.</p>
<p>The recall is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fn)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fn</span></code> the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.</p>
<p>The last precision and recall values are 1. and 0. respectively and do not
have a corresponding threshold. This ensures that the graph starts on the
y axis.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True binary labels. If labels are not either {-1, 1} or {0, 1}, then
pos_label should be explicitly given.</p></li>
<li><p><strong>probas_pred</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, or non-thresholded measure of decisions (as returned by
<cite>decision_function</cite> on some classifiers).</p></li>
<li><p><strong>pos_label</strong> (<em>int</em><em> or </em><em>str</em><em>, </em><em>default=None</em>) – The label of the positive class.
When <code class="docutils literal notranslate"><span class="pre">pos_label=None</span></code>, if y_true is in {-1, 1} or {0, 1},
<code class="docutils literal notranslate"><span class="pre">pos_label</span></code> is set to 1, otherwise an error will be raised.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>precision</strong> (<em>ndarray of shape (n_thresholds + 1,)</em>) – Precision values such that element i is the precision of
predictions with score &gt;= thresholds[i] and the last element is 1.</p></li>
<li><p><strong>recall</strong> (<em>ndarray of shape (n_thresholds + 1,)</em>) – Decreasing recall values such that element i is the recall of
predictions with score &gt;= thresholds[i] and the last element is 0.</p></li>
<li><p><strong>thresholds</strong> (<em>ndarray of shape (n_thresholds,)</em>) – Increasing thresholds on the decision function used to compute
precision and recall. n_thresholds &lt;= len(np.unique(probas_pred)).</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.plot_precision_recall_curve" title="sklearn.metrics.plot_precision_recall_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_precision_recall_curve</span></code></a></dt><dd><p>Plot Precision Recall Curve for binary classifiers.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.PrecisionRecallDisplay" title="sklearn.metrics.PrecisionRecallDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">PrecisionRecallDisplay</span></code></a></dt><dd><p>Precision Recall visualization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">average_precision_score</span></code></a></dt><dd><p>Compute average precision from prediction scores.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.det_curve" title="sklearn.metrics.det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det_curve</span></code></a></dt><dd><p>Compute error rates for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a></dt><dd><p>Compute Receiver operating characteristic (ROC) curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_curve</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">y_true</span><span class="p">,</span> <span class="n">y_scores</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision</span>
<span class="go">array([0.66666667, 0.5       , 1.        , 1.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall</span>
<span class="go">array([1. , 0.5, 0.5, 0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">thresholds</span>
<span class="go">array([0.35, 0.4 , 0.8 ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.precision_recall_fscore_support">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">precision_recall_fscore_support</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warn_for</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">('precision',</span> <span class="pre">'recall',</span> <span class="pre">'f-score')</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.precision_recall_fscore_support" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute precision, recall, F-measure and support for each class.</p>
<p>The precision is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fp)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fp</span></code> the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.</p>
<p>The recall is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fn)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fn</span></code> the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.</p>
<p>The F-beta score can be interpreted as a weighted harmonic mean of
the precision and recall, where an F-beta score reaches its best
value at 1 and worst score at 0.</p>
<p>The F-beta score weights recall more than precision by a factor of
<code class="docutils literal notranslate"><span class="pre">beta</span></code>. <code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">==</span> <span class="pre">1.0</span></code> means recall and precision are equally important.</p>
<p>The support is the number of occurrences of each class in <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">pos_label</span> <span class="pre">is</span> <span class="pre">None</span></code> and in binary classification, this function
returns the average precision, recall and F-measure if <code class="docutils literal notranslate"><span class="pre">average</span></code>
is one of <code class="docutils literal notranslate"><span class="pre">'micro'</span></code>, <code class="docutils literal notranslate"><span class="pre">'macro'</span></code>, <code class="docutils literal notranslate"><span class="pre">'weighted'</span></code> or <code class="docutils literal notranslate"><span class="pre">'samples'</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The strength of recall versus precision in the F-score.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{'binary'</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,</em><em>'weighted'}</em><em>,             </em><em>default=None</em>) – <p>If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>warn_for</strong> (<em>tuple</em><em> or </em><em>set</em><em>, </em><em>for internal use</em>) – This determines which warnings will be made in the case that this
function is being used to return only one of its metrics.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – <dl class="simple">
<dt>Sets the value to return when there is a zero division:</dt><dd><ul>
<li><p>recall: when there are no positive labels</p></li>
<li><p>precision: when there are no positive predictions</p></li>
<li><p>f-score: both</p></li>
</ul>
</dd>
</dl>
<p>If set to “warn”, this acts as 0, but warnings are also raised.</p>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>precision</strong> (<em>float (if average is not None) or array of float, shape =        [n_unique_labels]</em>)</p></li>
<li><p><strong>recall</strong> (<em>float (if average is not None) or array of float, , shape =        [n_unique_labels]</em>)</p></li>
<li><p><strong>fbeta_score</strong> (<em>float (if average is not None) or array of float, shape =        [n_unique_labels]</em>)</p></li>
<li><p><strong>support</strong> (<em>None (if average is not None) or array of int, shape =        [n_unique_labels]</em>) – The number of occurrences of each label in <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.</p></li>
</ul>
</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code>, precision is undefined.
When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, recall is undefined.
In such cases, by default the metric will be set to 0, as will f-score,
and <code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code> will be raised. This behavior can be
modified with <code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id38"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Precision_and_recall">Wikipedia entry for the Precision and recall</a>.</p>
</dd>
<dt class="label" id="id39"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p>
</dd>
<dt class="label" id="id41"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf">Discriminative Methods for Multi-labeled Classification Advances
in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu
Godbole, Sunita Sarawagi</a>.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_recall_fscore_support</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;pig&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">(0.22..., 0.33..., 0.26..., None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="go">(0.33..., 0.33..., 0.33..., None)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="go">(0.22..., 0.33..., 0.26..., None)</span>
</pre></div>
</div>
<p>It is possible to compute per-label precisions, recalls, F1-scores and
supports instead of averaging:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span><span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;pig&#39;</span><span class="p">,</span> <span class="s1">&#39;dog&#39;</span><span class="p">,</span> <span class="s1">&#39;cat&#39;</span><span class="p">])</span>
<span class="go">(array([0.        , 0.        , 0.66...]),</span>
<span class="go"> array([0., 0., 1.]), array([0. , 0. , 0.8]),</span>
<span class="go"> array([2, 2, 2]))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.precision_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">precision_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.precision_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the precision.</p>
<p>The precision is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fp)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fp</span></code> the number of false positives. The precision is
intuitively the ability of the classifier not to label as positive a sample
that is negative.</p>
<p>The best value is 1 and the worst value is 0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <cite>labels</cite> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'</em><em>, </em><em>'binary'}             default='binary'</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division. If set to
“warn”, this acts as 0, but warnings are also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>precision</strong> – (n_unique_labels,)
Precision of the positive class in binary classification or weighted
average of the precision of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float (if average is not None) or array of float of shape</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code>, precision returns 0 and
raises <code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>. This behavior can be
modified with <code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">precision_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">0.22...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="go">0.22...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.66..., 0.        , 0.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.33..., 0.        , 0.        ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([0.33..., 1.        , 1.        ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.r2_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">r2_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multioutput</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'uniform_average'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.r2_score" title="Permalink to this definition">¶</a></dt>
<dd><p>R^2 (coefficient of determination) regression score function.</p>
<p>Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>{'raw_values'</em><em>, </em><em>'uniform_average'</em><em>, </em><em>'variance_weighted'}</em><em>,             </em><em>array-like of shape</em><em> (</em><em>n_outputs</em><em>,</em><em>) or </em><em>None</em><em>, </em><em>default='uniform_average'</em>) – <p>Defines aggregating of multiple output scores.
Array-like value defines weights used to average scores.
Default is “uniform_average”.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of scores in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Scores of all outputs are averaged with uniform weight.</p>
</dd>
<dt>’variance_weighted’ :</dt><dd><p>Scores of all outputs are averaged, weighted by the variances
of each individual output.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>Default value of multioutput is ‘uniform_average’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>z</strong> – The R^2 score or ndarray of scores if ‘multioutput’ is
‘raw_values’.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is not a symmetric function.</p>
<p>Unlike most other scores, R^2 score may be negative (it need not actually
be the square of a quantity R).</p>
<p>This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id42"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Wikipedia entry on the Coefficient of determination</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.948...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
<span class="gp">... </span>         <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
<span class="go">0.938...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">-3.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.rand_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">rand_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.rand_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Rand index.</p>
<p>The Rand Index computes a similarity measure between two clusterings
by considering all pairs of samples and counting pairs that are
assigned in the same or different clusters in the predicted and
true clusterings.</p>
<p>The raw RI score is:</p>
<blockquote>
<div><p>RI = (number of agreeing pairs) / (number of pairs)</p>
</div></blockquote>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>dtype=integral</em>) – Ground truth class labels to be used as a reference.</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>dtype=integral</em>) – Cluster labels to evaluate.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>RI</strong> – Similarity score between 0.0 and 1.0, inclusive, 1.0 stands for
perfect match.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_rand_score" title="sklearn.metrics.adjusted_rand_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_rand_score</span></code></a></dt><dd><p>Adjusted Rand Score</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.adjusted_mutual_info_score" title="sklearn.metrics.adjusted_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">adjusted_mutual_info_score</span></code></a></dt><dd><p>Adjusted Mutual Information</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Perfectly matching labelings have a score of 1 even</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">rand_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rand_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.83...</span>
</pre></div>
</div>
<p class="rubric">References</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.recall_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">recall_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'binary'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">zero_division</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'warn'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.recall_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the recall.</p>
<p>The recall is the ratio <code class="docutils literal notranslate"><span class="pre">tp</span> <span class="pre">/</span> <span class="pre">(tp</span> <span class="pre">+</span> <span class="pre">fn)</span></code> where <code class="docutils literal notranslate"><span class="pre">tp</span></code> is the number of
true positives and <code class="docutils literal notranslate"><span class="pre">fn</span></code> the number of false negatives. The recall is
intuitively the ability of the classifier to find all the positive samples.</p>
<p>The best value is 1 and the worst value is 0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>array-like</em><em>, </em><em>default=None</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <cite>labels</cite> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>default=1</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'</em><em>, </em><em>'binary'}             default='binary'</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code></a>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>zero_division</strong> (<em>&quot;warn&quot;</em><em>, </em><em>0</em><em> or </em><em>1</em><em>, </em><em>default=&quot;warn&quot;</em>) – Sets the value to return when there is a zero division. If set to
“warn”, this acts as 0, but warnings are also raised.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>recall</strong> – (n_unique_labels,)
Recall of the positive class in binary classification or weighted
average of the recall of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float (if average is not None) or array of float of shape</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.precision_recall_fscore_support" title="sklearn.metrics.precision_recall_fscore_support"><code class="xref py py-obj docutils literal notranslate"><span class="pre">precision_recall_fscore_support</span></code></a>, <a class="reference internal" href="#sklearn.metrics.balanced_accuracy_score" title="sklearn.metrics.balanced_accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">balanced_accuracy_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code class="xref py py-obj docutils literal notranslate"><span class="pre">multilabel_confusion_matrix</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, recall returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>. This behavior can be modified with
<code class="docutils literal notranslate"><span class="pre">zero_division</span></code>.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([1., 0., 0.])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.5, 0. , 0. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">zero_division</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="go">array([0.5, 1. , 1. ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.roc_auc_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">roc_auc_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">average</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'macro'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_fpr</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multi_class</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'raise'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.roc_auc_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)
from prediction scores.</p>
<p>Note: this implementation can be used with binary, multiclass and
multilabel classification, but some restrictions apply (see Parameters).</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – True labels or binary label indicators. The binary and multiclass cases
expect labels with shape (n_samples,) while the multilabel case expects
binary label indicators with shape (n_samples, n_classes).</p></li>
<li><p><strong>y_score</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – <p>Target scores.</p>
<ul>
<li><p>In the binary case, it corresponds to an array of shape
<cite>(n_samples,)</cite>. Both probability estimates and non-thresholded
decision values can be provided. The probability estimates correspond
to the <strong>probability of the class with the greater label</strong>,
i.e. <cite>estimator.classes_[1]</cite> and thus
<cite>estimator.predict_proba(X, y)[:, 1]</cite>. The decision values
corresponds to the output of <cite>estimator.decision_function(X, y)</cite>.
See more information in the <span class="xref std std-ref">User guide</span>;</p></li>
<li><p>In the multiclass case, it corresponds to an array of shape
<cite>(n_samples, n_classes)</cite> of probability estimates provided by the
<cite>predict_proba</cite> method. The probability estimates <strong>must</strong>
sum to 1 across the possible classes. In addition, the order of the
class scores must correspond to the order of <code class="docutils literal notranslate"><span class="pre">labels</span></code>,
if provided, or else to the numerical or lexicographical order of
the labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code>. See more information in the
<span class="xref std std-ref">User guide</span>;</p></li>
<li><p>In the multilabel case, it corresponds to an array of shape
<cite>(n_samples, n_classes)</cite>. Probability estimates are provided by the
<cite>predict_proba</cite> method and the non-thresholded decision values by
the <cite>decision_function</cite> method. The probability estimates correspond
to the <strong>probability of the class with the greater label for each
output</strong> of the classifier. See more information in the
<span class="xref std std-ref">User guide</span>.</p></li>
</ul>
</p></li>
<li><p><strong>average</strong> (<em>{'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>, </em><em>'weighted'}</em><em> or </em><em>None</em><em>,             </em><em>default='macro'</em>) – <p>If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise,
this determines the type of averaging performed on the data:
Note: multiclass ROC AUC currently only handles the ‘macro’ and
‘weighted’ averages.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by considering each element of the label
indicator matrix as a label.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average, weighted
by support (the number of true instances for each label).</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average.</p>
</dd>
</dl>
<p>Will be ignored when <code class="docutils literal notranslate"><span class="pre">y_true</span></code> is binary.</p>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>max_fpr</strong> (<em>float &gt; 0 and &lt;= 1</em><em>, </em><em>default=None</em>) – If not <code class="docutils literal notranslate"><span class="pre">None</span></code>, the standardized partial AUC <a href="#id66"><span class="problematic" id="id43">[2]_</span></a> over the range
[0, max_fpr] is returned. For the multiclass case, <code class="docutils literal notranslate"><span class="pre">max_fpr</span></code>,
should be either equal to <code class="docutils literal notranslate"><span class="pre">None</span></code> or <code class="docutils literal notranslate"><span class="pre">1.0</span></code> as AUC ROC partial
computation currently is not supported for multiclass.</p></li>
<li><p><strong>multi_class</strong> (<em>{'raise'</em><em>, </em><em>'ovr'</em><em>, </em><em>'ovo'}</em><em>, </em><em>default='raise'</em>) – <p>Only used for multiclass targets. Determines the type of configuration
to use. The default value raises an error, so either
<code class="docutils literal notranslate"><span class="pre">'ovr'</span></code> or <code class="docutils literal notranslate"><span class="pre">'ovo'</span></code> must be passed explicitly.</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'ovr'</span></code>:</dt><dd><p>Stands for One-vs-rest. Computes the AUC of each class
against the rest <a href="#id67"><span class="problematic" id="id44">[3]_</span></a> <a href="#id68"><span class="problematic" id="id45">[4]_</span></a>. This
treats the multiclass case in the same way as the multilabel case.
Sensitive to class imbalance even when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">==</span> <span class="pre">'macro'</span></code>,
because class imbalance affects the composition of each of the
‘rest’ groupings.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'ovo'</span></code>:</dt><dd><p>Stands for One-vs-one. Computes the average AUC of all
possible pairwise combinations of classes <a class="footnote-reference brackets" href="#id51" id="id46">5</a>.
Insensitive to class imbalance when
<code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">==</span> <span class="pre">'macro'</span></code>.</p>
</dd>
</dl>
</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Only used for multiclass targets. List of labels that index the
classes in <code class="docutils literal notranslate"><span class="pre">y_score</span></code>. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the numerical or lexicographical
order of the labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>auc</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id47"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia entry for the Receiver operating characteristic</a></p>
</dd>
<dt class="label" id="id48"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://www.ncbi.nlm.nih.gov/pubmed/2668680">Analyzing a portion of the ROC curve. McClish, 1989</a></p>
</dd>
<dt class="label" id="id49"><span class="brackets">3</span></dt>
<dd><p>Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
probability estimation trees (Section 6.2), CeDER Working Paper
#IS-00-04, Stern School of Business, New York University.</p>
</dd>
<dt class="label" id="id50"><span class="brackets">4</span></dt>
<dd><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S016786550500303X">Fawcett, T. (2006). An introduction to ROC analysis. Pattern
Recognition Letters, 27(8), 861-874.</a></p>
</dd>
<dt class="label" id="id51"><span class="brackets"><a class="fn-backref" href="#id46">5</a></span></dt>
<dd><p><a class="reference external" href="http://link.springer.com/article/10.1023/A:1010920819831">Hand, D.J., Till, R.J. (2001). A Simple Generalisation of the Area
Under the ROC Curve for Multiple Class Classification Problems.
Machine Learning, 45(2), 171-186.</a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.average_precision_score" title="sklearn.metrics.average_precision_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">average_precision_score</span></code></a></dt><dd><p>Area under the precision-recall curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_curve" title="sklearn.metrics.roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_curve</span></code></a></dt><dd><p>Compute Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.plot_roc_curve" title="sklearn.metrics.plot_roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_roc_curve</span></code></a></dt><dd><p>Plot Receiver operating characteristic (ROC) curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<p>Binary case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">0.99...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">0.99...</span>
</pre></div>
</div>
<p>Multiclass case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;ovr&#39;</span><span class="p">)</span>
<span class="go">0.99...</span>
</pre></div>
</div>
<p>Multilabel case:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_multilabel_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.multioutput</span> <span class="kn">import</span> <span class="n">MultiOutputClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_multilabel_classification</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MultiOutputClassifier</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># get a list of n_output containing probability arrays of shape</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># (n_samples, n_classes)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># extract the positive columns for each output</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">([</span><span class="n">pred</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">pred</span> <span class="ow">in</span> <span class="n">y_pred</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeClassifierCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RidgeClassifierCV</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.roc_curve">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">roc_curve</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pos_label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">drop_intermediate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.roc_curve" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute Receiver operating characteristic (ROC).</p>
<p>Note: this implementation is restricted to the binary classification task.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True binary labels. If labels are not either {-1, 1} or {0, 1}, then
pos_label should be explicitly given.</p></li>
<li><p><strong>y_score</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target scores, can either be probability estimates of the positive
class, confidence values, or non-thresholded measure of decisions
(as returned by “decision_function” on some classifiers).</p></li>
<li><p><strong>pos_label</strong> (<em>int</em><em> or </em><em>str</em><em>, </em><em>default=None</em>) – The label of the positive class.
When <code class="docutils literal notranslate"><span class="pre">pos_label=None</span></code>, if <cite>y_true</cite> is in {-1, 1} or {0, 1},
<code class="docutils literal notranslate"><span class="pre">pos_label</span></code> is set to 1, otherwise an error will be raised.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
<li><p><strong>drop_intermediate</strong> (<em>bool</em><em>, </em><em>default=True</em>) – <p>Whether to drop some suboptimal thresholds which would not appear
on a plotted ROC curve. This is useful in order to create lighter
ROC curves.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span>parameter <em>drop_intermediate</em>.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>fpr</strong> (<em>ndarray of shape (&gt;2,)</em>) – Increasing false positive rates such that element i is the false
positive rate of predictions with score &gt;= <cite>thresholds[i]</cite>.</p></li>
<li><p><strong>tpr</strong> (<em>ndarray of shape (&gt;2,)</em>) – Increasing true positive rates such that element <cite>i</cite> is the true
positive rate of predictions with score &gt;= <cite>thresholds[i]</cite>.</p></li>
<li><p><strong>thresholds</strong> (<em>ndarray of shape = (n_thresholds,)</em>) – Decreasing thresholds on the decision function used to compute
fpr and tpr. <cite>thresholds[0]</cite> represents no instances being predicted
and is arbitrarily set to <cite>max(y_score) + 1</cite>.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.metrics.plot_roc_curve" title="sklearn.metrics.plot_roc_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">plot_roc_curve</span></code></a></dt><dd><p>Plot Receiver operating characteristic (ROC) curve.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.RocCurveDisplay" title="sklearn.metrics.RocCurveDisplay"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RocCurveDisplay</span></code></a></dt><dd><p>ROC Curve visualization.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.det_curve" title="sklearn.metrics.det_curve"><code class="xref py py-obj docutils literal notranslate"><span class="pre">det_curve</span></code></a></dt><dd><p>Compute error rates for different probability thresholds.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.metrics.roc_auc_score" title="sklearn.metrics.roc_auc_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">roc_auc_score</span></code></a></dt><dd><p>Compute the area under the ROC curve.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>Since the thresholds are sorted from low to high values, they
are reversed upon returning them to ensure they correspond to both <code class="docutils literal notranslate"><span class="pre">fpr</span></code>
and <code class="docutils literal notranslate"><span class="pre">tpr</span></code>, which are sorted in reversed order during their calculation.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id52"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia entry for the Receiver operating characteristic</a></p>
</dd>
<dt class="label" id="id54"><span class="brackets">2</span></dt>
<dd><p>Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition
Letters, 2006, 27(8):861-874.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.35</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_curve</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">fpr</span>
<span class="go">array([0. , 0. , 0.5, 0.5, 1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tpr</span>
<span class="go">array([0. , 0.5, 0.5, 1. , 1. ])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">thresholds</span>
<span class="go">array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.silhouette_samples">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">silhouette_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.silhouette_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the Silhouette Coefficient for each sample.</p>
<p>The Silhouette Coefficient is a measure of how well samples are clustered
with samples that are similar to themselves. Clustering models with a high
Silhouette Coefficient are said to be dense, where samples in the same
cluster are similar to each other, and well separated, where samples in
different clusters are not very similar to each other.</p>
<p>The Silhouette Coefficient is calculated using the mean intra-cluster
distance (<code class="docutils literal notranslate"><span class="pre">a</span></code>) and the mean nearest-cluster distance (<code class="docutils literal notranslate"><span class="pre">b</span></code>) for each
sample.  The Silhouette Coefficient for a sample is <code class="docutils literal notranslate"><span class="pre">(b</span> <span class="pre">-</span> <span class="pre">a)</span> <span class="pre">/</span> <span class="pre">max(a,</span>
<span class="pre">b)</span></code>.
Note that Silhouette Coefficient is only defined if number of labels
is 2 <code class="docutils literal notranslate"><span class="pre">&lt;=</span> <span class="pre">n_labels</span> <span class="pre">&lt;=</span> <span class="pre">n_samples</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p>
<p>This function returns the Silhouette Coefficient for each sample.</p>
<p>The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_a</em><em>, </em><em>n_samples_a</em><em>) </em><em>if metric ==             &quot;precomputed&quot;</em><em> or </em><em>(</em><em>n_samples_a</em><em>, </em><em>n_features</em><em>) </em><em>otherwise</em>) – An array of pairwise distances between samples, or a feature array.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Label values for each sample.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by <a class="reference internal" href="#sklearn.metrics.pairwise.pairwise_distances" title="sklearn.metrics.pairwise.pairwise_distances"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise.pairwise_distances()</span></code></a>.
If <code class="docutils literal notranslate"><span class="pre">X</span></code> is the distance array itself, use “precomputed” as the metric.
Precomputed distance matrices must have 0 along the diagonal.</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a <code class="docutils literal notranslate"><span class="pre">scipy.spatial.distance</span></code> metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>silhouette</strong> – Silhouette Coefficients for each sample.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples,)</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id55"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/0377042787901257">Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”. Computational
and Applied Mathematics 20: 53-65.</a></p>
</dd>
<dt class="label" id="id56"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Wikipedia entry on the Silhouette Coefficient</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.silhouette_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">silhouette_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'euclidean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwds</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.silhouette_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the mean Silhouette Coefficient of all samples.</p>
<p>The Silhouette Coefficient is calculated using the mean intra-cluster
distance (<code class="docutils literal notranslate"><span class="pre">a</span></code>) and the mean nearest-cluster distance (<code class="docutils literal notranslate"><span class="pre">b</span></code>) for each
sample.  The Silhouette Coefficient for a sample is <code class="docutils literal notranslate"><span class="pre">(b</span> <span class="pre">-</span> <span class="pre">a)</span> <span class="pre">/</span> <span class="pre">max(a,</span>
<span class="pre">b)</span></code>.  To clarify, <code class="docutils literal notranslate"><span class="pre">b</span></code> is the distance between a sample and the nearest
cluster that the sample is not a part of.
Note that Silhouette Coefficient is only defined if number of labels
is <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">&lt;=</span> <span class="pre">n_labels</span> <span class="pre">&lt;=</span> <span class="pre">n_samples</span> <span class="pre">-</span> <span class="pre">1</span></code>.</p>
<p>This function returns the mean Silhouette Coefficient over all samples.
To obtain the values for each sample, use <a class="reference internal" href="#sklearn.metrics.silhouette_samples" title="sklearn.metrics.silhouette_samples"><code class="xref py py-func docutils literal notranslate"><span class="pre">silhouette_samples()</span></code></a>.</p>
<p>The best value is 1 and the worst value is -1. Values near 0 indicate
overlapping clusters. Negative values generally indicate that a sample has
been assigned to the wrong cluster, as a different cluster is more similar.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples_a</em><em>, </em><em>n_samples_a</em><em>) </em><em>if metric ==             &quot;precomputed&quot;</em><em> or </em><em>(</em><em>n_samples_a</em><em>, </em><em>n_features</em><em>) </em><em>otherwise</em>) – An array of pairwise distances between samples, or a feature array.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Predicted labels for each sample.</p></li>
<li><p><strong>metric</strong> (<em>str</em><em> or </em><em>callable</em><em>, </em><em>default='euclidean'</em>) – The metric to use when calculating distance between instances in a
feature array. If metric is a string, it must be one of the options
allowed by <a class="reference internal" href="#sklearn.metrics.pairwise.pairwise_distances" title="sklearn.metrics.pairwise.pairwise_distances"><code class="xref py py-func docutils literal notranslate"><span class="pre">metrics.pairwise.pairwise_distances</span></code></a>. If <code class="docutils literal notranslate"><span class="pre">X</span></code> is
the distance array itself, use <code class="docutils literal notranslate"><span class="pre">metric=&quot;precomputed&quot;</span></code>.</p></li>
<li><p><strong>sample_size</strong> (<em>int</em><em>, </em><em>default=None</em>) – The size of the sample to use when computing the Silhouette Coefficient
on a random subset of the data.
If <code class="docutils literal notranslate"><span class="pre">sample_size</span> <span class="pre">is</span> <span class="pre">None</span></code>, no sampling is used.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Determines random number generation for selecting a subset of samples.
Used when <code class="docutils literal notranslate"><span class="pre">sample_size</span> <span class="pre">is</span> <span class="pre">not</span> <span class="pre">None</span></code>.
Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>**kwds</strong> (<em>optional keyword parameters</em>) – Any further parameters are passed directly to the distance function.
If using a scipy.spatial.distance metric, the parameters are still
metric dependent. See the scipy docs for usage examples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>silhouette</strong> – Mean Silhouette Coefficient for all samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id57"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://www.sciencedirect.com/science/article/pii/0377042787901257">Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”. Computational
and Applied Mathematics 20: 53-65.</a></p>
</dd>
<dt class="label" id="id59"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">Wikipedia entry on the Silhouette Coefficient</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.top_k_accuracy_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">top_k_accuracy_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_score</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.top_k_accuracy_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Top-k Accuracy classification score.</p>
<p>This metric computes the number of times where the correct label is among
the top <cite>k</cite> labels predicted (ranked by predicted scores). Note that the
multilabel case isn’t covered here.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – True labels.</p></li>
<li><p><strong>y_score</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_classes</em><em>)</em>) – Target scores. These can be either probability estimates or
non-thresholded decision values (as returned by
<span class="xref std std-term">decision_function</span> on some classifiers). The binary case expects
scores with shape (n_samples,) while the multiclass case expects scores
with shape (n_samples, n_classes). In the nulticlass case, the order of
the class scores must correspond to the order of <code class="docutils literal notranslate"><span class="pre">labels</span></code>, if
provided, or else to the numerical or lexicographical order of the
labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code>.</p></li>
<li><p><strong>k</strong> (<em>int</em><em>, </em><em>default=2</em>) – Number of most likely outcomes considered to find the correct label.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <cite>True</cite>, return the fraction of correctly classified samples.
Otherwise, return the number of correctly classified samples.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If <cite>None</cite>, all samples are given the same weight.</p></li>
<li><p><strong>labels</strong> (<em>array-like of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Multiclass only. List of labels that index the classes in <code class="docutils literal notranslate"><span class="pre">y_score</span></code>.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the numerical or lexicographical order of the labels in
<code class="docutils literal notranslate"><span class="pre">y_true</span></code> is used.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The top-k accuracy score. The best performance is 1 with
<cite>normalize == True</cite> and the number of samples with
<cite>normalize == False</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_score</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>In cases where two or more labels are assigned equal predicted scores,
the labels with the highest indices will be chosen first. This might
impact the result if the correct label falls after the threshold because
of that.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">top_k_accuracy_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># 0 is in top 2</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span>  <span class="c1"># 1 is in top 2</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># 2 is in top 2</span>
<span class="gp">... </span>                    <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">]])</span> <span class="c1"># 2 isn&#39;t in top 2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_k_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="go">0.75</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Not normalizing gives the number of &quot;correctly&quot; classified samples</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">top_k_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_score</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">3</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.v_measure_score">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">v_measure_score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">labels_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">labels_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.v_measure_score" title="Permalink to this definition">¶</a></dt>
<dd><p>V-measure cluster labeling given a ground truth.</p>
<p>This score is identical to <a class="reference internal" href="#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-func docutils literal notranslate"><span class="pre">normalized_mutual_info_score()</span></code></a> with
the <code class="docutils literal notranslate"><span class="pre">'arithmetic'</span></code> option for averaging.</p>
<p>The V-measure is the harmonic mean between homogeneity and completeness:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">v</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">homogeneity</span> <span class="o">*</span> <span class="n">completeness</span>
     <span class="o">/</span> <span class="p">(</span><span class="n">beta</span> <span class="o">*</span> <span class="n">homogeneity</span> <span class="o">+</span> <span class="n">completeness</span><span class="p">)</span>
</pre></div>
</div>
<p>This metric is independent of the absolute values of the labels:
a permutation of the class or cluster label values won’t change the
score value in any way.</p>
<p>This metric is furthermore symmetric: switching <code class="docutils literal notranslate"><span class="pre">label_true</span></code> with
<code class="docutils literal notranslate"><span class="pre">label_pred</span></code> will return the same score value. This can be useful to
measure the agreement of two independent label assignments strategies
on the same dataset when the real ground truth is not known.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>labels_true</strong> (<em>int array</em><em>, </em><em>shape =</em><em> [</em><em>n_samples</em><em>]</em>) – ground truth class labels to be used as a reference</p></li>
<li><p><strong>labels_pred</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – cluster labels to evaluate</p></li>
<li><p><strong>beta</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – Ratio of weight attributed to <code class="docutils literal notranslate"><span class="pre">homogeneity</span></code> vs <code class="docutils literal notranslate"><span class="pre">completeness</span></code>.
If <code class="docutils literal notranslate"><span class="pre">beta</span></code> is greater than 1, <code class="docutils literal notranslate"><span class="pre">completeness</span></code> is weighted more
strongly in the calculation. If <code class="docutils literal notranslate"><span class="pre">beta</span></code> is less than 1,
<code class="docutils literal notranslate"><span class="pre">homogeneity</span></code> is weighted more strongly.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>v_measure</strong> – score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id61"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://aclweb.org/anthology/D/D07/D07-1043.pdf">Andrew Rosenberg and Julia Hirschberg, 2007. V-Measure: A
conditional entropy-based external cluster evaluation measure</a></p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.homogeneity_score" title="sklearn.metrics.homogeneity_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">homogeneity_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.completeness_score" title="sklearn.metrics.completeness_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">completeness_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.normalized_mutual_info_score" title="sklearn.metrics.normalized_mutual_info_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">normalized_mutual_info_score</span></code></a></p>
</div>
<p class="rubric">Examples</p>
<p>Perfect labelings are both homogeneous and complete, hence have score 1.0:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics.cluster</span> <span class="kn">import</span> <span class="n">v_measure_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="go">1.0</span>
</pre></div>
</div>
<p>Labelings that assign all classes members to the same clusters
are complete be not homogeneous, hence penalized:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">0.8...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
<span class="go">0.66...</span>
</pre></div>
</div>
<p>Labelings that have pure clusters with members coming from the same
classes are homogeneous but un-necessary splits harms completeness
and thus penalize V-measure as well:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]))</span>
<span class="go">0.8...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">0.66...</span>
</pre></div>
</div>
<p>If classes members are completely split across different clusters,
the assignment is totally incomplete, hence the V-Measure is null:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]))</span>
<span class="go">0.0...</span>
</pre></div>
</div>
<p>Clusters that include samples from totally different classes totally
destroy the homogeneity of the labeling, hence:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%.6f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">v_measure_score</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="go">0.0...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.metrics.zero_one_loss">
<span class="sig-prename descclassname"><span class="pre">sklearn.metrics.</span></span><span class="sig-name descname"><span class="pre">zero_one_loss</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">y_true</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y_pred</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.metrics.zero_one_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Zero-one classification loss.</p>
<p>If normalize is <code class="docutils literal notranslate"><span class="pre">True</span></code>, return the fraction of misclassifications
(float), else it returns the number of misclassifications (int). The best
performance is 0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) labels.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Predicted labels, as returned by a classifier.</p></li>
<li><p><strong>normalize</strong> (<em>bool</em><em>, </em><em>default=True</em>) – If <code class="docutils literal notranslate"><span class="pre">False</span></code>, return the number of misclassifications.
Otherwise, return the fraction of misclassifications.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>loss</strong> – If <code class="docutils literal notranslate"><span class="pre">normalize</span> <span class="pre">==</span> <span class="pre">True</span></code>, return the fraction of misclassifications
(float), else it returns the number of misclassifications (int).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or int,</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>In multilabel classification, the zero_one_loss function corresponds to
the subset zero-one loss: for each sample, the entire set of labels must be
correctly predicted, otherwise the loss for that sample is equal to one.</p>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">accuracy_score</span></code></a>, <a class="reference internal" href="#sklearn.metrics.hamming_loss" title="sklearn.metrics.hamming_loss"><code class="xref py py-obj docutils literal notranslate"><span class="pre">hamming_loss</span></code></a>, <a class="reference internal" href="#sklearn.metrics.jaccard_score" title="sklearn.metrics.jaccard_score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">jaccard_score</span></code></a></p>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">zero_one_loss</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.25</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">1</span>
</pre></div>
</div>
<p>In the multilabel case with binary label indicators:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">zero_one_loss</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)))</span>
<span class="go">0.5</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Tommaso Fioravanti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>