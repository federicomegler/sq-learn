

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.ensemble package &mdash; sqlearn  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> sqlearn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">sqlearn</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">sqlearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>sklearn.ensemble package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/sklearn.ensemble.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sklearn-ensemble-package">
<h1>sklearn.ensemble package<a class="headerlink" href="#sklearn-ensemble-package" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sklearn.ensemble.tests.html">sklearn.ensemble.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-bagging-module">sklearn.ensemble.tests.test_bagging module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-base-module">sklearn.ensemble.tests.test_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-common-module">sklearn.ensemble.tests.test_common module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-forest-module">sklearn.ensemble.tests.test_forest module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-gradient-boosting-module">sklearn.ensemble.tests.test_gradient_boosting module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-gradient-boosting-loss-functions-module">sklearn.ensemble.tests.test_gradient_boosting_loss_functions module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-iforest-module">sklearn.ensemble.tests.test_iforest module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-stacking-module">sklearn.ensemble.tests.test_stacking module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-voting-module">sklearn.ensemble.tests.test_voting module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#sklearn-ensemble-tests-test-weight-boosting-module">sklearn.ensemble.tests.test_weight_boosting module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.ensemble.tests.html#module-sklearn.ensemble.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-sklearn.ensemble.setup">
<span id="sklearn-ensemble-setup-module"></span><h2>sklearn.ensemble.setup module<a class="headerlink" href="#module-sklearn.ensemble.setup" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="sklearn.ensemble.setup.configuration">
<span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.setup.</span></span><span class="sig-name descname"><span class="pre">configuration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parent_package</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.setup.configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-sklearn.ensemble">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sklearn.ensemble" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> module includes ensemble-based methods for
classification, regression and anomaly detection.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">AdaBoostClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'SAMME.R'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._weight_boosting.BaseWeightBoosting</span></code></p>
<p>An AdaBoost classifier.</p>
<p>An AdaBoost [1] classifier is a meta-estimator that begins by fitting a
classifier on the original dataset and then fits additional copies of the
classifier on the same dataset but where the weights of incorrectly
classified instances are adjusted such that subsequent classifiers focus
more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost-SAMME [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.14.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator from which the boosted ensemble is built.
Support for sample weighting is required, as well as proper
<code class="docutils literal notranslate"><span class="pre">classes_</span></code> and <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code> attributes. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then
the base estimator is <a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a>
initialized with <cite>max_depth=1</cite>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=50</em>) – The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=1.</em>) – Learning rate shrinks the contribution of each classifier by
<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> and
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p></li>
<li><p><strong>algorithm</strong> (<em>{'SAMME'</em><em>, </em><em>'SAMME.R'}</em><em>, </em><em>default='SAMME.R'</em>) – If ‘SAMME.R’ then use the SAMME.R real boosting algorithm.
<code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> must support calculation of class probabilities.
If ‘SAMME’ then use the SAMME discrete boosting algorithm.
The SAMME.R algorithm typically converges faster than SAMME,
achieving a lower test error with fewer boosting iterations.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given at each <cite>base_estimator</cite> at each
boosting iteration.
Thus, it is only used when <cite>base_estimator</cite> exposes a <cite>random_state</cite>.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of classifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.estimator_weights_">
<span class="sig-name descname"><span class="pre">estimator_weights_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.estimator_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights for each estimator in the boosted ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.estimator_errors_">
<span class="sig-name descname"><span class="pre">estimator_errors_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.estimator_errors_" title="Permalink to this definition">¶</a></dt>
<dd><p>Classification error for each estimator in the boosted
ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances if supported by the
<code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> (when based on decision trees).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.ensemble.AdaBoostRegressor" title="sklearn.ensemble.AdaBoostRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdaBoostRegressor</span></code></a></dt><dd><p>An AdaBoost regressor that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a></dt><dd><p>GB builds an additive model in a forward stage-wise fashion. Regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.</p>
</dd>
<dt><a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeClassifier</span></code></a></dt><dd><p>A non-parametric supervised learning method used for classification. Creates a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</p>
</dd>
</dl>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p>Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.</p>
</dd>
<dt class="label" id="id2"><span class="brackets">2</span></dt>
<dd><ol class="upperalpha simple" start="10">
<li><p>Zhu, H. Zou, S. Rosset, T. Hastie, “Multi-class AdaBoost”, 2009.</p></li>
</ol>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">AdaBoostClassifier(n_estimators=100, random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.983...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.
Binary classification is a special cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>,
otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in <code class="docutils literal notranslate"><span class="pre">classes_</span></code>, respectively.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape of (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a boosted classifier from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The target values (class labels).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, the sample weights are initialized to
<code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">n_samples</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict classes for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the weighted mean predicted class log-probabilities of the classifiers
in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.staged_decision_function">
<span class="sig-name descname"><span class="pre">staged_decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.staged_decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code> for each boosting iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each boosting iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>score</strong> (<em>generator of ndarray of shape (n_samples, k)</em>) – The decision function of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.
Binary classification is a special cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>,
otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>. For binary classification,
values closer to -1 or 1 mean more like the first or second
class in <code class="docutils literal notranslate"><span class="pre">classes_</span></code>, respectively.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return staged predictions for X.</p>
<p>The predicted class of an input sample is computed as the weighted mean
prediction of the classifiers in the ensemble.</p>
<p>This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted classes.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostClassifier.staged_predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the weighted mean predicted class probabilities of the classifiers
in the ensemble.</p>
<p>This generator method yields the ensemble predicted class probabilities
after each iteration of boosting and therefore allows monitoring, such
as to determine the predicted class probabilities on a test set after
each boost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>p</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The class probabilities of the input samples. The order of
outputs is the same of that of the <span class="xref std std-term">classes_</span> attribute.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">AdaBoostRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">50</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'linear'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._weight_boosting.BaseWeightBoosting</span></code></p>
<p>An AdaBoost regressor.</p>
<p>An AdaBoost [1] regressor is a meta-estimator that begins by fitting a
regressor on the original dataset and then fits additional copies of the
regressor on the same dataset but where the weights of instances are
adjusted according to the error of the current prediction. As such,
subsequent regressors focus more on difficult cases.</p>
<p>This class implements the algorithm known as AdaBoost.R2 [2].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.14.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator from which the boosted ensemble is built.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then the base estimator is
<a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a> initialized with
<cite>max_depth=3</cite>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=50</em>) – The maximum number of estimators at which boosting is terminated.
In case of perfect fit, the learning procedure is stopped early.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=1.</em>) – Learning rate shrinks the contribution of each regressor by
<code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>. There is a trade-off between <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> and
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p></li>
<li><p><strong>loss</strong> (<em>{'linear'</em><em>, </em><em>'square'</em><em>, </em><em>'exponential'}</em><em>, </em><em>default='linear'</em>) – The loss function to use when updating the weights after each
boosting iteration.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given at each <cite>base_estimator</cite> at each
boosting iteration.
Thus, it is only used when <cite>base_estimator</cite> exposes a <cite>random_state</cite>.
In addition, it controls the bootstrap of the weights used to train the
<cite>base_estimator</cite> at each boosting iteration.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of classifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.estimator_weights_">
<span class="sig-name descname"><span class="pre">estimator_weights_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.estimator_weights_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weights for each estimator in the boosted ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.estimator_errors_">
<span class="sig-name descname"><span class="pre">estimator_errors_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.estimator_errors_" title="Permalink to this definition">¶</a></dt>
<dd><p>Regression error for each estimator in the boosted ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of floats</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances if supported by the
<code class="docutils literal notranslate"><span class="pre">base_estimator</span></code> (when based on decision trees).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">AdaBoostRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">AdaBoostRegressor(n_estimators=100, random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([4.7972...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.9771...</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><a class="reference internal" href="#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a>, <a class="reference internal" href="#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>, <a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code></a></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets">1</span></dt>
<dd><p>Y. Freund, R. Schapire, “A Decision-Theoretic Generalization of
on-Line Learning and an Application to Boosting”, 1995.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">2</span></dt>
<dd><ol class="upperalpha simple" start="8">
<li><p>Drucker, “Improving Regressors using Boosting Techniques”, 1997.</p></li>
</ol>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a boosted regressor from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The target values (real numbers).</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, the sample weights are initialized to
1 / n_samples.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression value for X.</p>
<p>The predicted regression value of an input sample is computed
as the weighted median prediction of the classifiers in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrix can be CSC, CSR, COO,
DOK, or LIL. COO, DOK, and LIL are converted to CSR.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted regression values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.AdaBoostRegressor.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.AdaBoostRegressor.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Return staged predictions for X.</p>
<p>The predicted regression value of an input sample is computed
as the weighted median prediction of the classifiers in the ensemble.</p>
<p>This generator method yields the ensemble prediction after each
iteration of boosting and therefore allows monitoring, such as to
determine the prediction on a test set after each boost.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted regression values.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">BaggingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>A Bagging classifier.</p>
<p>A Bagging classifier is an ensemble meta-estimator that fits base
classifiers each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id33"><span class="problematic" id="id5">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id34"><span class="problematic" id="id6">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id35"><span class="problematic" id="id7">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id36"><span class="problematic" id="id8">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
<a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=10</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of samples to draw from X to train each base estimator (with
replacement by default, see <cite>bootstrap</cite> for more details).</p>
<ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
</ul>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator (
without replacement by default, see <cite>bootstrap_features</cite> for more
details).</p>
<ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</p></li>
<li><p><strong>bootstrap_features</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether features are drawn with replacement.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate
the generalization error. Only available if bootstrap=True.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17: </span><em>warm_start</em> constructor parameter.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and
<a class="reference internal" href="#sklearn.ensemble.BaggingClassifier.predict" title="sklearn.ensemble.BaggingClassifier.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random resampling of the original dataset
(sample wise and feature wise).
If the base estimator accepts a <cite>random_state</cite> attribute, a different
seed is generated for each instance in the ensemble.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted base estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.estimators_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.estimators_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.oob_decision_function_" title="Permalink to this definition">¶</a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">SVC</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id9"><span class="brackets">1</span></dt>
<dd><p>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</p>
</dd>
<dt class="label" id="id11"><span class="brackets">3</span></dt>
<dd><p>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</p>
</dd>
<dt class="label" id="id12"><span class="brackets">4</span></dt>
<dd><p>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Average of the decision functions of the base classifiers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples. The columns correspond
to the classes in sorted order, as they appear in the attribute
<code class="docutils literal notranslate"><span class="pre">classes_</span></code>. Regression and binary classification are special
cases with <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class for X.</p>
<p>The predicted class of an input sample is computed as the class with
the highest mean predicted probability. If base estimators do not
implement a <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> method, then it resorts to voting.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<p>The predicted class log-probabilities of an input sample is computed as
the log of the mean predicted class probabilities of the base
estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X.</p>
<p>The predicted class probabilities of an input sample is computed as
the mean predicted class probabilities of the base estimators in the
ensemble. If base estimators do not implement a <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>
method, then it resorts to voting and the predicted class probabilities
of an input sample represents the proportion of estimators predicting
each class.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">BaggingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>A Bagging regressor.</p>
<p>A Bagging regressor is an ensemble meta-estimator that fits base
regressors each on random subsets of the original dataset and then
aggregate their individual predictions (either by voting or by averaging)
to form a final prediction. Such a meta-estimator can typically be used as
a way to reduce the variance of a black-box estimator (e.g., a decision
tree), by introducing randomization into its construction procedure and
then making an ensemble out of it.</p>
<p>This algorithm encompasses several works from the literature. When random
subsets of the dataset are drawn as random subsets of the samples, then
this algorithm is known as Pasting <a href="#id37"><span class="problematic" id="id13">[1]_</span></a>. If samples are drawn with
replacement, then the method is known as Bagging <a href="#id38"><span class="problematic" id="id14">[2]_</span></a>. When random subsets
of the dataset are drawn as random subsets of the features, then the method
is known as Random Subspaces <a href="#id39"><span class="problematic" id="id15">[3]_</span></a>. Finally, when base estimators are built
on subsets of both samples and features, then the method is known as
Random Patches <a href="#id40"><span class="problematic" id="id16">[4]_</span></a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.15.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em><em>, </em><em>default=None</em>) – The base estimator to fit on random subsets of the dataset.
If None, then the base estimator is a
<a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a>.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=10</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of samples to draw from X to train each base estimator (with
replacement by default, see <cite>bootstrap</cite> for more details).</p>
<ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
</ul>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator (
without replacement by default, see <cite>bootstrap_features</cite> for more
details).</p>
<ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether samples are drawn with replacement. If False, sampling
without replacement is performed.</p></li>
<li><p><strong>bootstrap_features</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether features are drawn with replacement.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate
the generalization error. Only available if bootstrap=True.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit
a whole new ensemble. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and
<a class="reference internal" href="#sklearn.ensemble.BaggingRegressor.predict" title="sklearn.ensemble.BaggingRegressor.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random resampling of the original dataset
(sample wise and feature wise).
If the base estimator accepts a <cite>random_state</cite> attribute, a different
seed is generated for each instance in the ensemble.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.estimators_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator. Each subset is defined by an array of the indices selected.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.estimators_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.oob_prediction_" title="Permalink to this definition">¶</a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_prediction_</cite> might contain NaN. This attribute exists only
when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_targets</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">SVR</span><span class="p">(),</span>
<span class="gp">... </span>                        <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([-2.8720...])</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id17"><span class="brackets">1</span></dt>
<dd><p>L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.</p>
</dd>
<dt class="label" id="id18"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140,
1996.</p>
</dd>
<dt class="label" id="id19"><span class="brackets">3</span></dt>
<dd><p>T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</p>
</dd>
<dt class="label" id="id20"><span class="brackets">4</span></dt>
<dd><p>G. Louppe and P. Geurts, “Ensembles on Random Patches”, Machine
Learning and Knowledge Discovery in Databases, 346-361, 2012.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.BaggingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaggingRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression target for X.</p>
<p>The predicted regression target of an input sample is computed as the
mean predicted regression targets of the estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Sparse matrices are accepted only if
they are supported by the base estimator.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.BaseEnsemble">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">BaseEnsemble</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_estimator</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">estimator_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">()</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.BaseEnsemble" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MetaEstimatorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Base class for all ensemble classes.</p>
<p>Warning: This class should not be used directly. Use derived classes
instead.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_estimator</strong> (<em>object</em>) – The base estimator from which the ensemble is built.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=10</em>) – The number of estimators in the ensemble.</p></li>
<li><p><strong>estimator_params</strong> (<em>list of str</em><em>, </em><em>default=tuple</em><em>(</em><em>)</em>) – The list of attributes to use as parameters when instantiating a
new base estimator. If none are given, default parameters are used.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaseEnsemble.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.BaseEnsemble.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The base estimator from which the ensemble is grown.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.BaseEnsemble.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.BaseEnsemble.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted base estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">ExtraTreesClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestClassifier</span></code></p>
<p>An extra-trees classifier.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls 3 sources of randomness:</p>
<ul>
<li><p>the bootstrapping of the samples used when building trees
(if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>)</p></li>
<li><p>the sampling of the features to consider when looking for the best
split at each node (if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>)</p></li>
<li><p>the draw of the splits for each of the <cite>max_features</cite></p></li>
</ul>
<p>See <span class="xref std std-term">Glossary</span> for details.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>class_weight</strong> (<em>{&quot;balanced&quot;</em><em>, </em><em>&quot;balanced_subsample&quot;}</em><em>, </em><em>dict</em><em> or </em><em>list of dicts</em><em>,             </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that
weights are computed based on the bootstrap sample for every tree
grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier">ExtraTreesClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or a list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesClassifier.oob_decision_function_" title="Permalink to this definition">¶</a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.tree.html#sklearn.tree.ExtraTreeClassifier" title="sklearn.tree.ExtraTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.ExtraTreeClassifier</span></code></a></dt><dd><p>Base classifier for this ensemble.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a></dt><dd><p>Ensemble Classifier based on trees with optimal splits.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id21"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">ExtraTreesClassifier(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">ExtraTreesRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestRegressor</span></code></p>
<p>An extra-trees regressor.</p>
<p>This class implements a meta estimator that fits a number of
randomized decision trees (a.k.a. extra-trees) on various sub-samples
of the dataset and uses averaging to improve the predictive accuracy
and control over-fitting.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;squared_error&quot;</em><em>, </em><em>&quot;mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>, </em><em>default=&quot;squared_error&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “squared_error” and “mse” for the mean squared error, which is
equal to variance reduction as feature selection criterion, and “mae”
for the mean absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion “mse” was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=”squared_error”</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls 3 sources of randomness:</p>
<ul>
<li><p>the bootstrapping of the samples used when building trees
(if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>)</p></li>
<li><p>the sampling of the features to consider when looking for the best
split at each node (if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>)</p></li>
<li><p>the draw of the splits for each of the <cite>max_features</cite></p></li>
</ul>
<p>See <span class="xref std std-term">Glossary</span> for details.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="sklearn.tree.html#sklearn.tree.ExtraTreeRegressor" title="sklearn.tree.ExtraTreeRegressor">ExtraTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeRegressor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.ExtraTreesRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#sklearn.ensemble.ExtraTreesRegressor.oob_prediction_" title="Permalink to this definition">¶</a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training set.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,) or (n_samples, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.tree.html#sklearn.tree.ExtraTreeRegressor" title="sklearn.tree.ExtraTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.ExtraTreeRegressor</span></code></a></dt><dd><p>Base estimator for this ensemble.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.ensemble.RandomForestRegressor" title="sklearn.ensemble.RandomForestRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code></a></dt><dd><p>Ensemble regressor using trees with optimal splits.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id22"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">ExtraTreesRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.2708...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">GradientBoostingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deviance'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'friedman_mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._gb.BaseGradientBoosting</span></code></p>
<p>Gradient Boosting for classification.</p>
<p>GB builds an additive model in a
forward stage-wise fashion; it allows for the optimization of
arbitrary differentiable loss functions. In each stage <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code>
regression trees are fit on the negative gradient of the
binomial or multinomial deviance loss function. Binary classification
is a special case where only a single regression tree is induced.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'deviance'</em><em>, </em><em>'exponential'}</em><em>, </em><em>default='deviance'</em>) – The loss function to be optimized. ‘deviance’ refers to
deviance (= logistic regression) for classification
with probabilistic outputs. For loss ‘exponential’ gradient
boosting recovers the AdaBoost algorithm.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</p></li>
<li><p><strong>subsample</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</p></li>
<li><p><strong>criterion</strong> (<em>{'friedman_mse'</em><em>, </em><em>'squared_error'</em><em>, </em><em>'mse'</em><em>, </em><em>'mae'}</em><em>,             </em><em>default='friedman_mse'</em>) – <p>The function to measure the quality of a split. Supported criteria
are ‘friedman_mse’ for the mean squared error with improvement
score by Friedman, ‘squared_error’ for mean squared error, and ‘mae’
for the mean absolute error. The default value of ‘friedman_mse’ is
generally the best as it can provide a better approximation in some
cases.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><cite>criterion=’mae’</cite> is deprecated and will be removed in version
1.1 (renaming of 0.26). Use <cite>criterion=’friedman_mse’</cite> or
<cite>‘squared_error’</cite> instead, as trees should use a squared error
criterion in Gradient Boosting.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion ‘mse’ was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=3</em>) – The maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>init</strong> (<em>estimator</em><em> or </em><em>'zero'</em><em>, </em><em>default=None</em>) – An estimator object that is used to compute the initial predictions.
<code class="docutils literal notranslate"><span class="pre">init</span></code> has to provide <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code> and <a class="reference internal" href="#sklearn.ensemble.GradientBoostingClassifier.predict_proba" title="sklearn.ensemble.GradientBoostingClassifier.predict_proba"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_proba()</span></code></a>. If
‘zero’, the initial raw predictions are set to zero. By default, a
<code class="docutils literal notranslate"><span class="pre">DummyEstimator</span></code> predicting the classes priors is used.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random spliting of the training data to obtain a
validation set if <cite>n_iter_no_change</cite> is not None.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_features</strong> (<em>{'auto'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log2'}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If ‘auto’, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If ‘sqrt’, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If ‘log2’, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> numbers of
iterations. The split is stratified.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – <p>Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.n_estimators_">
<span class="sig-name descname"><span class="pre">n_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.n_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of estimators as selected by early stopping (if
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is specified). Otherwise it is set to
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.oob_improvement_">
<span class="sig-name descname"><span class="pre">oob_improvement_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.oob_improvement_" title="Permalink to this definition">¶</a></dt>
<dd><p>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal notranslate"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal notranslate"><span class="pre">init</span></code> estimator.
Only available if <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">&lt;</span> <span class="pre">1.0</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.train_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The i-th score <code class="docutils literal notranslate"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>LossFunction</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.init_">
<span class="sig-name descname"><span class="pre">init_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.init_" title="Permalink to this definition">¶</a></dt>
<dd><p>The estimator that provides the initial predictions.
Set via the <code class="docutils literal notranslate"><span class="pre">init</span></code> argument or <code class="docutils literal notranslate"><span class="pre">loss.init_estimator</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators. <code class="docutils literal notranslate"><span class="pre">loss_.K</span></code> is 1 for binary
classification, otherwise n_classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of DecisionTreeRegressor of             shape (n_estimators, <code class="docutils literal notranslate"><span class="pre">loss_.K</span></code>)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of data features.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a></dt><dd><p>Histogram-based Gradient Boosting Classification Tree.</p>
</dd>
<dt><a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeClassifier</span></code></a></dt><dd><p>A decision tree classifier.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a></dt><dd><p>A meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.</p>
</dd>
<dt><a class="reference internal" href="#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a></dt><dd><p>A meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">References</p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li><p>Friedman, Stochastic Gradient Boosting, 1999</p></li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<p class="rubric">Examples</p>
<p>The following example shows how to fit a gradient boosting classifier with
100 decision stumps as weak learners.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.913...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
order of the classes corresponds to that in the attribute
<span class="xref std std-term">classes_</span>. Regression and binary classification produce an
array of shape (n_samples,).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class log-probabilities for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the <code class="docutils literal notranslate"><span class="pre">loss</span></code> does not support probabilities.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>p</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Raises</dt>
<dd class="field-even"><p><strong>AttributeError</strong> – If the <code class="docutils literal notranslate"><span class="pre">loss</span></code> does not support probabilities.</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>p</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.staged_decision_function">
<span class="sig-name descname"><span class="pre">staged_decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.staged_decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code> for each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>score</strong> – The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.
Regression and binary classification are special cases with
<code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">==</span> <span class="pre">1</span></code>, otherwise <code class="docutils literal notranslate"><span class="pre">k==n_classes</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples, k)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingClassifier.staged_predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">GradientBoostingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">subsample</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'friedman_mse'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._gb.BaseGradientBoosting</span></code></p>
<p>Gradient Boosting for regression.</p>
<p>GB builds an additive model in a forward stage-wise fashion;
it allows for the optimization of arbitrary differentiable loss functions.
In each stage a regression tree is fit on the negative gradient of the
given loss function.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'squared_error'</em><em>, </em><em>'ls'</em><em>, </em><em>'lad'</em><em>, </em><em>'huber'</em><em>, </em><em>'quantile'}</em><em>,             </em><em>default='squared_error'</em>) – <p>Loss function to be optimized. ‘squared_error’ refers to the squared
error for regression.
‘lad’ (least absolute deviation) is a highly robust
loss function solely based on order information of the input
variables. ‘huber’ is a combination of the two. ‘quantile’
allows quantile regression (use <cite>alpha</cite> to specify the quantile).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The loss ‘ls’ was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>loss=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – Learning rate shrinks the contribution of each tree by <cite>learning_rate</cite>.
There is a trade-off between learning_rate and n_estimators.</p></li>
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of boosting stages to perform. Gradient boosting
is fairly robust to over-fitting so a large number usually
results in better performance.</p></li>
<li><p><strong>subsample</strong> (<em>float</em><em>, </em><em>default=1.0</em>) – The fraction of samples to be used for fitting the individual base
learners. If smaller than 1.0 this results in Stochastic Gradient
Boosting. <cite>subsample</cite> interacts with the parameter <cite>n_estimators</cite>.
Choosing <cite>subsample &lt; 1.0</cite> leads to a reduction of variance
and an increase in bias.</p></li>
<li><p><strong>criterion</strong> (<em>{'friedman_mse'</em><em>, </em><em>'squared_error'</em><em>, </em><em>'mse'</em><em>, </em><em>'mae'}</em><em>,             </em><em>default='friedman_mse'</em>) – <p>The function to measure the quality of a split. Supported criteria
are “friedman_mse” for the mean squared error with improvement
score by Friedman, “squared_error” for mean squared error, and “mae”
for the mean absolute error. The default value of “friedman_mse” is
generally the best as it can provide a better approximation in some
cases.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span><cite>criterion=’mae’</cite> is deprecated and will be removed in version
1.1 (renaming of 0.26). The correct way of minimizing the absolute
error is to use <cite>loss=’lad’</cite> instead.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion ‘mse’ was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=3</em>) – Maximum depth of the individual regression estimators. The maximum
depth limits the number of nodes in the tree. Tune this parameter
for best performance; the best value depends on the interaction
of the input variables.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>init</strong> (<em>estimator</em><em> or </em><em>'zero'</em><em>, </em><em>default=None</em>) – An estimator object that is used to compute the initial predictions.
<code class="docutils literal notranslate"><span class="pre">init</span></code> has to provide <span class="xref std std-term">fit</span> and <span class="xref std std-term">predict</span>. If ‘zero’, the
initial raw predictions are set to zero. By default a
<code class="docutils literal notranslate"><span class="pre">DummyEstimator</span></code> is used, predicting either the average target value
(for loss=’squared_error’), or a quantile for the other losses.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the random seed given to each Tree estimator at each
boosting iteration.
In addition, it controls the random permutation of the features at
each split (see Notes for more details).
It also controls the random spliting of the training data to obtain a
validation set if <cite>n_iter_no_change</cite> is not None.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>max_features</strong> (<em>{'auto'</em><em>, </em><em>'sqrt'</em><em>, </em><em>'log2'}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Choosing <cite>max_features &lt; n_features</cite> leads to a reduction of variance
and an increase in bias.</p>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – The alpha-quantile of the huber loss function and the quantile
loss function. Only if <code class="docutils literal notranslate"><span class="pre">loss='huber'</span></code> or <code class="docutils literal notranslate"><span class="pre">loss='quantile'</span></code>.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Enable verbose output. If 1 then it prints progress and performance
once in a while (the more trees the lower the frequency). If greater
than 1 then it prints progress and performance for every tree.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – <p>The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is set to an integer.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p><code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is used to decide if early stopping will be used
to terminate training when validation score is not improving. By
default it is set to None to disable early stopping. If set to a
number, it will set aside <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> size of the training
data as validation and terminate training when validation score is not
improving in all of the previous <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> numbers of
iterations.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – <p>Tolerance for the early stopping. When the loss is not improving
by at least tol for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> iterations (if set to a
number), the training stops.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.oob_improvement_">
<span class="sig-name descname"><span class="pre">oob_improvement_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.oob_improvement_" title="Permalink to this definition">¶</a></dt>
<dd><p>The improvement in loss (= deviance) on the out-of-bag samples
relative to the previous iteration.
<code class="docutils literal notranslate"><span class="pre">oob_improvement_[0]</span></code> is the improvement in
loss of the first stage over the <code class="docutils literal notranslate"><span class="pre">init</span></code> estimator.
Only available if <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">&lt;</span> <span class="pre">1.0</span></code></p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.train_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The i-th score <code class="docutils literal notranslate"><span class="pre">train_score_[i]</span></code> is the deviance (= loss) of the
model at iteration <code class="docutils literal notranslate"><span class="pre">i</span></code> on the in-bag sample.
If <code class="docutils literal notranslate"><span class="pre">subsample</span> <span class="pre">==</span> <span class="pre">1</span></code> this is the deviance on the training data.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_estimators,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The concrete <code class="docutils literal notranslate"><span class="pre">LossFunction</span></code> object.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>LossFunction</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.init_">
<span class="sig-name descname"><span class="pre">init_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.init_" title="Permalink to this definition">¶</a></dt>
<dd><p>The estimator that provides the initial predictions.
Set via the <code class="docutils literal notranslate"><span class="pre">init</span></code> argument or <code class="docutils literal notranslate"><span class="pre">loss.init_estimator</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of DecisionTreeRegressor of shape (n_estimators, 1)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes, set to 1 for regressors.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24: </span>Attribute <code class="docutils literal notranslate"><span class="pre">n_classes_</span></code> was deprecated in version 0.24 and
will be removed in 1.1 (renaming of 0.26).</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.n_estimators_">
<span class="sig-name descname"><span class="pre">n_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.n_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of estimators as selected by early stopping (if
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> is specified). Otherwise it is set to
<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of data features.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a></dt><dd><p>Histogram-based Gradient Boosting Classification Tree.</p>
</dd>
<dt><a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.DecisionTreeRegressor</span></code></a></dt><dd><p>A decision tree regressor.</p>
</dd>
<dt><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.tree.RandomForestRegressor</span></code></dt><dd><p>A random forest regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data and
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>, if the improvement of the criterion is
identical for several splits enumerated during the search of the best
split. To obtain a deterministic behaviour during fitting,
<code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">GradientBoostingRegressor(random_state=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">])</span>
<span class="go">array([-61...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.4...</span>
</pre></div>
</div>
<p class="rubric">References</p>
<p>J. Friedman, Greedy Function Approximation: A Gradient Boosting
Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.</p>
<ol class="upperalpha simple" start="10">
<li><p>Friedman, Stochastic Gradient Boosting, 1999</p></li>
</ol>
<p>T. Hastie, R. Tibshirani and J. Friedman.
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply trees in the ensemble to X, return leaf indices.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, its dtype will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code>. If a sparse matrix is provided, it will
be converted to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X and for each tree in the ensemble,
return the index of the leaf x ends up in each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_estimators)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="id0">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression target for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.GradientBoostingRegressor.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.GradientBoostingRegressor.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression target at each stage for X.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted value of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>generator of ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">HistGradientBoostingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_regularization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">255</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">categorical_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">monotonic_cst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'loss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting</span></code></p>
<p>Histogram-based Gradient Boosting Classification Tree.</p>
<p>This estimator is much faster than
<a class="reference internal" href="#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a>
for big datasets (n_samples &gt;= 10 000).</p>
<p>This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.</p>
<p>This implementation is inspired by
<a class="reference external" href="https://github.com/Microsoft/LightGBM">LightGBM</a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'auto'</em><em>, </em><em>'binary_crossentropy'</em><em>, </em><em>'categorical_crossentropy'}</em><em>,             </em><em>default='auto'</em>) – The loss function to use in the boosting process. ‘binary_crossentropy’
(also known as logistic loss) is used for binary classification and
generalizes to ‘categorical_crossentropy’ for multiclass
classification. ‘auto’ will automatically choose either loss depending
on the nature of the problem.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The learning rate, also known as <em>shrinkage</em>. This is used as a
multiplicative factor for the leaves values. Use <code class="docutils literal notranslate"><span class="pre">1</span></code> for no
shrinkage.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – The maximum number of iterations of the boosting process, i.e. the
maximum number of trees for binary classification. For multiclass
classification, <cite>n_classes</cite> trees per iteration are built.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=31</em>) – The maximum number of leaves for each tree. Must be strictly greater
than 1. If None, there is no maximum limit.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – The maximum depth of each tree. The depth of a tree is the number of
edges to go from the root to the deepest leaf.
Depth isn’t constrained by default.</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em>, </em><em>default=20</em>) – The minimum number of samples per leaf. For small datasets with less
than a few hundred samples, it is recommended to lower this value
since only very shallow trees would be built.</p></li>
<li><p><strong>l2_regularization</strong> (<em>float</em><em>, </em><em>default=0</em>) – The L2 regularization parameter. Use 0 for no regularization.</p></li>
<li><p><strong>max_bins</strong> (<em>int</em><em>, </em><em>default=255</em>) – The maximum number of bins to use for non-missing values. Before
training, each feature of the input array <cite>X</cite> is binned into
integer-valued bins, which allows for a much faster training stage.
Features with a small number of unique values may use less than
<code class="docutils literal notranslate"><span class="pre">max_bins</span></code> bins. In addition to the <code class="docutils literal notranslate"><span class="pre">max_bins</span></code> bins, one more bin
is always reserved for missing values. Must be no larger than 255.</p></li>
<li><p><strong>categorical_features</strong> (<em>array-like of {bool</em><em>, </em><em>int} of shape</em><em> (</em><em>n_features</em><em>)             or </em><em>shape</em><em> (</em><em>n_categorical_features</em><em>,</em><em>)</em><em>, </em><em>default=None.</em>) – <p>Indicates the categorical features.</p>
<ul>
<li><p>None : no feature will be considered categorical.</p></li>
<li><p>boolean array-like : boolean mask indicating categorical features.</p></li>
<li><p>integer array-like : integer indices indicating categorical
features.</p></li>
</ul>
<p>For each categorical feature, there must be at most <cite>max_bins</cite> unique
categories, and each categorical value must be in [0, max_bins -1].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
<li><p><strong>monotonic_cst</strong> (<em>array-like of int of shape</em><em> (</em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – <p>Indicates the monotonic constraint to enforce on each feature. -1, 1
and 0 respectively correspond to a negative constraint, positive
constraint and no constraint. Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble. For results to be valid, the
estimator should be re-trained on the same data only.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>early_stopping</strong> (<em>'auto'</em><em> or </em><em>bool</em><em>, </em><em>default='auto'</em>) – <p>If ‘auto’, early stopping is enabled if the sample size is larger than
10000. If True, early stopping is enabled, otherwise early stopping is
disabled.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>scoring</strong> (<em>str</em><em> or </em><em>callable</em><em> or </em><em>None</em><em>, </em><em>default='loss'</em>) – Scoring parameter to use for early stopping. It can be a single
string (see <span class="xref std std-ref">scoring_parameter</span>) or a callable (see
<span class="xref std std-ref">scoring</span>). If None, the estimator’s default scorer
is used. If <code class="docutils literal notranslate"><span class="pre">scoring='loss'</span></code>, early stopping is checked
w.r.t the loss value. Only used if early stopping is performed.</p></li>
<li><p><strong>validation_fraction</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>None</em><em>, </em><em>default=0.1</em>) – Proportion (or absolute size) of training data to set aside as
validation data for early stopping. If None, early stopping is done on
the training data. Only used if early stopping is performed.</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=10</em>) – Used to determine when to “early stop”. The fitting process is
stopped when none of the last <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> scores are better
than the <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span> <span class="pre">-</span> <span class="pre">1</span></code> -th-to-last one, up to some
tolerance. Only used if early stopping is performed.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-7</em>) – The absolute tolerance to use when comparing scores. The higher the
tolerance, the more likely we are to early stop: higher tolerance
means that it will be harder for subsequent iterations to be
considered an improvement upon the reference score.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level. If not zero, print some information about the
fitting process.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Pseudo-random number generator to control the subsampling in the
binning process, and the train/validation data split if early stopping
is enabled.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array, shape = (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.do_early_stopping_">
<span class="sig-name descname"><span class="pre">do_early_stopping_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.do_early_stopping_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indicates whether early stopping is used during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations as selected by early stopping, depending on
the <cite>early_stopping</cite> parameter. Otherwise it corresponds to max_iter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.n_trees_per_iteration_">
<span class="sig-name descname"><span class="pre">n_trees_per_iteration_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.n_trees_per_iteration_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of tree that are built at each iteration. This is equal to 1
for binary classification, and to <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> for multiclass
classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.train_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The scores at each iteration on the training data. The first entry
is the score of the ensemble before the first iteration. Scores are
computed according to the <code class="docutils literal notranslate"><span class="pre">scoring</span></code> parameter. If <code class="docutils literal notranslate"><span class="pre">scoring</span></code> is
not ‘loss’, scores are computed on a subset of at most 10 000
samples. Empty if no early stopping.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_iter_+1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.validation_score_">
<span class="sig-name descname"><span class="pre">validation_score_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.validation_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The scores at each iteration on the held-out validation data. The
first entry is the score of the ensemble before the first iteration.
Scores are computed according to the <code class="docutils literal notranslate"><span class="pre">scoring</span></code> parameter. Empty if
no early stopping or if <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> is None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_iter_+1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.is_categorical_">
<span class="sig-name descname"><span class="pre">is_categorical_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.is_categorical_" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean mask for the categorical features. <code class="docutils literal notranslate"><span class="pre">None</span></code> if there are no
categorical features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_features, ) or None</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">1.0</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>decision</strong> – The raw predicted values (i.e. the sum of the trees leaves) for
each sample. n_trees_per_iteration is equal to the number of
classes in multiclass classification.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray, shape (n_samples,) or                 (n_samples, n_trees_per_iteration)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict classes for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>p</strong> – The class probabilities of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray, shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.staged_decision_function">
<span class="sig-name descname"><span class="pre">staged_decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.staged_decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute decision function of <code class="docutils literal notranslate"><span class="pre">X</span></code> for each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>decision</strong> (<em>generator of ndarray of shape (n_samples,) or                 (n_samples, n_trees_per_iteration)</em>) – The decision function of the input samples, which corresponds to
the raw values predicted from the trees of the ensemble . The
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict classes at each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted classes of the input samples, for each iteration.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingClassifier.staged_predict_proba">
<span class="sig-name descname"><span class="pre">staged_predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingClassifier.staged_predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities at each iteration.</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted class probabilities of the input samples,
for each iteration.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">HistGradientBoostingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">31</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">l2_regularization</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_bins</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">255</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">categorical_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">monotonic_cst</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scoring</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'loss'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._hist_gradient_boosting.gradient_boosting.BaseHistGradientBoosting</span></code></p>
<p>Histogram-based Gradient Boosting Regression Tree.</p>
<p>This estimator is much faster than
<a class="reference internal" href="#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>
for big datasets (n_samples &gt;= 10 000).</p>
<p>This estimator has native support for missing values (NaNs). During
training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are
assigned to the left or right child consequently. If no missing values
were encountered for a given feature during training, then samples with
missing values are mapped to whichever child has the most samples.</p>
<p>This implementation is inspired by
<a class="reference external" href="https://github.com/Microsoft/LightGBM">LightGBM</a>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong> (<em>{'squared_error'</em><em>, </em><em>'least_squares'</em><em>, </em><em>'least_absolute_deviation'</em><em>,             </em><em>'poisson'}</em><em>, </em><em>default='squared_error'</em>) – <p>The loss function to use in the boosting process. Note that the
“least squares” and “poisson” losses actually implement
“half least squares loss” and “half poisson deviance” to simplify the
computation of the gradient. Furthermore, “poisson” loss internally
uses a log-link and requires <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">&gt;=</span> <span class="pre">0</span></code></p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.23: </span>Added option ‘poisson’.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>The loss ‘least_squares’ was deprecated in v1.0 and will be removed
in version 1.2. Use <cite>loss=’squared_error’</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The learning rate, also known as <em>shrinkage</em>. This is used as a
multiplicative factor for the leaves values. Use <code class="docutils literal notranslate"><span class="pre">1</span></code> for no
shrinkage.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=100</em>) – The maximum number of iterations of the boosting process, i.e. the
maximum number of trees.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=31</em>) – The maximum number of leaves for each tree. Must be strictly greater
than 1. If None, there is no maximum limit.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – The maximum depth of each tree. The depth of a tree is the number of
edges to go from the root to the deepest leaf.
Depth isn’t constrained by default.</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em>, </em><em>default=20</em>) – The minimum number of samples per leaf. For small datasets with less
than a few hundred samples, it is recommended to lower this value
since only very shallow trees would be built.</p></li>
<li><p><strong>l2_regularization</strong> (<em>float</em><em>, </em><em>default=0</em>) – The L2 regularization parameter. Use <code class="docutils literal notranslate"><span class="pre">0</span></code> for no regularization
(default).</p></li>
<li><p><strong>max_bins</strong> (<em>int</em><em>, </em><em>default=255</em>) – The maximum number of bins to use for non-missing values. Before
training, each feature of the input array <cite>X</cite> is binned into
integer-valued bins, which allows for a much faster training stage.
Features with a small number of unique values may use less than
<code class="docutils literal notranslate"><span class="pre">max_bins</span></code> bins. In addition to the <code class="docutils literal notranslate"><span class="pre">max_bins</span></code> bins, one more bin
is always reserved for missing values. Must be no larger than 255.</p></li>
<li><p><strong>categorical_features</strong> (<em>array-like of {bool</em><em>, </em><em>int} of shape</em><em> (</em><em>n_features</em><em>)             or </em><em>shape</em><em> (</em><em>n_categorical_features</em><em>,</em><em>)</em><em>, </em><em>default=None.</em>) – <p>Indicates the categorical features.</p>
<ul>
<li><p>None : no feature will be considered categorical.</p></li>
<li><p>boolean array-like : boolean mask indicating categorical features.</p></li>
<li><p>integer array-like : integer indices indicating categorical
features.</p></li>
</ul>
<p>For each categorical feature, there must be at most <cite>max_bins</cite> unique
categories, and each categorical value must be in [0, max_bins -1].</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
</p></li>
<li><p><strong>monotonic_cst</strong> (<em>array-like of int of shape</em><em> (</em><em>n_features</em><em>)</em><em>, </em><em>default=None</em>) – <p>Indicates the monotonic constraint to enforce on each feature. -1, 1
and 0 respectively correspond to a negative constraint, positive
constraint and no constraint. Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble. For results to be valid, the
estimator should be re-trained on the same data only.
See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>early_stopping</strong> (<em>'auto'</em><em> or </em><em>bool</em><em>, </em><em>default='auto'</em>) – <p>If ‘auto’, early stopping is enabled if the sample size is larger than
10000. If True, early stopping is enabled, otherwise early stopping is
disabled.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
<li><p><strong>scoring</strong> (<em>str</em><em> or </em><em>callable</em><em> or </em><em>None</em><em>, </em><em>default='loss'</em>) – Scoring parameter to use for early stopping. It can be a single
string (see <span class="xref std std-ref">scoring_parameter</span>) or a callable (see
<span class="xref std std-ref">scoring</span>). If None, the estimator’s default scorer is used. If
<code class="docutils literal notranslate"><span class="pre">scoring='loss'</span></code>, early stopping is checked w.r.t the loss value.
Only used if early stopping is performed.</p></li>
<li><p><strong>validation_fraction</strong> (<em>int</em><em> or </em><em>float</em><em> or </em><em>None</em><em>, </em><em>default=0.1</em>) – Proportion (or absolute size) of training data to set aside as
validation data for early stopping. If None, early stopping is done on
the training data. Only used if early stopping is performed.</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=10</em>) – Used to determine when to “early stop”. The fitting process is
stopped when none of the last <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> scores are better
than the <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span> <span class="pre">-</span> <span class="pre">1</span></code> -th-to-last one, up to some
tolerance. Only used if early stopping is performed.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-7</em>) – The absolute tolerance to use when comparing scores during early
stopping. The higher the tolerance, the more likely we are to early
stop: higher tolerance means that it will be harder for subsequent
iterations to be considered an improvement upon the reference score.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level. If not zero, print some information about the
fitting process.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Pseudo-random number generator to control the subsampling in the
binning process, and the train/validation data split if early stopping
is enabled.
Pass an int for reproducible output across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.do_early_stopping_">
<span class="sig-name descname"><span class="pre">do_early_stopping_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.do_early_stopping_" title="Permalink to this definition">¶</a></dt>
<dd><p>Indicates whether early stopping is used during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations as selected by early stopping, depending on
the <cite>early_stopping</cite> parameter. Otherwise it corresponds to max_iter.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.n_trees_per_iteration_">
<span class="sig-name descname"><span class="pre">n_trees_per_iteration_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.n_trees_per_iteration_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of tree that are built at each iteration. For regressors,
this is always 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.train_score_">
<span class="sig-name descname"><span class="pre">train_score_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.train_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The scores at each iteration on the training data. The first entry
is the score of the ensemble before the first iteration. Scores are
computed according to the <code class="docutils literal notranslate"><span class="pre">scoring</span></code> parameter. If <code class="docutils literal notranslate"><span class="pre">scoring</span></code> is
not ‘loss’, scores are computed on a subset of at most 10 000
samples. Empty if no early stopping.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_iter_+1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.validation_score_">
<span class="sig-name descname"><span class="pre">validation_score_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.validation_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>The scores at each iteration on the held-out validation data. The
first entry is the score of the ensemble before the first iteration.
Scores are computed according to the <code class="docutils literal notranslate"><span class="pre">scoring</span></code> parameter. Empty if
no early stopping or if <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code> is None.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_iter_+1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.is_categorical_">
<span class="sig-name descname"><span class="pre">is_categorical_</span></span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.is_categorical_" title="Permalink to this definition">¶</a></dt>
<dd><p>Boolean mask for the categorical features. <code class="docutils literal notranslate"><span class="pre">None</span></code> if there are no
categorical features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray, shape (n_features, ) or None</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">0.92...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict values for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like</em><em>, </em><em>shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray, shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.HistGradientBoostingRegressor.staged_predict">
<span class="sig-name descname"><span class="pre">staged_predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.HistGradientBoostingRegressor.staged_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression target for each iteration</p>
<p>This method allows monitoring (i.e. determine error on testing set)
after each stage.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Yields</dt>
<dd class="field-even"><p><strong>y</strong> (<em>generator of ndarray of shape (n_samples,)</em>) – The predicted values of the input samples, for each iteration.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">IsolationForest</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">contamination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.IsolationForest" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.OutlierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._bagging.BaseBagging</span></code></p>
<p>Isolation Forest Algorithm.</p>
<p>Return the anomaly score of each sample using the IsolationForest algorithm</p>
<p>The IsolationForest ‘isolates’ observations by randomly selecting a feature
and then randomly selecting a split value between the maximum and minimum
values of the selected feature.</p>
<p>Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.</p>
<p>This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.</p>
<p>Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – The number of base estimators in the ensemble.</p></li>
<li><p><strong>max_samples</strong> (<em>&quot;auto&quot;</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <dl class="simple">
<dt>The number of samples to draw from X to train each base estimator.</dt><dd><ul>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples.</p></li>
<li><p>If “auto”, then <cite>max_samples=min(256, n_samples)</cite>.</p></li>
</ul>
</dd>
</dl>
<p>If max_samples is larger than the number of samples provided,
all samples will be used for all trees (no sampling).</p>
</p></li>
<li><p><strong>contamination</strong> (<em>'auto'</em><em> or </em><em>float</em><em>, </em><em>default='auto'</em>) – <p>The amount of contamination of the data set, i.e. the proportion
of outliers in the data set. Used when fitting to define the threshold
on the scores of the samples.</p>
<blockquote>
<div><ul>
<li><p>If ‘auto’, the threshold is determined as in the
original paper.</p></li>
<li><p>If float, the contamination should be in the range (0, 0.5].</p></li>
</ul>
</div></blockquote>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">contamination</span></code> changed from 0.1
to <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>.</p>
</div>
</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1.0</em>) – <p>The number of features to draw from X to train each base estimator.</p>
<blockquote>
<div><ul>
<li><p>If int, then draw <cite>max_features</cite> features.</p></li>
<li><p>If float, then draw <cite>max_features * X.shape[1]</cite> features.</p></li>
</ul>
</div></blockquote>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If True, individual trees are fit on random subsets of the training
data sampled with replacement. If False, sampling without replacement
is performed.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for both <a class="reference internal" href="#sklearn.ensemble.IsolationForest.fit" title="sklearn.ensemble.IsolationForest.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a> and
<a class="reference internal" href="#sklearn.ensemble.IsolationForest.predict" title="sklearn.ensemble.IsolationForest.predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></a>. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a
<code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all
processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Controls the pseudo-randomness of the selection of the feature
and split values for each branching step and each tree in the forest.</p>
<p>Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p>
</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity of the tree building process.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of
fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ExtraTreeRegressor instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ExtraTreeRegressor instances</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.estimators_features_">
<span class="sig-name descname"><span class="pre">estimators_features_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.estimators_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn features for each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.estimators_samples_">
<span class="sig-name descname"><span class="pre">estimators_samples_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.estimators_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The subset of drawn samples (i.e., the in-bag samples) for each base
estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.max_samples_">
<span class="sig-name descname"><span class="pre">max_samples_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.max_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>The actual number of samples.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.offset_">
<span class="sig-name descname"><span class="pre">offset_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.offset_" title="Permalink to this definition">¶</a></dt>
<dd><p>Offset used to define the decision function from the raw scores. We
have the relation: <code class="docutils literal notranslate"><span class="pre">decision_function</span> <span class="pre">=</span> <span class="pre">score_samples</span> <span class="pre">-</span> <span class="pre">offset_</span></code>.
<code class="docutils literal notranslate"><span class="pre">offset_</span></code> is defined as follows. When the contamination parameter is
set to “auto”, the offset is equal to -0.5 as the scores of inliers are
close to 0 and the scores of outliers are close to -1. When a
contamination parameter different than “auto” is provided, the offset
is defined in such a way we obtain the expected number of outliers
(samples with decision function &lt; 0) in training.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>The implementation is based on an ensemble of ExtraTreeRegressor. The
maximum depth of each tree is set to <code class="docutils literal notranslate"><span class="pre">ceil(log_2(n))</span></code> where
<span class="math notranslate nohighlight">\(n\)</span> is the number of samples used to build the tree
(see (Liu et al., 2008) for more details).</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id24"><span class="brackets">1</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.</p>
</dd>
<dt class="label" id="id25"><span class="brackets">2</span></dt>
<dd><p>Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation-based
anomaly detection.” ACM Transactions on Knowledge Discovery from
Data (TKDD) 6.1 (2012): 3.</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.covariance.html#sklearn.covariance.EllipticEnvelope" title="sklearn.covariance.EllipticEnvelope"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.covariance.EllipticEnvelope</span></code></a></dt><dd><p>An object for detecting outliers in a Gaussian distributed dataset.</p>
</dd>
<dt><a class="reference internal" href="sklearn.svm.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.OneClassSVM</span></code></a></dt><dd><p>Unsupervised Outlier Detection. Estimate the support of a high-dimensional distribution. The implementation is based on libsvm.</p>
</dd>
<dt><a class="reference internal" href="sklearn.neighbors.html#sklearn.neighbors.LocalOutlierFactor" title="sklearn.neighbors.LocalOutlierFactor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.neighbors.LocalOutlierFactor</span></code></a></dt><dd><p>Unsupervised Outlier Detection using Local Outlier Factor (LOF).</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">IsolationForest</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">1.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mi">100</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">IsolationForest</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">90</span><span class="p">]])</span>
<span class="go">array([ 1,  1, -1])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Average anomaly score of X of the base classifiers.</p>
<p>The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.</p>
<p>The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scores</strong> – The anomaly score of the input samples.
The lower, the more abnormal. Negative scores represent outliers,
positive scores represent inliers.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> for maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict if a particular sample is an outlier or not.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>is_inlier</strong> – For each observation, tells whether or not (+1 or -1) it should
be considered as an inlier according to the fitted model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.IsolationForest.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.IsolationForest.score_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Opposite of the anomaly score defined in the original paper.</p>
<p>The anomaly score of an input sample is computed as
the mean anomaly score of the trees in the forest.</p>
<p>The measure of normality of an observation given a tree is the depth
of the leaf containing this observation, which is equivalent to
the number of splittings required to isolate this point. In case of
several observations n_left in the leaf, the average path length of
a n_left samples isolation tree is added.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>scores</strong> – The anomaly score of the input samples.
The lower, the more abnormal.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">RandomForestClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestClassifier</span></code></p>
<p>A random forest classifier.</p>
<p>A random forest is a meta estimator that fits a number of decision tree
classifiers on various sub-samples of the dataset and uses averaging to
improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <cite>max_samples</cite> parameter if
<cite>bootstrap=True</cite> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.
Note: this parameter is tree-specific.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite> (same as “auto”).</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls both the randomness of the bootstrapping of the samples used
when building trees (if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>) and the sampling of the
features to consider when looking for the best split at each node
(if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>).
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>class_weight</strong> (<em>{&quot;balanced&quot;</em><em>, </em><em>&quot;balanced_subsample&quot;}</em><em>, </em><em>dict</em><em> or </em><em>list of dicts</em><em>,             </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If not given, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>The “balanced_subsample” mode is the same as “balanced” except that
weights are computed based on the bootstrap sample for every tree
grown.</p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier">DecisionTreeClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels (single output problem), or a list of arrays of
class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or a list of such arrays</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes (single output problem), or a list containing the
number of classes for each output (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestClassifier.oob_decision_function_">
<span class="sig-name descname"><span class="pre">oob_decision_function_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestClassifier.oob_decision_function_" title="Permalink to this definition">¶</a></dt>
<dd><p>Decision function computed with out-of-bag estimate on the training
set. If n_estimators is small it might be possible that a data point
was never left out during the bootstrap. In this case,
<cite>oob_decision_function_</cite> might contain NaN. This attribute exists
only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code>, <a class="reference internal" href="#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id26"><span class="brackets">1</span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
<span class="gp">... </span>                           <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">RandomForestRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bootstrap</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">oob_score</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_samples</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.ForestRegressor</span></code></p>
<p>A random forest regressor.</p>
<p>A random forest is a meta estimator that fits a number of classifying
decision trees on various sub-samples of the dataset and uses averaging
to improve the predictive accuracy and control over-fitting.
The sub-sample size is controlled with the <cite>max_samples</cite> parameter if
<cite>bootstrap=True</cite> (default), otherwise the whole dataset is used to build
each tree.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>The number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>criterion</strong> (<em>{&quot;squared_error&quot;</em><em>, </em><em>&quot;mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>, </em><em>default=&quot;squared_error&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “squared_error” for the mean squared error, which is equal to
variance reduction as feature selection criterion, and “mae” for the
mean absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion “mse” was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=”squared_error”</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>int</em><em> or </em><em>float</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>round(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>bootstrap</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether bootstrap samples are used when building trees. If False, the
whole dataset is used to build each tree.</p></li>
<li><p><strong>oob_score</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use out-of-bag samples to estimate the generalization score.
Only available if bootstrap=True.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls both the randomness of the bootstrapping of the samples used
when building trees (if <code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>) and the sampling of the
features to consider when looking for the best split at each node
(if <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>).
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
<li><p><strong>max_samples</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=None</em>) – <p>If bootstrap is True, the number of samples to draw from X
to train each base estimator.</p>
<ul>
<li><p>If None (default), then draw <cite>X.shape[0]</cite> samples.</p></li>
<li><p>If int, then draw <cite>max_samples</cite> samples.</p></li>
<li><p>If float, then draw <cite>max_samples * X.shape[0]</cite> samples. Thus,
<cite>max_samples</cite> should be in the interval <cite>(0, 1)</cite>.</p></li>
</ul>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><a class="reference internal" href="sklearn.tree.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor">DecisionTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeRegressor</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.oob_score_">
<span class="sig-name descname"><span class="pre">oob_score_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.oob_score_" title="Permalink to this definition">¶</a></dt>
<dd><p>Score of the training dataset obtained using an out-of-bag estimate.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomForestRegressor.oob_prediction_">
<span class="sig-name descname"><span class="pre">oob_prediction_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomForestRegressor.oob_prediction_" title="Permalink to this definition">¶</a></dt>
<dd><p>Prediction computed with out-of-bag estimate on the training set.
This attribute exists only when <code class="docutils literal notranslate"><span class="pre">oob_score</span></code> is True.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,) or (n_samples, n_outputs)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code>, <a class="reference internal" href="#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code></a></p>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The features are always randomly permuted at each split. Therefore,
the best found split may vary, even with the same training data,
<code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>, if the improvement
of the criterion is identical for several splits enumerated during the
search of the best split. To obtain a deterministic behaviour during
fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed.</p>
<p>The default value <code class="docutils literal notranslate"><span class="pre">max_features=&quot;auto&quot;</span></code> uses <code class="docutils literal notranslate"><span class="pre">n_features</span></code>
rather than <code class="docutils literal notranslate"><span class="pre">n_features</span> <span class="pre">/</span> <span class="pre">3</span></code>. The latter was originally suggested in
[1], whereas the former was more recently justified empirically in [2].</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id27"><span class="brackets">1</span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
<dt class="label" id="id28"><span class="brackets">2</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_features</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                       <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">RandomForestRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
<span class="go">[-8.32987858]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">RandomTreesEmbedding</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_estimators</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sparse_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._forest.BaseForest</span></code></p>
<p>An ensemble of totally random trees.</p>
<p>An unsupervised transformation of a dataset to a high-dimensional
sparse representation. A datapoint is coded according to which leaf of
each tree it is sorted into. Using a one-hot encoding of the leaves,
this leads to a binary coding with as many ones as there are trees in
the forest.</p>
<p>The dimensionality of the resulting representation is
<code class="docutils literal notranslate"><span class="pre">n_out</span> <span class="pre">&lt;=</span> <span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">max_leaf_nodes</span></code>. If <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">None</span></code>,
the number of leaf nodes is at most <code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></code>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_estimators</strong> (<em>int</em><em>, </em><em>default=100</em>) – <p>Number of trees in the forest.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.22: </span>The default value of <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> changed from 10 to 100
in 0.22.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=5</em>) – The maximum depth of each tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> is the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> is the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow trees with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>sparse_output</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether or not to return a sparse CSR matrix, as default behavior,
or to return a dense array compatible with dense pipeline operators.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel. <a class="reference internal" href="#sklearn.ensemble.RandomTreesEmbedding.fit" title="sklearn.ensemble.RandomTreesEmbedding.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></a>, <a class="reference internal" href="#sklearn.ensemble.RandomTreesEmbedding.transform" title="sklearn.ensemble.RandomTreesEmbedding.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transform()</span></code></a>,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">decision_path()</span></code> and <code class="xref py py-meth docutils literal notranslate"><span class="pre">apply()</span></code> are all parallelized over the
trees. <code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code>
context. <code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span> for more details.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the generation of the random <cite>y</cite> used to fit the trees
and the draw of the splits for each feature at the trees’ nodes.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Controls the verbosity when fitting and predicting.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
and add more estimators to the ensemble, otherwise, just fit a whole
new forest. See <span class="xref std std-term">the Glossary</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.base_estimator_">
<span class="sig-name descname"><span class="pre">base_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.base_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The child estimator template used to create the collection of fitted
sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>DecisionTreeClassifier instance</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of DecisionTreeClassifier instances</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The feature importances (the higher, the more important the feature).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Attribute <cite>n_features_</cite> was deprecated in version 1.0 and will be
removed in 1.2. Use <cite>n_features_in_</cite> instead.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.one_hot_encoder_">
<span class="sig-name descname"><span class="pre">one_hot_encoder_</span></span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.one_hot_encoder_" title="Permalink to this definition">¶</a></dt>
<dd><p>One-hot encoder used to create the sparse embedding.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>OneHotEncoder instance</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id29"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
<dt class="label" id="id30"><span class="brackets">2</span></dt>
<dd><p>Moosmann, F. and Triggs, B. and Jurie, F.  “Fast discriminative
visual codebooks using randomized clustering forests”
NIPS 2007</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomTreesEmbedding</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">random_trees</span> <span class="o">=</span> <span class="n">RandomTreesEmbedding</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">n_estimators</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_sparse_embedding</span> <span class="o">=</span> <span class="n">random_trees</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_sparse_embedding</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],</span>
<span class="go">       [1., 0., 1., 0., 1., 0., 1., 0., 1., 0.],</span>
<span class="go">       [0., 1., 1., 0., 1., 0., 0., 1., 1., 0.]])</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.criterion">
<span class="sig-name descname"><span class="pre">criterion</span></span><em class="property"> <span class="pre">=</span> <span class="pre">'squared_error'</span></em><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> for maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.fit_transform">
<span class="sig-name descname"><span class="pre">fit_transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit estimator and transform dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data used to build forests. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for
maximum efficiency.</p></li>
<li><p><strong>y</strong> (<em>Ignored</em>) – Not used, present for API consistency by convention.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. In the case of
classification, splits are also ignored if they would result in any
single class carrying a negative weight in either child node.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_transformed</strong> – Transformed dataset.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_out)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.max_features">
<span class="sig-name descname"><span class="pre">max_features</span></span><em class="property"> <span class="pre">=</span> <span class="pre">1</span></em><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.max_features" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.RandomTreesEmbedding.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.RandomTreesEmbedding.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Input data to be transformed. Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> for maximum
efficiency. Sparse matrices are also supported, use sparse
<code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code> for maximum efficiency.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_transformed</strong> – Transformed dataset.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_out)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">StackingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stack_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">passthrough</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._stacking._BaseStacking</span></code></p>
<p>Stack of estimators with a final classifier.</p>
<p>Stacked generalization consists in stacking the output of individual
estimator and use a classifier to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.</p>
<p>Note that <cite>estimators_</cite> are fitted on the full <cite>X</cite> while <cite>final_estimator_</cite>
is trained using cross-validated predictions of the base estimators using
<cite>cross_val_predict</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>)</em>) – Base estimators which will be stacked together. Each element of the
list is defined as a tuple of string (i.e. name) and an estimator
instance. An estimator can be set to ‘drop’ using <cite>set_params</cite>.</p></li>
<li><p><strong>final_estimator</strong> (<em>estimator</em><em>, </em><em>default=None</em>) – A classifier which will be used to combine the base estimators.
The default classifier is a
<a class="reference internal" href="sklearn.linear_model.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-class docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy used in
<cite>cross_val_predict</cite> to train <cite>final_estimator</cite>. Possible inputs for
cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross validation,</p></li>
<li><p>integer, to specify the number of folds in a (Stratified) KFold,</p></li>
<li><p>An object to be used as a cross-validation generator,</p></li>
<li><p>An iterable yielding train, test splits.</p></li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and y is
either binary or multiclass,
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code></a> is used.
In all other cases, <a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code></a> is used.
These splitters are instantiated with <cite>shuffle=False</cite> so the splits
will be the same across calls.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A larger number of split will provide no benefits if the number
of training samples is large enough. Indeed, the training time
will increase. <code class="docutils literal notranslate"><span class="pre">cv</span></code> is not used for model evaluation but for
prediction.</p>
</div>
</p></li>
<li><p><strong>stack_method</strong> (<em>{'auto'</em><em>, </em><em>'predict_proba'</em><em>, </em><em>'decision_function'</em><em>, </em><em>'predict'}</em><em>,             </em><em>default='auto'</em>) – <p>Methods called for each base estimator. It can be:</p>
<ul>
<li><p>if ‘auto’, it will try to invoke, for each estimator,
<cite>‘predict_proba’</cite>, <cite>‘decision_function’</cite> or <cite>‘predict’</cite> in that
order.</p></li>
<li><p>otherwise, one of <cite>‘predict_proba’</cite>, <cite>‘decision_function’</cite> or
<cite>‘predict’</cite>. If the method is not implemented by the estimator, it
will raise an error.</p></li>
</ul>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel all <cite>estimators</cite> <cite>fit</cite>.
<cite>None</cite> means 1 unless in a <cite>joblib.parallel_backend</cite> context. -1 means
using all processors. See Glossary for more details.</p></li>
<li><p><strong>passthrough</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When False, only the predictions of estimators will be used as
training data for <cite>final_estimator</cite>. When True, the
<cite>final_estimator</cite> is trained on the predictions as well as the
original training data.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Verbosity level.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The elements of the estimators parameter, having been fitted on the
training data. If an estimator has been set to <cite>‘drop’</cite>, it
will not appear in <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimators</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.named_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.final_estimator_">
<span class="sig-name descname"><span class="pre">final_estimator_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.final_estimator_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classifier which predicts given the output of <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.stack_method_">
<span class="sig-name descname"><span class="pre">stack_method_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.stack_method_" title="Permalink to this definition">¶</a></dt>
<dd><p>The method used by each base estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of str</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Notes</p>
<p>When <cite>predict_proba</cite> is used by each estimator (i.e. most of the time for
<cite>stack_method=’auto’</cite> or specifically for <cite>stack_method=’predict_proba’</cite>),
The first column predicted by each estimator will be dropped in the case
of a binary classification problem. Indeed, both feature will be perfectly
collinear.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id31"><span class="brackets">1</span></dt>
<dd><p>Wolpert, David H. “Stacked generalization.” Neural networks 5.2
(1992): 241-259.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                          <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)))</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span> <span class="n">final_estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">()</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.9...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict decision function for samples in X using
<cite>final_estimator_.decision_function</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>decisions</strong> – The decision function computed the final estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,), (n_samples, n_classes),             or (n_samples, n_classes * (n_classes-1) / 2)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">predict_params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict target for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>**predict_params</strong> (<em>dict of str -&gt; obj</em>) – Parameters to the <cite>predict</cite> called by the <cite>final_estimator</cite>. Note
that this may be used to return uncertainties from some estimators
with <cite>return_std</cite> or <cite>return_cov</cite>. Be aware that it will only
accounts for uncertainty in the final estimator.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_pred</strong> – Predicted targets.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,) or (n_samples, n_output)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities for X using
<cite>final_estimator_.predict_proba</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>probabilities</strong> – The class probabilities of the input samples.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or             list of ndarray of shape (n_output,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="pre">:</span> <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingClassifier.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Return class labels or probabilities for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_preds</strong> – Prediction outputs for each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_estimators) or                 (n_samples, n_classes * n_estimators)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">StackingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_estimator</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">passthrough</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._stacking._BaseStacking</span></code></p>
<p>Stack of estimators with a final regressor.</p>
<p>Stacked generalization consists in stacking the output of individual
estimator and use a regressor to compute the final prediction. Stacking
allows to use the strength of each individual estimator by using their
output as input of a final estimator.</p>
<p>Note that <cite>estimators_</cite> are fitted on the full <cite>X</cite> while <cite>final_estimator_</cite>
is trained using cross-validated predictions of the base estimators using
<cite>cross_val_predict</cite>.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>)</em>) – Base estimators which will be stacked together. Each element of the
list is defined as a tuple of string (i.e. name) and an estimator
instance. An estimator can be set to ‘drop’ using <cite>set_params</cite>.</p></li>
<li><p><strong>final_estimator</strong> (<em>estimator</em><em>, </em><em>default=None</em>) – A regressor which will be used to combine the base estimators.
The default regressor is a <a class="reference internal" href="sklearn.linear_model.html#sklearn.linear_model.RidgeCV" title="sklearn.linear_model.RidgeCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">RidgeCV</span></code></a>.</p></li>
<li><p><strong>cv</strong> (<em>int</em><em>, </em><em>cross-validation generator</em><em> or </em><em>an iterable</em><em>, </em><em>default=None</em>) – <p>Determines the cross-validation splitting strategy used in
<cite>cross_val_predict</cite> to train <cite>final_estimator</cite>. Possible inputs for
cv are:</p>
<ul>
<li><p>None, to use the default 5-fold cross validation,</p></li>
<li><p>integer, to specify the number of folds in a (Stratified) KFold,</p></li>
<li><p>An object to be used as a cross-validation generator,</p></li>
<li><p>An iterable yielding train, test splits.</p></li>
</ul>
<p>For integer/None inputs, if the estimator is a classifier and y is
either binary or multiclass,
<a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">StratifiedKFold</span></code></a> is used.
In all other cases, <a class="reference internal" href="sklearn.model_selection.html#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code class="xref py py-class docutils literal notranslate"><span class="pre">KFold</span></code></a> is used.
These splitters are instantiated with <cite>shuffle=False</cite> so the splits
will be the same across calls.</p>
<p>Refer <span class="xref std std-ref">User Guide</span> for the various
cross-validation strategies that can be used here.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A larger number of split will provide no benefits if the number
of training samples is large enough. Indeed, the training time
will increase. <code class="docutils literal notranslate"><span class="pre">cv</span></code> is not used for model evaluation but for
prediction.</p>
</div>
</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for <cite>fit</cite> of all <cite>estimators</cite>.
<cite>None</cite> means 1 unless in a <cite>joblib.parallel_backend</cite> context. -1 means
using all processors. See Glossary for more details.</p></li>
<li><p><strong>passthrough</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When False, only the predictions of estimators will be used as
training data for <cite>final_estimator</cite>. When True, the
<cite>final_estimator</cite> is trained on the predictions as well as the
original training data.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – Verbosity level.</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The elements of the estimators parameter, having been fitted on the
training data. If an estimator has been set to <cite>‘drop’</cite>, it
will not appear in <cite>estimators_</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of estimator</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.StackingRegressor.named_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="simple">
<dt><a href="#id41"><span class="problematic" id="id42">final_estimator_</span></a><span class="classifier">estimator</span></dt><dd><p>The regressor to stacked the base estimators fitted.</p>
</dd>
</dl>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id32"><span class="brackets">1</span></dt>
<dd><p>Wolpert, David H. “Stacked generalization.” Neural networks 5.2
(1992): 241-259.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">LinearSVR</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
<span class="gp">... </span>    <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">LinearSVR</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
<span class="gp">... </span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="gp">... </span>                                          <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.3...</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="pre">:</span> <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sklearn.ensemble.StackingRegressor.steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.StackingRegressor.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.StackingRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the predictions for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where <cite>n_samples</cite> is the number of samples and
<cite>n_features</cite> is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_preds</strong> – Prediction outputs for each estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_estimators)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">VotingClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">voting</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'hard'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">flatten_transform</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._voting._BaseVoting</span></code></p>
<p>Soft Voting/Majority Rule classifier for unfitted estimators.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>) </em><em>tuples</em>) – <p>Invoking the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">VotingClassifier</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<code class="docutils literal notranslate"><span class="pre">self.estimators_</span></code>. An estimator can be set to <code class="docutils literal notranslate"><span class="pre">'drop'</span></code>
using <code class="docutils literal notranslate"><span class="pre">set_params</span></code>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.21: </span><code class="docutils literal notranslate"><span class="pre">'drop'</span></code> is accepted. Using None was deprecated in 0.22 and
support was removed in 0.24.</p>
</div>
</p></li>
<li><p><strong>voting</strong> (<em>{'hard'</em><em>, </em><em>'soft'}</em><em>, </em><em>default='hard'</em>) – If ‘hard’, uses predicted class labels for majority rule voting.
Else if ‘soft’, predicts the class label based on the argmax of
the sums of the predicted probabilities, which is recommended for
an ensemble of well-calibrated classifiers.</p></li>
<li><p><strong>weights</strong> (<em>array-like of shape</em><em> (</em><em>n_classifiers</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurrences of
predicted class labels (<cite>hard</cite> voting) or class probabilities
before averaging (<cite>soft</cite> voting). Uses uniform weights if <cite>None</cite>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – <p>The number of jobs to run in parallel for <code class="docutils literal notranslate"><span class="pre">fit</span></code>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
<li><p><strong>flatten_transform</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Affects shape of transform output only when voting=’soft’
If voting=’soft’ and flatten_transform=True, transform method returns
matrix with shape (n_samples, n_classifiers * n_classes). If
flatten_transform=False, it returns
(n_classifiers, n_samples, n_classes).</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If True, the time elapsed while fitting will be printed as it
is completed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators as defined in <code class="docutils literal notranslate"><span class="pre">estimators</span></code>
that are not ‘drop’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of classifiers</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.named_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_predictions,)</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.ensemble.VotingRegressor" title="sklearn.ensemble.VotingRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VotingRegressor</span></code></a></dt><dd><p>Prediction voting regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">VotingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span> <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf1</span> <span class="o">=</span> <span class="n">eclf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">np</span><span class="o">.</span><span class="n">array_equal</span><span class="p">(</span><span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="o">.</span><span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
<span class="gp">... </span>               <span class="n">eclf1</span><span class="o">.</span><span class="n">named_estimators_</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">True</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>        <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf2</span> <span class="o">=</span> <span class="n">eclf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[</span>
<span class="gp">... </span>       <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>       <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span>
<span class="gp">... </span>       <span class="n">flatten_transform</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf3</span> <span class="o">=</span> <span class="n">eclf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[1 1 1 2 2 2]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">eclf3</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="go">(6, 6)</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – <p>Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class labels for X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>maj</strong> – Predicted class labels.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.predict_proba">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">predict_proba</span></span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute probabilities of possible outcomes for samples in X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>avg</strong> – Weighted average probability for each class per sample.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="pre">:</span> <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingClassifier.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingClassifier.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Return class labels or probabilities for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>If <cite>voting=’soft’</cite> and <cite>flatten_transform=True</cite>:</dt><dd><p>returns ndarray of shape (n_classifiers, n_samples *
n_classes), being class probabilities calculated by each
classifier.</p>
</dd>
<dt>If <cite>voting=’soft’ and `flatten_transform=False</cite>:</dt><dd><p>ndarray of shape (n_classifiers, n_samples, n_classes)</p>
</dd>
<dt>If <cite>voting=’hard’</cite>:</dt><dd><p>ndarray of shape (n_samples, n_classifiers), being
class labels predicted by each classifier.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>probabilities_or_labels</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.ensemble.</span></span><span class="sig-name descname"><span class="pre">VotingRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">estimators</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_jobs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble._voting._BaseVoting</span></code></p>
<p>Prediction voting regressor for unfitted estimators.</p>
<p>A voting regressor is an ensemble meta-estimator that fits several base
regressors, each on the whole dataset. Then it averages the individual
predictions to form a final prediction.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>estimators</strong> (<em>list of</em><em> (</em><em>str</em><em>, </em><em>estimator</em><em>) </em><em>tuples</em>) – <p>Invoking the <code class="docutils literal notranslate"><span class="pre">fit</span></code> method on the <code class="docutils literal notranslate"><span class="pre">VotingRegressor</span></code> will fit clones
of those original estimators that will be stored in the class attribute
<code class="docutils literal notranslate"><span class="pre">self.estimators_</span></code>. An estimator can be set to <code class="docutils literal notranslate"><span class="pre">'drop'</span></code> using
<code class="docutils literal notranslate"><span class="pre">set_params</span></code>.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.21: </span><code class="docutils literal notranslate"><span class="pre">'drop'</span></code> is accepted. Using None was deprecated in 0.22 and
support was removed in 0.24.</p>
</div>
</p></li>
<li><p><strong>weights</strong> (<em>array-like of shape</em><em> (</em><em>n_regressors</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sequence of weights (<cite>float</cite> or <cite>int</cite>) to weight the occurrences of
predicted values before averaging. Uses uniform weights if <cite>None</cite>.</p></li>
<li><p><strong>n_jobs</strong> (<em>int</em><em>, </em><em>default=None</em>) – The number of jobs to run in parallel for <code class="docutils literal notranslate"><span class="pre">fit</span></code>.
<code class="docutils literal notranslate"><span class="pre">None</span></code> means 1 unless in a <code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code> context.
<code class="docutils literal notranslate"><span class="pre">-1</span></code> means using all processors. See <span class="xref std std-term">Glossary</span>
for more details.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>If True, the time elapsed while fitting will be printed as it
is completed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.23.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.estimators_">
<span class="sig-name descname"><span class="pre">estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>The collection of fitted sub-estimators as defined in <code class="docutils literal notranslate"><span class="pre">estimators</span></code>
that are not ‘drop’.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of regressors</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.named_estimators_">
<span class="sig-name descname"><span class="pre">named_estimators_</span></span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.named_estimators_" title="Permalink to this definition">¶</a></dt>
<dd><p>Attribute to access any fitted sub-estimators by name.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Bunch</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a></dt><dd><p>Soft Voting/Majority Rule classifier.</p>
</dd>
</dl>
</div>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r1</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">9</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">16</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">36</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">42</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">er</span> <span class="o">=</span> <span class="n">VotingRegressor</span><span class="p">([(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">r1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">r2</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">er</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="go">[ 3.3  5.7 11.8 19.7 28.  40.3]</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the estimators.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training vectors, where n_samples is the number of samples and
n_features is the number of features.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – Target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted.
Note that this is supported only if all underlying estimators
support sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict regression target for X.</p>
<p>The predicted regression target of an input sample is computed as the
mean predicted regression targets of the estimators in the ensemble.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.steps">
<span class="sig-name descname"><span class="pre">steps</span></span><em class="property"><span class="pre">:</span> <span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.ensemble.VotingRegressor.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.ensemble.VotingRegressor.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Return predictions for X for each estimator.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>predictions</strong> – Values predicted by each regressor.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classifiers)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Tommaso Fioravanti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>