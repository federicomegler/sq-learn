

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.neural_network package &mdash; sqlearn  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> sqlearn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">sqlearn</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">sqlearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>sklearn.neural_network package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/sklearn.neural_network.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sklearn-neural-network-package">
<h1>sklearn.neural_network package<a class="headerlink" href="#sklearn-neural-network-package" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sklearn.neural_network.tests.html">sklearn.neural_network.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#sklearn-neural-network-tests-test-base-module">sklearn.neural_network.tests.test_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#sklearn-neural-network-tests-test-mlp-module">sklearn.neural_network.tests.test_mlp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#sklearn-neural-network-tests-test-rbm-module">sklearn.neural_network.tests.test_rbm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#module-sklearn.neural_network.tests.test_stochastic_optimizers">sklearn.neural_network.tests.test_stochastic_optimizers module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.neural_network.tests.html#module-sklearn.neural_network.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="module-sklearn.neural_network">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sklearn.neural_network" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#module-sklearn.neural_network" title="sklearn.neural_network"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neural_network</span></code></a> module includes models based on neural
networks.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.neural_network.</span></span><span class="sig-name descname"><span class="pre">BernoulliRBM</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">n_components</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">256</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.TransformerMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Bernoulli Restricted Boltzmann Machine (RBM).</p>
<p>A Restricted Boltzmann Machine with binary visible units and
binary hidden units. Parameters are estimated using Stochastic Maximum
Likelihood (SML), also known as Persistent Contrastive Divergence (PCD)
[2].</p>
<p>The time complexity of this implementation is <code class="docutils literal notranslate"><span class="pre">O(d</span> <span class="pre">**</span> <span class="pre">2)</span></code> assuming
d ~ n_features ~ n_components.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>n_components</strong> (<em>int</em><em>, </em><em>default=256</em>) – Number of binary hidden units.</p></li>
<li><p><strong>learning_rate</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The learning rate for weight updates. It is <em>highly</em> recommended
to tune this hyper-parameter. Reasonable values are in the
10**[0., -3.] range.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default=10</em>) – Number of examples per minibatch.</p></li>
<li><p><strong>n_iter</strong> (<em>int</em><em>, </em><em>default=10</em>) – Number of iterations/sweeps over the training dataset to perform
during training.</p></li>
<li><p><strong>verbose</strong> (<em>int</em><em>, </em><em>default=0</em>) – The verbosity level. The default, zero, means silent mode.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – <p>Determines random number generation for:</p>
<ul>
<li><p>Gibbs sampling from visible and hidden layers.</p></li>
<li><p>Initializing components, sampling from layers during fit.</p></li>
<li><p>Corrupting the data when scoring samples.</p></li>
</ul>
<p>Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.intercept_hidden_">
<span class="sig-name descname"><span class="pre">intercept_hidden_</span></span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.intercept_hidden_" title="Permalink to this definition">¶</a></dt>
<dd><p>Biases of the hidden units.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_components,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.intercept_visible_">
<span class="sig-name descname"><span class="pre">intercept_visible_</span></span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.intercept_visible_" title="Permalink to this definition">¶</a></dt>
<dd><p>Biases of the visible units.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.components_">
<span class="sig-name descname"><span class="pre">components_</span></span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.components_" title="Permalink to this definition">¶</a></dt>
<dd><p>Weight matrix, where n_features in the number of
visible units and n_components is the number of hidden units.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (n_components, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.h_samples_">
<span class="sig-name descname"><span class="pre">h_samples_</span></span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.h_samples_" title="Permalink to this definition">¶</a></dt>
<dd><p>Hidden Activation sampled from the model distribution,
where batch_size in the number of examples per minibatch and
n_components is the number of hidden units.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>array-like of shape (batch_size, n_components)</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">BernoulliRBM</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">BernoulliRBM</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">BernoulliRBM(n_components=2)</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="simple">
<dt>[1] Hinton, G. E., Osindero, S. and Teh, Y. A fast learning algorithm for</dt><dd><p>deep belief nets. Neural Computation 18, pp 1527-1554.
<a class="reference external" href="https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf">https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf</a></p>
</dd>
<dt>[2] Tieleman, T. Training Restricted Boltzmann Machines using</dt><dd><p>Approximations to the Likelihood Gradient. International Conference
on Machine Learning (ICML) 2008</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model to the data X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM">BernoulliRBM</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.gibbs">
<span class="sig-name descname"><span class="pre">gibbs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">v</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.gibbs" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform one Gibbs sampling step.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>v</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Values of the visible layer to start from.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>v_new</strong> – Values of the visible layer after one Gibbs step.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_features)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the model to the data X which should contain a partial
segment of the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Training data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – The fitted model.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.neural_network.BernoulliRBM" title="sklearn.neural_network.BernoulliRBM">BernoulliRBM</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.score_samples">
<span class="sig-name descname"><span class="pre">score_samples</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.score_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pseudo-likelihood of X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – Values of the visible layer. Must be all-boolean (not checked).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>pseudo_likelihood</strong> – Value of the pseudo-likelihood (proxy for likelihood).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples,)</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This method is not deterministic: it computes a quantity called the
free energy on X, then on a randomly corrupted version of X, and
returns the log of the logistic function of the difference.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.BernoulliRBM.transform">
<span class="sig-name descname"><span class="pre">transform</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.BernoulliRBM.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the hidden layer activation probabilities, P(h=1|v=X).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The data to be transformed.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>h</strong> – Latent representations of the data.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_components)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.neural_network.</span></span><span class="sig-name descname"><span class="pre">MLPClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_layer_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(100,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nesterovs_momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron</span></code></p>
<p>Multi-layer Perceptron classifier.</p>
<p>This model optimizes the log-loss function using LBFGS or stochastic
gradient descent.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_layer_sizes</strong> (<em>tuple</em><em>, </em><em>length = n_layers - 2</em><em>, </em><em>default=</em><em>(</em><em>100</em><em>,</em><em>)</em>) – The ith element represents the number of neurons in the ith
hidden layer.</p></li>
<li><p><strong>activation</strong> (<em>{'identity'</em><em>, </em><em>'logistic'</em><em>, </em><em>'tanh'</em><em>, </em><em>'relu'}</em><em>, </em><em>default='relu'</em>) – <p>Activation function for the hidden layer.</p>
<ul>
<li><p>’identity’, no-op activation, useful to implement linear bottleneck,
returns f(x) = x</p></li>
<li><p>’logistic’, the logistic sigmoid function,
returns f(x) = 1 / (1 + exp(-x)).</p></li>
<li><p>’tanh’, the hyperbolic tan function,
returns f(x) = tanh(x).</p></li>
<li><p>’relu’, the rectified linear unit function,
returns f(x) = max(0, x)</p></li>
</ul>
</p></li>
<li><p><strong>solver</strong> (<em>{'lbfgs'</em><em>, </em><em>'sgd'</em><em>, </em><em>'adam'}</em><em>, </em><em>default='adam'</em>) – <p>The solver for weight optimization.</p>
<ul>
<li><p>’lbfgs’ is an optimizer in the family of quasi-Newton methods.</p></li>
<li><p>’sgd’ refers to stochastic gradient descent.</p></li>
<li><p>’adam’ refers to a stochastic gradient-based optimizer proposed
by Kingma, Diederik, and Jimmy Ba</p></li>
</ul>
<p>Note: The default solver ‘adam’ works pretty well on relatively
large datasets (with thousands of training samples or more) in terms of
both training time and validation score.
For small datasets, however, ‘lbfgs’ can converge faster and perform
better.</p>
</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – L2 penalty (regularization term) parameter.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default='auto'</em>) – Size of minibatches for stochastic optimizers.
If the solver is ‘lbfgs’, the classifier will not use minibatch.
When set to “auto”, <cite>batch_size=min(200, n_samples)</cite></p></li>
<li><p><strong>learning_rate</strong> (<em>{'constant'</em><em>, </em><em>'invscaling'</em><em>, </em><em>'adaptive'}</em><em>, </em><em>default='constant'</em>) – <p>Learning rate schedule for weight updates.</p>
<ul>
<li><p>’constant’ is a constant learning rate given by
‘learning_rate_init’.</p></li>
<li><p>’invscaling’ gradually decreases the learning rate at each
time step ‘t’ using an inverse scaling exponent of ‘power_t’.
effective_learning_rate = learning_rate_init / pow(t, power_t)</p></li>
<li><p>’adaptive’ keeps the learning rate constant to
‘learning_rate_init’ as long as training loss keeps decreasing.
Each time two consecutive epochs fail to decrease training loss by at
least tol, or fail to increase validation score by at least tol if
‘early_stopping’ is on, the current learning rate is divided by 5.</p></li>
</ul>
<p>Only used when <code class="docutils literal notranslate"><span class="pre">solver='sgd'</span></code>.</p>
</p></li>
<li><p><strong>learning_rate_init</strong> (<em>double</em><em>, </em><em>default=0.001</em>) – The initial learning rate used. It controls the step-size
in updating the weights. Only used when solver=’sgd’ or ‘adam’.</p></li>
<li><p><strong>power_t</strong> (<em>double</em><em>, </em><em>default=0.5</em>) – The exponent for inverse scaling learning rate.
It is used in updating effective learning rate when the learning_rate
is set to ‘invscaling’. Only used when solver=’sgd’.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=200</em>) – Maximum number of iterations. The solver iterates until convergence
(determined by ‘tol’) or this number of iterations. For stochastic
solvers (‘sgd’, ‘adam’), note that this determines the number of epochs
(how many times each data point will be used), not the number of
gradient steps.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to shuffle samples in each iteration. Only used when
solver=’sgd’ or ‘adam’.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Determines random number generation for weights and bias
initialization, train-test split if early stopping is used, and batch
sampling when solver=’sgd’ or ‘adam’.
Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for the optimization. When the loss or score is not improving
by at least <code class="docutils literal notranslate"><span class="pre">tol</span></code> for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive iterations,
unless <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> is set to ‘adaptive’, convergence is
considered to be reached and training stops.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to print progress messages to stdout.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous
call to fit as initialization, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – Momentum for gradient descent update. Should be between 0 and 1. Only
used when solver=’sgd’.</p></li>
<li><p><strong>nesterovs_momentum</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and
momentum &gt; 0.</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use early stopping to terminate training when validation
score is not improving. If set to true, it will automatically set
aside 10% of training data as validation and terminate training when
validation score is not improving by at least tol for
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive epochs. The split is stratified,
except in a multilabel setting.
Only effective when solver=’sgd’ or ‘adam’</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True</p></li>
<li><p><strong>beta_1</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – Exponential decay rate for estimates of first moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</p></li>
<li><p><strong>beta_2</strong> (<em>float</em><em>, </em><em>default=0.999</em>) – Exponential decay rate for estimates of second moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=1e-8</em>) – Value for numerical stability in adam. Only used when solver=’adam’</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=10</em>) – <p>Maximum number of epochs to not meet <code class="docutils literal notranslate"><span class="pre">tol</span></code> improvement.
Only effective when solver=’sgd’ or ‘adam’</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>max_fun</strong> (<em>int</em><em>, </em><em>default=15000</em>) – <p>Only used when solver=’lbfgs’. Maximum number of loss function calls.
The solver iterates until convergence (determined by ‘tol’), number
of iterations reaches max_iter, or this number of loss function calls.
Note that number of loss function calls will be greater than or equal
to the number of iterations for the <cite>MLPClassifier</cite>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>Class labels for each output.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray or list of ndarray of shape (n_classes,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The current loss computed with the loss function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.best_loss_">
<span class="sig-name descname"><span class="pre">best_loss_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.best_loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The minimum loss reached by the solver throughout fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.loss_curve_">
<span class="sig-name descname"><span class="pre">loss_curve_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.loss_curve_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ith element in the list represents the loss at the ith iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (<cite>n_iter_</cite>,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of training samples seen by the solver during fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.coefs_">
<span class="sig-name descname"><span class="pre">coefs_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.coefs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ith element in the list represents the weight matrix corresponding
to layer i.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (n_layers - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.intercepts_">
<span class="sig-name descname"><span class="pre">intercepts_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.intercepts_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ith element in the list represents the bias vector corresponding to
layer i + 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (n_layers - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations the solver has run.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.n_layers_">
<span class="sig-name descname"><span class="pre">n_layers_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.n_layers_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.out_activation_">
<span class="sig-name descname"><span class="pre">out_activation_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.out_activation_" title="Permalink to this definition">¶</a></dt>
<dd><p>Name of the output activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_classification</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>
<span class="gp">... </span>                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="go">array([[0.038..., 0.961...]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>
<span class="go">array([1, 0, 1, 0, 1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.8...</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>MLPClassifier trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.</p>
<p>It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.</p>
<p>This implementation works with data represented as dense numpy arrays or
sparse scipy arrays of floating point values.</p>
<p class="rubric">References</p>
<dl class="simple">
<dt>Hinton, Geoffrey E.</dt><dd><p>“Connectionist learning procedures.” Artificial intelligence 40.1
(1989): 185-234.</p>
</dd>
<dt>Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of</dt><dd><p>training deep feedforward neural networks.” International Conference
on Artificial Intelligence and Statistics. 2010.</p>
</dd>
<dt>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level</dt><dd><p>performance on imagenet classification.” arXiv preprint
arXiv:1502.01852 (2015).</p>
</dd>
<dt>Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochastic</dt><dd><p>optimization.” arXiv preprint arXiv:1412.6980 (2014).</p>
</dd>
</dl>
<dl class="py property">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.partial_fit">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">partial_fit</span></span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Update the model with a single iteration over the given data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em>) – The target values.</p></li>
<li><p><strong>classes</strong> (<em>array of shape</em><em> (</em><em>n_classes</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Classes across all calls to partial_fit.
Can be obtained via <cite>np.unique(y_all)</cite>, where y_all is the
target vector of the entire dataset.
This argument is required for the first call to partial_fit
and can be omitted in the subsequent calls.
Note that y doesn’t need to contain all labels in <cite>classes</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>returns a trained MLP model.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the multi-layer perceptron classifier</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray, shape (n_samples,) or (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the log of probability estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>ndarray of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>log_y_prob</strong> – The predicted log-probability of the sample for each class
in the model, where classes are ordered as they are in
<cite>self.classes_</cite>. Equivalent to log(predict_proba(X))</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Probability estimates.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y_prob</strong> – The predicted probability of the sample for each class in the
model, where classes are ordered as they are in <cite>self.classes_</cite>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.neural_network.</span></span><span class="sig-name descname"><span class="pre">MLPRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hidden_layer_sizes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">(100,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activation</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'relu'</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">solver</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'adam'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'constant'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate_init</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">power_t</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_iter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tol</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_start</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nesterovs_momentum</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">early_stopping</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">validation_fraction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_1</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.9</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">beta_2</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.999</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1e-08</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_iter_no_change</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_fun</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">15000</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neural_network._multilayer_perceptron.BaseMultilayerPerceptron</span></code></p>
<p>Multi-layer Perceptron regressor.</p>
<p>This model optimizes the squared error using LBFGS or stochastic gradient
descent.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hidden_layer_sizes</strong> (<em>tuple</em><em>, </em><em>length = n_layers - 2</em><em>, </em><em>default=</em><em>(</em><em>100</em><em>,</em><em>)</em>) – The ith element represents the number of neurons in the ith
hidden layer.</p></li>
<li><p><strong>activation</strong> (<em>{'identity'</em><em>, </em><em>'logistic'</em><em>, </em><em>'tanh'</em><em>, </em><em>'relu'}</em><em>, </em><em>default='relu'</em>) – <p>Activation function for the hidden layer.</p>
<ul>
<li><p>’identity’, no-op activation, useful to implement linear bottleneck,
returns f(x) = x</p></li>
<li><p>’logistic’, the logistic sigmoid function,
returns f(x) = 1 / (1 + exp(-x)).</p></li>
<li><p>’tanh’, the hyperbolic tan function,
returns f(x) = tanh(x).</p></li>
<li><p>’relu’, the rectified linear unit function,
returns f(x) = max(0, x)</p></li>
</ul>
</p></li>
<li><p><strong>solver</strong> (<em>{'lbfgs'</em><em>, </em><em>'sgd'</em><em>, </em><em>'adam'}</em><em>, </em><em>default='adam'</em>) – <p>The solver for weight optimization.</p>
<ul>
<li><p>’lbfgs’ is an optimizer in the family of quasi-Newton methods.</p></li>
<li><p>’sgd’ refers to stochastic gradient descent.</p></li>
<li><p>’adam’ refers to a stochastic gradient-based optimizer proposed by
Kingma, Diederik, and Jimmy Ba</p></li>
</ul>
<p>Note: The default solver ‘adam’ works pretty well on relatively
large datasets (with thousands of training samples or more) in terms of
both training time and validation score.
For small datasets, however, ‘lbfgs’ can converge faster and perform
better.</p>
</p></li>
<li><p><strong>alpha</strong> (<em>float</em><em>, </em><em>default=0.0001</em>) – L2 penalty (regularization term) parameter.</p></li>
<li><p><strong>batch_size</strong> (<em>int</em><em>, </em><em>default='auto'</em>) – Size of minibatches for stochastic optimizers.
If the solver is ‘lbfgs’, the classifier will not use minibatch.
When set to “auto”, <cite>batch_size=min(200, n_samples)</cite></p></li>
<li><p><strong>learning_rate</strong> (<em>{'constant'</em><em>, </em><em>'invscaling'</em><em>, </em><em>'adaptive'}</em><em>, </em><em>default='constant'</em>) – <p>Learning rate schedule for weight updates.</p>
<ul>
<li><p>’constant’ is a constant learning rate given by
‘learning_rate_init’.</p></li>
<li><p>’invscaling’ gradually decreases the learning rate <code class="docutils literal notranslate"><span class="pre">learning_rate_</span></code>
at each time step ‘t’ using an inverse scaling exponent of ‘power_t’.
effective_learning_rate = learning_rate_init / pow(t, power_t)</p></li>
<li><p>’adaptive’ keeps the learning rate constant to
‘learning_rate_init’ as long as training loss keeps decreasing.
Each time two consecutive epochs fail to decrease training loss by at
least tol, or fail to increase validation score by at least tol if
‘early_stopping’ is on, the current learning rate is divided by 5.</p></li>
</ul>
<p>Only used when solver=’sgd’.</p>
</p></li>
<li><p><strong>learning_rate_init</strong> (<em>double</em><em>, </em><em>default=0.001</em>) – The initial learning rate used. It controls the step-size
in updating the weights. Only used when solver=’sgd’ or ‘adam’.</p></li>
<li><p><strong>power_t</strong> (<em>double</em><em>, </em><em>default=0.5</em>) – The exponent for inverse scaling learning rate.
It is used in updating effective learning rate when the learning_rate
is set to ‘invscaling’. Only used when solver=’sgd’.</p></li>
<li><p><strong>max_iter</strong> (<em>int</em><em>, </em><em>default=200</em>) – Maximum number of iterations. The solver iterates until convergence
(determined by ‘tol’) or this number of iterations. For stochastic
solvers (‘sgd’, ‘adam’), note that this determines the number of epochs
(how many times each data point will be used), not the number of
gradient steps.</p></li>
<li><p><strong>shuffle</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to shuffle samples in each iteration. Only used when
solver=’sgd’ or ‘adam’.</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em>, </em><em>default=None</em>) – Determines random number generation for weights and bias
initialization, train-test split if early stopping is used, and batch
sampling when solver=’sgd’ or ‘adam’.
Pass an int for reproducible results across multiple function calls.
See <span class="xref std std-term">Glossary</span>.</p></li>
<li><p><strong>tol</strong> (<em>float</em><em>, </em><em>default=1e-4</em>) – Tolerance for the optimization. When the loss or score is not improving
by at least <code class="docutils literal notranslate"><span class="pre">tol</span></code> for <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive iterations,
unless <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> is set to ‘adaptive’, convergence is
considered to be reached and training stops.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to print progress messages to stdout.</p></li>
<li><p><strong>warm_start</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to True, reuse the solution of the previous
call to fit as initialization, otherwise, just erase the
previous solution. See <span class="xref std std-term">the Glossary</span>.</p></li>
<li><p><strong>momentum</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – Momentum for gradient descent update.  Should be between 0 and 1. Only
used when solver=’sgd’.</p></li>
<li><p><strong>nesterovs_momentum</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and
momentum &gt; 0.</p></li>
<li><p><strong>early_stopping</strong> (<em>bool</em><em>, </em><em>default=False</em>) – Whether to use early stopping to terminate training when validation
score is not improving. If set to true, it will automatically set
aside 10% of training data as validation and terminate training when
validation score is not improving by at least <code class="docutils literal notranslate"><span class="pre">tol</span></code> for
<code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> consecutive epochs.
Only effective when solver=’sgd’ or ‘adam’</p></li>
<li><p><strong>validation_fraction</strong> (<em>float</em><em>, </em><em>default=0.1</em>) – The proportion of training data to set aside as validation set for
early stopping. Must be between 0 and 1.
Only used if early_stopping is True</p></li>
<li><p><strong>beta_1</strong> (<em>float</em><em>, </em><em>default=0.9</em>) – Exponential decay rate for estimates of first moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</p></li>
<li><p><strong>beta_2</strong> (<em>float</em><em>, </em><em>default=0.999</em>) – Exponential decay rate for estimates of second moment vector in adam,
should be in [0, 1). Only used when solver=’adam’</p></li>
<li><p><strong>epsilon</strong> (<em>float</em><em>, </em><em>default=1e-8</em>) – Value for numerical stability in adam. Only used when solver=’adam’</p></li>
<li><p><strong>n_iter_no_change</strong> (<em>int</em><em>, </em><em>default=10</em>) – <p>Maximum number of epochs to not meet <code class="docutils literal notranslate"><span class="pre">tol</span></code> improvement.
Only effective when solver=’sgd’ or ‘adam’</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.20.</span></p>
</div>
</p></li>
<li><p><strong>max_fun</strong> (<em>int</em><em>, </em><em>default=15000</em>) – <p>Only used when solver=’lbfgs’. Maximum number of function calls.
The solver iterates until convergence (determined by ‘tol’), number
of iterations reaches max_iter, or this number of function calls.
Note that number of function calls will be greater than or equal to
the number of iterations for the MLPRegressor.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.loss_">
<span class="sig-name descname"><span class="pre">loss_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The current loss computed with the loss function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.best_loss_">
<span class="sig-name descname"><span class="pre">best_loss_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.best_loss_" title="Permalink to this definition">¶</a></dt>
<dd><p>The minimum loss reached by the solver throughout fitting.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.loss_curve_">
<span class="sig-name descname"><span class="pre">loss_curve_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.loss_curve_" title="Permalink to this definition">¶</a></dt>
<dd><p>Loss value evaluated at the end of each training step.
The ith element in the list represents the loss at the ith iteration.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (<cite>n_iter_</cite>,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.t_">
<span class="sig-name descname"><span class="pre">t_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.t_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of training samples seen by the solver during fitting.
Mathematically equals <cite>n_iters * X.shape[0]</cite>, it means
<cite>time_step</cite> and it is used by optimizer’s learning rate scheduler.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.coefs_">
<span class="sig-name descname"><span class="pre">coefs_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.coefs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ith element in the list represents the weight matrix corresponding
to layer i.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (n_layers - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.intercepts_">
<span class="sig-name descname"><span class="pre">intercepts_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.intercepts_" title="Permalink to this definition">¶</a></dt>
<dd><p>The ith element in the list represents the bias vector corresponding to
layer i + 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>list of shape (n_layers - 1,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.n_iter_">
<span class="sig-name descname"><span class="pre">n_iter_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.n_iter_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of iterations the solver has run.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.n_layers_">
<span class="sig-name descname"><span class="pre">n_layers_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.n_layers_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of layers.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of outputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.out_activation_">
<span class="sig-name descname"><span class="pre">out_activation_</span></span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.out_activation_" title="Permalink to this definition">¶</a></dt>
<dd><p>Name of the output activation function.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>

<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span>                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span> <span class="o">=</span> <span class="n">MLPRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">2</span><span class="p">])</span>
<span class="go">array([-0.9..., -7.1...])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regr</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.4...</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>MLPRegressor trains iteratively since at each time step
the partial derivatives of the loss function with respect to the model
parameters are computed to update the parameters.</p>
<p>It can also have a regularization term added to the loss function
that shrinks model parameters to prevent overfitting.</p>
<p>This implementation works with data represented as dense and sparse numpy
arrays of floating point values.</p>
<p class="rubric">References</p>
<dl class="simple">
<dt>Hinton, Geoffrey E.</dt><dd><p>“Connectionist learning procedures.” Artificial intelligence 40.1
(1989): 185-234.</p>
</dd>
<dt>Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of</dt><dd><p>training deep feedforward neural networks.” International Conference
on Artificial Intelligence and Statistics. 2010.</p>
</dd>
<dt>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level</dt><dd><p>performance on imagenet classification.” arXiv preprint
arXiv:1502.01852 (2015).</p>
</dd>
<dt>Kingma, Diederik, and Jimmy Ba. “Adam: A method for stochastic</dt><dd><p>optimization.” arXiv preprint arXiv:1412.6980 (2014).</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.neural_network.MLPRegressor.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.neural_network.MLPRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict using the multi-layer perceptron model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input data.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_outputs)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Tommaso Fioravanti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>