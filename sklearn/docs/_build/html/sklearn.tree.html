

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.tree package &mdash; sqlearn  documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> sqlearn
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">sqlearn</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">sqlearn</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>sklearn.tree package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/sklearn.tree.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="sklearn-tree-package">
<h1>sklearn.tree package<a class="headerlink" href="#sklearn-tree-package" title="Permalink to this headline">¶</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="sklearn.tree.tests.html">sklearn.tree.tests package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="sklearn.tree.tests.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.tree.tests.html#sklearn-tree-tests-test-export-module">sklearn.tree.tests.test_export module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.tree.tests.html#sklearn-tree-tests-test-reingold-tilford-module">sklearn.tree.tests.test_reingold_tilford module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.tree.tests.html#sklearn-tree-tests-test-tree-module">sklearn.tree.tests.test_tree module</a></li>
<li class="toctree-l2"><a class="reference internal" href="sklearn.tree.tests.html#module-sklearn.tree.tests">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</section>
<section id="module-sklearn.tree.setup">
<span id="sklearn-tree-setup-module"></span><h2>sklearn.tree.setup module<a class="headerlink" href="#module-sklearn.tree.setup" title="Permalink to this headline">¶</a></h2>
<dl class="py function">
<dt class="sig sig-object py" id="sklearn.tree.setup.configuration">
<span class="sig-prename descclassname"><span class="pre">sklearn.tree.setup.</span></span><span class="sig-name descname"><span class="pre">configuration</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">parent_package</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">top_path</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.setup.configuration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</section>
<section id="module-sklearn.tree">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-sklearn.tree" title="Permalink to this headline">¶</a></h2>
<p>The <a class="reference internal" href="#module-sklearn.tree" title="sklearn.tree"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.tree</span></code></a> module includes decision tree-based models for
classification and regression.</p>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">BaseDecisionTree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.MultiOutputMixin</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.BaseEstimator</span></code></p>
<p>Base class for decision trees.</p>
<p>Warning: This class should not be used directly.
Use derived classes instead.</p>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the index of the leaf that each sample is predicted as.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.17.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>X_leaves</strong> – For each datapoint x in X, return the index of the leaf x
ends up in. Leaves are numbered within
<code class="docutils literal notranslate"><span class="pre">[0;</span> <span class="pre">self.tree_.node_count)</span></code>, possibly with gaps in the
numbering.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.cost_complexity_pruning_path">
<span class="sig-name descname"><span class="pre">cost_complexity_pruning_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.cost_complexity_pruning_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the pruning path during Minimal Cost-Complexity Pruning.</p>
<p>See <span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details on the pruning
process.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – The target values (class labels) as integers or strings.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. Splits are also
ignored if they would result in any single class carrying a
negative weight in either child node.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>ccp_path</strong> – Dictionary-like object, with the following attributes.</p>
<dl class="simple">
<dt>ccp_alphas<span class="classifier">ndarray</span></dt><dd><p>Effective alphas of subtree during pruning.</p>
</dd>
<dt>impurities<span class="classifier">ndarray</span></dt><dd><p>Sum of the impurities of the subtree leaves for the
corresponding alpha value in <code class="docutils literal notranslate"><span class="pre">ccp_alphas</span></code>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Bunch</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.decision_path">
<span class="sig-name descname"><span class="pre">decision_path</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.decision_path" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the decision path in the tree.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>indicator</strong> – Return a node indicator CSR matrix where non zero elements
indicates that the samples goes through the nodes.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>sparse matrix of shape (n_samples, n_nodes)</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.feature_importances_">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the feature importances.</p>
<p>The importance of a feature is computed as the (normalized) total
reduction of the criterion brought by that feature.
It is also known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>feature_importances_</strong> – Normalized total reduction of criteria by feature
(Gini importance).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.fit" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.get_depth">
<span class="sig-name descname"><span class="pre">get_depth</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.get_depth" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the depth of the decision tree.</p>
<p>The depth of a tree is the maximum distance between the root
and any leaf.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>self.tree_.max_depth</strong> – The maximum depth of the tree.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.get_n_leaves">
<span class="sig-name descname"><span class="pre">get_n_leaves</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.get_n_leaves" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the number of leaves of the decision tree.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><strong>self.tree_.n_leaves</strong> – Number of leaves.</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.BaseDecisionTree.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.BaseDecisionTree.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class or regression value for X.</p>
<p>For a classification model, the predicted class for each sample in X is
returned. For a regression model, the predicted value based on X is
returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>y</strong> – The predicted classes, or the predict values.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>array-like of shape (n_samples,) or (n_samples, n_outputs)</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">DecisionTreeClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.ClassifierMixin</span></code>, <a class="reference internal" href="#sklearn.tree.BaseDecisionTree" title="sklearn.tree._classes.BaseDecisionTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.BaseDecisionTree</span></code></a></p>
<p>A decision tree classifier.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;best&quot;</em><em>, </em><em>&quot;random&quot;}</em><em>, </em><em>default=&quot;best&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em> or </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
</div></blockquote>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the randomness of the estimator. The features are always
randomly permuted at each split, even if <code class="docutils literal notranslate"><span class="pre">splitter</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">&quot;best&quot;</span></code>. When <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>, the algorithm will
select <code class="docutils literal notranslate"><span class="pre">max_features</span></code> at random at each split before finding the best
split among them. But the best found split may vary across different
runs, even if <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>. That is the case, if the
improvement of the criterion is identical for several splits and one
split has to be selected at random. To obtain a deterministic behaviour
during fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed to an integer.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>list of dict</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If None, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance <a href="#id13"><span class="problematic" id="id1">[4]_</span></a>.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.tree_" title="Permalink to this definition">¶</a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a></dt><dd><p>A decision tree regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code> method operates using the <code class="xref py py-func docutils literal notranslate"><span class="pre">numpy.argmax()</span></code>
function on the outputs of <a class="reference internal" href="#sklearn.tree.DecisionTreeClassifier.predict_proba" title="sklearn.tree.DecisionTreeClassifier.predict_proba"><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_proba()</span></code></a>. This means that in
case the highest predicted probabilities are tied, the classifier will
predict the tied class with the lowest index in <span class="xref std std-term">classes_</span>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></p>
</dd>
<dt class="label" id="id3"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</p>
</dd>
<dt class="label" id="id4"><span class="brackets">3</span></dt>
<dd><p>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">4</span></dt>
<dd><p>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                            
<span class="gp">...</span>
<span class="go">array([ 1.     ,  0.93...,  0.86...,  0.93...,  0.93...,</span>
<span class="go">        0.93...,  0.93...,  1.     ,  0.93...,  1.      ])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a decision tree classifier from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – The target values (class labels) as integers or strings.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node. Splits are also
ignored if they would result in any single class carrying a
negative weight in either child node.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
<li><p><strong>X_idx_sorted</strong> (<a class="reference internal" href="sklearn.utils.html#sklearn.utils.deprecation.deprecated" title="sklearn.utils.deprecation.deprecated"><em>deprecated</em></a><em>, </em><em>default=&quot;deprecated&quot;</em>) – <p>This parameter is deprecated and has no effect.
It will be removed in 1.1 (renaming of 0.26).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier">DecisionTreeClassifier</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.predict_log_proba">
<span class="sig-name descname"><span class="pre">predict_log_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.predict_log_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class log-probabilities of the input samples X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>proba</strong> – The class log-probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeClassifier.predict_proba">
<span class="sig-name descname"><span class="pre">predict_proba</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict class probabilities of the input samples X.</p>
<p>The predicted class probability is the fraction of samples of the same
class in a leaf.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code>.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>proba</strong> – The class probabilities of the input samples. The order of the
classes corresponds to that in the attribute <span class="xref std std-term">classes_</span>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ndarray of shape (n_samples, n_classes) or list of n_outputs             such arrays if n_outputs &gt; 1</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">DecisionTreeRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'best'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.base.RegressorMixin</span></code>, <a class="reference internal" href="#sklearn.tree.BaseDecisionTree" title="sklearn.tree._classes.BaseDecisionTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.BaseDecisionTree</span></code></a></p>
<p>A decision tree regressor.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;squared_error&quot;</em><em>, </em><em>&quot;mse&quot;</em><em>, </em><em>&quot;friedman_mse&quot;</em><em>, </em><em>&quot;mae&quot;</em><em>, </em><em>&quot;poisson&quot;}</em><em>,             </em><em>default=&quot;squared_error&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “squared_error” for the mean squared error, which is equal to
variance reduction as feature selection criterion and minimizes the L2
loss using the mean of each terminal node, “friedman_mse”, which uses
mean squared error with Friedman’s improvement score for potential
splits, “mae” for the mean absolute error, which minimizes the L1 loss
using the median of each terminal node, and “poisson” which uses
reduction in Poisson deviance to find splits.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24: </span>Poisson deviance criterion.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion “mse” was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=”squared_error”</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;best&quot;</em><em>, </em><em>&quot;random&quot;}</em><em>, </em><em>default=&quot;best&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em> or </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em>, </em><em>default=None</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Controls the randomness of the estimator. The features are always
randomly permuted at each split, even if <code class="docutils literal notranslate"><span class="pre">splitter</span></code> is set to
<code class="docutils literal notranslate"><span class="pre">&quot;best&quot;</span></code>. When <code class="docutils literal notranslate"><span class="pre">max_features</span> <span class="pre">&lt;</span> <span class="pre">n_features</span></code>, the algorithm will
select <code class="docutils literal notranslate"><span class="pre">max_features</span></code> at random at each split before finding the best
split among them. But the best found split may vary across different
runs, even if <code class="docutils literal notranslate"><span class="pre">max_features=n_features</span></code>. That is the case, if the
improvement of the criterion is identical for several splits and one
split has to be selected at random. To obtain a deterministic behaviour
during fitting, <code class="docutils literal notranslate"><span class="pre">random_state</span></code> has to be fixed to an integer.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=0</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the
(normalized) total reduction of the criterion brought
by that feature. It is also known as the Gini importance <a href="#id14"><span class="problematic" id="id6">[4]_</span></a>.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.tree_" title="Permalink to this definition">¶</a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a></dt><dd><p>A decision tree classifier.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Decision_tree_learning">https://en.wikipedia.org/wiki/Decision_tree_learning</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets">2</span></dt>
<dd><p>L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification
and Regression Trees”, Wadsworth, Belmont, CA, 1984.</p>
</dd>
<dt class="label" id="id9"><span class="brackets">3</span></dt>
<dd><p>T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical
Learning”, Springer, 2009.</p>
</dd>
<dt class="label" id="id10"><span class="brackets">4</span></dt>
<dd><p>L. Breiman, and A. Cutler, “Random Forests”,
<a class="reference external" href="https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">regressor</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">regressor</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">... </span>                   
<span class="gp">...</span>
<span class="go">array([-0.39..., -0.46...,  0.02...,  0.06..., -0.50...,</span>
<span class="go">       0.16...,  0.11..., -0.73..., -0.30..., -0.00...])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.tree.DecisionTreeRegressor.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sample_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_input</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">X_idx_sorted</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.DecisionTreeRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a decision tree regressor from the training set (X, y).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>X</strong> (<em>{array-like</em><em>, </em><em>sparse matrix} of shape</em><em> (</em><em>n_samples</em><em>, </em><em>n_features</em><em>)</em>) – The training input samples. Internally, it will be converted to
<code class="docutils literal notranslate"><span class="pre">dtype=np.float32</span></code> and if a sparse matrix is provided
to a sparse <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code>.</p></li>
<li><p><strong>y</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – The target values (real numbers). Use <code class="docutils literal notranslate"><span class="pre">dtype=np.float64</span></code> and
<code class="docutils literal notranslate"><span class="pre">order='C'</span></code> for maximum efficiency.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape</em><em> (</em><em>n_samples</em><em>,</em><em>)</em><em>, </em><em>default=None</em>) – Sample weights. If None, then samples are equally weighted. Splits
that would create child nodes with net zero or negative weight are
ignored while searching for a split in each node.</p></li>
<li><p><strong>check_input</strong> (<em>bool</em><em>, </em><em>default=True</em>) – Allow to bypass several input checking.
Don’t use this parameter unless you know what you do.</p></li>
<li><p><strong>X_idx_sorted</strong> (<a class="reference internal" href="sklearn.utils.html#sklearn.utils.deprecation.deprecated" title="sklearn.utils.deprecation.deprecated"><em>deprecated</em></a><em>, </em><em>default=&quot;deprecated&quot;</em>) – <p>This parameter is deprecated and has no effect.
It will be removed in 1.1 (renaming of 0.26).</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.24.</span></p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>self</strong> – Fitted estimator.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference internal" href="#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor">DecisionTreeRegressor</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">ExtraTreeClassifier</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'gini'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_weight</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree._classes.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.DecisionTreeClassifier</span></code></a></p>
<p>An extremely randomized tree classifier.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;gini&quot;</em><em>, </em><em>&quot;entropy&quot;}</em><em>, </em><em>default=&quot;gini&quot;</em>) – The function to measure the quality of a split. Supported criteria are
“gini” for the Gini impurity and “entropy” for the information gain.</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;random&quot;</em><em>, </em><em>&quot;best&quot;}</em><em>, </em><em>default=&quot;random&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em>, </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em> or </em><em>None</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<blockquote>
<div><ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
</div></blockquote>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Used to pick randomly the <cite>max_features</cite> used at each split.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>class_weight</strong> (<em>dict</em><em>, </em><em>list of dict</em><em> or </em><em>&quot;balanced&quot;</em><em>, </em><em>default=None</em>) – <p>Weights associated with classes in the form <code class="docutils literal notranslate"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
If None, all classes are supposed to have weight one. For
multi-output problems, a list of dicts can be provided in the same
order as the columns of y.</p>
<p>Note that for multioutput (including multilabel) weights should be
defined for each class of every column in its own dict. For example,
for four-class multilabel classification weights should be
[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
[{1:1}, {2:5}, {3:1}, {4:1}].</p>
<p>The “balanced” mode uses the values of y to automatically adjust
weights inversely proportional to class frequencies in the input data
as <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
<p>For multi-output, the weights of each column of y will be multiplied.</p>
<p>Note that these weights will be multiplied with sample_weight (passed
through the fit method) if sample_weight is specified.</p>
</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.classes_">
<span class="sig-name descname"><span class="pre">classes_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The classes labels (single output problem),
or a list of arrays of class labels (multi-output problem).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_classes,) or list of ndarray</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.n_classes_">
<span class="sig-name descname"><span class="pre">n_classes_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.n_classes_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of classes (for single output problems),
or a list containing the number of classes for each
output (for multi-output problems).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int or list of int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>The impurity-based feature importances.
The higher, the more important the feature.
The importance of a feature is computed as the (normalized)
total reduction of the criterion brought by that feature.  It is also
known as the Gini importance.</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeClassifier.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeClassifier.tree_" title="Permalink to this definition">¶</a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.tree.ExtraTreeRegressor" title="sklearn.tree.ExtraTreeRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreeRegressor</span></code></a></dt><dd><p>An extremely randomized tree regressor.</p>
</dd>
<dt><a class="reference internal" href="sklearn.ensemble.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesClassifier</span></code></a></dt><dd><p>An extra-trees classifier.</p>
</dd>
<dt><a class="reference internal" href="sklearn.ensemble.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesRegressor</span></code></a></dt><dd><p>An extra-trees regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">ExtraTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extra_tree</span> <span class="o">=</span> <span class="n">ExtraTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">cls</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">extra_tree</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">cls</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.8947...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">ExtraTreeRegressor</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">criterion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">splitter</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'random'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_samples_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_weight_fraction_leaf</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'auto'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_state</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_decrease</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_impurity_split</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_leaf_nodes</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ccp_alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree._classes.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.tree._classes.DecisionTreeRegressor</span></code></a></p>
<p>An extremely randomized tree regressor.</p>
<p>Extra-trees differ from classic decision trees in the way they are built.
When looking for the best split to separate the samples of a node into two
groups, random splits are drawn for each of the <cite>max_features</cite> randomly
selected features and the best split among those is chosen. When
<cite>max_features</cite> is set 1, this amounts to building a totally random
decision tree.</p>
<p>Warning: Extra-trees should only be used within ensemble methods.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>criterion</strong> (<em>{&quot;squared_error&quot;</em><em>, </em><em>&quot;mse&quot;</em><em>, </em><em>&quot;friedman_mse&quot;</em><em>, </em><em>&quot;mae&quot;}</em><em>,             </em><em>default=&quot;squared_error&quot;</em>) – <p>The function to measure the quality of a split. Supported criteria
are “squared_error” for the mean squared error, which is equal to
variance reduction as feature selection criterion and “mae” for the
mean absolute error.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18: </span>Mean Absolute Error (MAE) criterion.</p>
</div>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.24: </span>Poisson deviance criterion.</p>
</div>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 1.0: </span>Criterion “mse” was deprecated in v1.0 and will be removed in
version 1.2. Use <cite>criterion=”squared_error”</cite> which is equivalent.</p>
</div>
</p></li>
<li><p><strong>splitter</strong> (<em>{&quot;random&quot;</em><em>, </em><em>&quot;best&quot;}</em><em>, </em><em>default=&quot;random&quot;</em>) – The strategy used to choose the split at each node. Supported
strategies are “best” to choose the best split and “random” to choose
the best random split.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the tree. If None, then nodes are expanded until
all leaves are pure or until all leaves contain less than
min_samples_split samples.</p></li>
<li><p><strong>min_samples_split</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=2</em>) – <p>The minimum number of samples required to split an internal node:</p>
<ul>
<li><p>If int, then consider <cite>min_samples_split</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_split</cite> is a fraction and
<cite>ceil(min_samples_split * n_samples)</cite> are the minimum
number of samples for each split.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_samples_leaf</strong> (<em>int</em><em> or </em><em>float</em><em>, </em><em>default=1</em>) – <p>The minimum number of samples required to be at a leaf node.
A split point at any depth will only be considered if it leaves at
least <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
right branches.  This may have the effect of smoothing the model,
especially in regression.</p>
<ul>
<li><p>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</p></li>
<li><p>If float, then <cite>min_samples_leaf</cite> is a fraction and
<cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
number of samples for each node.</p></li>
</ul>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.18: </span>Added float values for fractions.</p>
</div>
</p></li>
<li><p><strong>min_weight_fraction_leaf</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – The minimum weighted fraction of the sum total of weights (of all
the input samples) required to be at a leaf node. Samples have
equal weight when sample_weight is not provided.</p></li>
<li><p><strong>max_features</strong> (<em>int</em><em>, </em><em>float</em><em>, </em><em>{&quot;auto&quot;</em><em>, </em><em>&quot;sqrt&quot;</em><em>, </em><em>&quot;log2&quot;}</em><em> or </em><em>None</em><em>, </em><em>default=&quot;auto&quot;</em>) – <p>The number of features to consider when looking for the best split:</p>
<ul>
<li><p>If int, then consider <cite>max_features</cite> features at each split.</p></li>
<li><p>If float, then <cite>max_features</cite> is a fraction and
<cite>int(max_features * n_features)</cite> features are considered at each
split.</p></li>
<li><p>If “auto”, then <cite>max_features=n_features</cite>.</p></li>
<li><p>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</p></li>
<li><p>If “log2”, then <cite>max_features=log2(n_features)</cite>.</p></li>
<li><p>If None, then <cite>max_features=n_features</cite>.</p></li>
</ul>
<p>Note: the search for a split does not stop until at least one
valid partition of the node samples is found, even if it requires to
effectively inspect more than <code class="docutils literal notranslate"><span class="pre">max_features</span></code> features.</p>
</p></li>
<li><p><strong>random_state</strong> (<em>int</em><em>, </em><em>RandomState instance</em><em> or </em><em>None</em><em>, </em><em>default=None</em>) – Used to pick randomly the <cite>max_features</cite> used at each split.
See <span class="xref std std-term">Glossary</span> for details.</p></li>
<li><p><strong>min_impurity_decrease</strong> (<em>float</em><em>, </em><em>default=0.0</em>) – <p>A node will be split if this split induces a decrease of the impurity
greater than or equal to this value.</p>
<p>The weighted impurity decrease equation is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal notranslate"><span class="pre">N_t</span></code> is the number of
samples at the current node, <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> is the number of samples in the
left child, and <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
<p><code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t</span></code>, <code class="docutils literal notranslate"><span class="pre">N_t_R</span></code> and <code class="docutils literal notranslate"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
if <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code> is passed.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.19.</span></p>
</div>
</p></li>
<li><p><strong>min_impurity_split</strong> (<em>float</em><em>, </em><em>default=None</em>) – <p>Threshold for early stopping in tree growth. A node will split
if its impurity is above the threshold, otherwise it is a leaf.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.19: </span><code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
<code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
<code class="docutils literal notranslate"><span class="pre">min_impurity_split</span></code> has changed from 1e-7 to 0 in 0.23 and it
will be removed in 1.0 (renaming of 0.25).
Use <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> instead.</p>
</div>
</p></li>
<li><p><strong>max_leaf_nodes</strong> (<em>int</em><em>, </em><em>default=None</em>) – Grow a tree with <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
Best nodes are defined as relative reduction in impurity.
If None then unlimited number of leaf nodes.</p></li>
<li><p><strong>ccp_alpha</strong> (<em>non-negative float</em><em>, </em><em>default=0.0</em>) – <p>Complexity parameter used for Minimal Cost-Complexity Pruning. The
subtree with the largest cost complexity that is smaller than
<code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code> will be chosen. By default, no pruning is performed. See
<span class="xref std std-ref">minimal_cost_complexity_pruning</span> for details.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.22.</span></p>
</div>
</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor.max_features_">
<span class="sig-name descname"><span class="pre">max_features_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor.max_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The inferred value of max_features.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor.n_features_">
<span class="sig-name descname"><span class="pre">n_features_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor.n_features_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of features when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor.feature_importances_">
<span class="sig-name descname"><span class="pre">feature_importances_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor.feature_importances_" title="Permalink to this definition">¶</a></dt>
<dd><p>Return impurity-based feature importances (the higher, the more
important the feature).</p>
<p>Warning: impurity-based feature importances can be misleading for
high cardinality features (many unique values). See
<a class="reference internal" href="sklearn.inspection.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance()</span></code></a> as an alternative.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>ndarray of shape (n_features,)</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor.n_outputs_">
<span class="sig-name descname"><span class="pre">n_outputs_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor.n_outputs_" title="Permalink to this definition">¶</a></dt>
<dd><p>The number of outputs when <code class="docutils literal notranslate"><span class="pre">fit</span></code> is performed.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="sklearn.tree.ExtraTreeRegressor.tree_">
<span class="sig-name descname"><span class="pre">tree_</span></span><a class="headerlink" href="#sklearn.tree.ExtraTreeRegressor.tree_" title="Permalink to this definition">¶</a></dt>
<dd><p>The underlying Tree object. Please refer to
<code class="docutils literal notranslate"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
<span class="xref std std-ref">sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py</span>
for basic usage of these attributes.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Tree instance</p>
</dd>
</dl>
</dd></dl>

<div class="admonition seealso">
<p class="admonition-title">See also</p>
<dl class="simple">
<dt><a class="reference internal" href="#sklearn.tree.ExtraTreeClassifier" title="sklearn.tree.ExtraTreeClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ExtraTreeClassifier</span></code></a></dt><dd><p>An extremely randomized tree classifier.</p>
</dd>
<dt><a class="reference internal" href="sklearn.ensemble.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesClassifier</span></code></a></dt><dd><p>An extra-trees classifier.</p>
</dd>
<dt><a class="reference internal" href="sklearn.ensemble.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.ensemble.ExtraTreesRegressor</span></code></a></dt><dd><p>An extra-trees regressor.</p>
</dd>
</dl>
</div>
<p class="rubric">Notes</p>
<p>The default values for the parameters controlling the size of the trees
(e.g. <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>, etc.) lead to fully grown and
unpruned trees which can potentially be very large on some data sets. To
reduce memory consumption, the complexity and size of the trees should be
controlled by setting those parameter values.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id12"><span class="brackets">1</span></dt>
<dd><p>P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized trees”,
Machine Learning, 63(1), 3-42, 2006.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">ExtraTreeRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">extra_tree</span> <span class="o">=</span> <span class="n">ExtraTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">BaggingRegressor</span><span class="p">(</span><span class="n">extra_tree</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.33...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.tree.export_graphviz">
<span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">export_graphviz</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tree</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">out_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">leaves_parallel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">impurity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">node_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proportion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rotate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounded</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">special_characters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fontname</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'helvetica'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.export_graphviz" title="Permalink to this definition">¶</a></dt>
<dd><p>Export a decision tree in DOT format.</p>
<p>This function generates a GraphViz representation of the decision tree,
which is then written into <cite>out_file</cite>. Once exported, graphical renderings
can be generated using, for example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ dot -Tps tree.dot -o tree.ps      (PostScript format)
$ dot -Tpng tree.dot -o tree.png    (PNG format)
</pre></div>
</div>
<p>The sample counts that are shown are weighted with any sample_weights that
might be present.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tree</strong> (<em>decision tree classifier</em>) – The decision tree to be exported to GraphViz.</p></li>
<li><p><strong>out_file</strong> (<em>object</em><em> or </em><em>str</em><em>, </em><em>default=None</em>) – <p>Handle or name of the output file. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the result is
returned as a string.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.20: </span>Default of out_file changed from “tree.dot” to None.</p>
</div>
</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the representation. If None, the tree is fully
generated.</p></li>
<li><p><strong>feature_names</strong> (<em>list of str</em><em>, </em><em>default=None</em>) – Names of each of the features.
If None generic names will be used (“feature_0”, “feature_1”, …).</p></li>
<li><p><strong>class_names</strong> (<em>list of str</em><em> or </em><em>bool</em><em>, </em><em>default=None</em>) – Names of each of the target classes in ascending numerical order.
Only relevant for classification and not supported for multi-output.
If <code class="docutils literal notranslate"><span class="pre">True</span></code>, shows a symbolic representation of the class name.</p></li>
<li><p><strong>label</strong> (<em>{'all'</em><em>, </em><em>'root'</em><em>, </em><em>'none'}</em><em>, </em><em>default='all'</em>) – Whether to show informative labels for impurity, etc.
Options include ‘all’ to show at every node, ‘root’ to show only at
the top root node, or ‘none’ to not show at any node.</p></li>
<li><p><strong>filled</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, paint nodes to indicate majority class for
classification, extremity of values for regression, or purity of node
for multi-output.</p></li>
<li><p><strong>leaves_parallel</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, draw all leaf nodes at the bottom of the tree.</p></li>
<li><p><strong>impurity</strong> (<em>bool</em><em>, </em><em>default=True</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, show the impurity at each node.</p></li>
<li><p><strong>node_ids</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, show the ID number on each node.</p></li>
<li><p><strong>proportion</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, change the display of ‘values’ and/or ‘samples’
to be proportions and percentages respectively.</p></li>
<li><p><strong>rotate</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, orient tree left to right rather than top-down.</p></li>
<li><p><strong>rounded</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, draw node boxes with rounded corners.</p></li>
<li><p><strong>special_characters</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, ignore special characters for PostScript
compatibility.</p></li>
<li><p><strong>precision</strong> (<em>int</em><em>, </em><em>default=3</em>) – Number of digits of precision for floating point in the values of
impurity, threshold and value attributes of each node.</p></li>
<li><p><strong>fontname</strong> (<em>str</em><em>, </em><em>default='helvetica'</em>) – Name of font used to render text.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><p><strong>dot_data</strong> – String representation of the input tree in GraphViz dot format.
Only returned if <code class="docutils literal notranslate"><span class="pre">out_file</span></code> is None.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.18.</span></p>
</div>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>
<span class="go">&#39;digraph Tree {...</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.tree.export_text">
<span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">export_text</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tree</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">spacing</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">decimals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">2</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_weights</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.export_text" title="Permalink to this definition">¶</a></dt>
<dd><p>Build a text report showing the rules of a decision tree.</p>
<p>Note that backwards compatibility may not be supported.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tree</strong> (<em>object</em>) – The decision tree estimator to be exported.
It can be an instance of
DecisionTreeClassifier or DecisionTreeRegressor.</p></li>
<li><p><strong>feature_names</strong> (<em>list of str</em><em>, </em><em>default=None</em>) – A list of length n_features containing the feature names.
If None generic names will be used (“feature_0”, “feature_1”, …).</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=10</em>) – Only the first max_depth levels of the tree are exported.
Truncated branches will be marked with “…”.</p></li>
<li><p><strong>spacing</strong> (<em>int</em><em>, </em><em>default=3</em>) – Number of spaces between edges. The higher it is, the wider the result.</p></li>
<li><p><strong>decimals</strong> (<em>int</em><em>, </em><em>default=2</em>) – Number of decimal digits to display.</p></li>
<li><p><strong>show_weights</strong> (<em>bool</em><em>, </em><em>default=False</em>) – If true the classification weights will be exported on each leaf.
The classification weights are the number of samples each class.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>report</strong> – Text summary of all the rules in the decision tree.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_text</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">export_text</span><span class="p">(</span><span class="n">decision_tree</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;feature_names&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="go">|--- petal width (cm) &lt;= 0.80</span>
<span class="go">|   |--- class: 0</span>
<span class="go">|--- petal width (cm) &gt;  0.80</span>
<span class="go">|   |--- petal width (cm) &lt;= 1.75</span>
<span class="go">|   |   |--- class: 1</span>
<span class="go">|   |--- petal width (cm) &gt;  1.75</span>
<span class="go">|   |   |--- class: 2</span>
</pre></div>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="sklearn.tree.plot_tree">
<span class="sig-prename descclassname"><span class="pre">sklearn.tree.</span></span><span class="sig-name descname"><span class="pre">plot_tree</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">decision_tree</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_depth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">feature_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">class_names</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">label</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">impurity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">node_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">proportion</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rotate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'deprecated'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rounded</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fontsize</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.tree.plot_tree" title="Permalink to this definition">¶</a></dt>
<dd><p>Plot a decision tree.</p>
<p>The sample counts that are shown are weighted with any sample_weights that
might be present.</p>
<p>The visualization is fit automatically to the size of the axis.
Use the <code class="docutils literal notranslate"><span class="pre">figsize</span></code> or <code class="docutils literal notranslate"><span class="pre">dpi</span></code> arguments of <code class="docutils literal notranslate"><span class="pre">plt.figure</span></code>  to control
the size of the rendering.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<div class="versionadded">
<p><span class="versionmodified added">New in version 0.21.</span></p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>decision_tree</strong> (<em>decision tree regressor</em><em> or </em><em>classifier</em>) – The decision tree to be plotted.</p></li>
<li><p><strong>max_depth</strong> (<em>int</em><em>, </em><em>default=None</em>) – The maximum depth of the representation. If None, the tree is fully
generated.</p></li>
<li><p><strong>feature_names</strong> (<em>list of strings</em><em>, </em><em>default=None</em>) – Names of each of the features.
If None, generic names will be used (“X[0]”, “X[1]”, …).</p></li>
<li><p><strong>class_names</strong> (<em>list of str</em><em> or </em><em>bool</em><em>, </em><em>default=None</em>) – Names of each of the target classes in ascending numerical order.
Only relevant for classification and not supported for multi-output.
If <code class="docutils literal notranslate"><span class="pre">True</span></code>, shows a symbolic representation of the class name.</p></li>
<li><p><strong>label</strong> (<em>{'all'</em><em>, </em><em>'root'</em><em>, </em><em>'none'}</em><em>, </em><em>default='all'</em>) – Whether to show informative labels for impurity, etc.
Options include ‘all’ to show at every node, ‘root’ to show only at
the top root node, or ‘none’ to not show at any node.</p></li>
<li><p><strong>filled</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, paint nodes to indicate majority class for
classification, extremity of values for regression, or purity of node
for multi-output.</p></li>
<li><p><strong>impurity</strong> (<em>bool</em><em>, </em><em>default=True</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, show the impurity at each node.</p></li>
<li><p><strong>node_ids</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, show the ID number on each node.</p></li>
<li><p><strong>proportion</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, change the display of ‘values’ and/or ‘samples’
to be proportions and percentages respectively.</p></li>
<li><p><strong>rotate</strong> (<em>bool</em><em>, </em><em>default=False</em>) – <p>This parameter has no effect on the matplotlib tree visualisation and
it is kept here for backward compatibility.</p>
<div class="deprecated">
<p><span class="versionmodified deprecated">Deprecated since version 0.23: </span><code class="docutils literal notranslate"><span class="pre">rotate</span></code> is deprecated in 0.23 and will be removed in 1.0
(renaming of 0.25).</p>
</div>
</p></li>
<li><p><strong>rounded</strong> (<em>bool</em><em>, </em><em>default=False</em>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, draw node boxes with rounded corners and use
Helvetica fonts instead of Times-Roman.</p></li>
<li><p><strong>precision</strong> (<em>int</em><em>, </em><em>default=3</em>) – Number of digits of precision for floating point in the values of
impurity, threshold and value attributes of each node.</p></li>
<li><p><strong>ax</strong> (<em>matplotlib axis</em><em>, </em><em>default=None</em>) – Axes to plot to. If None, use current axis. Any previous content
is cleared.</p></li>
<li><p><strong>fontsize</strong> (<em>int</em><em>, </em><em>default=None</em>) – Size of text font. If None, determined automatically to fit figure.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>annotations</strong> – List containing the artists for the annotation boxes making up the
tree.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of artists</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>  
<span class="go">[Text(251.5,345.217,&#39;X[3] &lt;= 0.8...</span>
</pre></div>
</div>
</dd></dl>

</section>
</section>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2021, Tommaso Fioravanti

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>